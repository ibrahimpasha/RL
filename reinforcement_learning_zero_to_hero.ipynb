{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Zero to Hero\n",
    "\n",
    "## A Comprehensive Journey from Foundational Concepts to Advanced Applications\n",
    "\n",
    "Welcome to this comprehensive Jupyter notebook on Reinforcement Learning! This notebook will guide you through a complete learning journey, starting from the basics and progressing to advanced research topics and real-world applications.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Foundational Concepts**: Understanding the core principles of reinforcement learning, including MDPs, value functions, and policies\n",
    "- **Core Algorithms**: Implementing and understanding key algorithms like Q-Learning, SARSA, DQN, and policy gradient methods\n",
    "- **Advanced Topics**: Exploring cutting-edge techniques in reward engineering, scaling, and specialized RL methods\n",
    "- **Real-World Applications**: Seeing how RL is applied in robotics, game playing, finance, healthcare, and more\n",
    "- **Research & Deployment**: Understanding current research trends and how to deploy RL systems in production\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "1. Execute cells sequentially from top to bottom\n",
    "2. Read the explanations carefully before running code\n",
    "3. Experiment with the code examples\n",
    "4. Modify parameters to see how they affect results\n",
    "5. Complete the exercises to reinforce your understanding\n",
    "\n",
    "Let's begin your journey into the exciting world of Reinforcement Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Dependencies](#setup)\n",
    "2. [Section 1: Foundational Concepts](#section1)\n",
    "   - [Introduction to Reinforcement Learning](#intro-rl)\n",
    "   - [Multi-Armed Bandit Problem](#bandits)\n",
    "   - [Core Terminology and MDP Framework](#mdp)\n",
    "   - [Policies and Value Functions](#policies)\n",
    "   - [Dynamic Programming](#dynamic-programming)\n",
    "   - [Learning Paradigms](#learning-paradigms)\n",
    "3. [Section 2: Core Algorithms](#section2)\n",
    "   - [Monte Carlo Methods](#monte-carlo)\n",
    "   - [Temporal Difference Learning](#td-learning)\n",
    "   - [Q-Learning](#q-learning)\n",
    "   - [Deep Q-Networks (DQN)](#dqn)\n",
    "   - [Policy Optimization Methods](#policy-optimization)\n",
    "4. [Section 3: Advanced Topics](#section3)\n",
    "   - [Reward Engineering](#reward-engineering)\n",
    "   - [Scaling and Generalization](#scaling)\n",
    "   - [Advanced Policy Methods](#advanced-policy)\n",
    "   - [Specialized RL Techniques](#specialized)\n",
    "5. [Section 4: Code Implementations](#section4)\n",
    "   - [Bandit Algorithms](#bandit-implementations)\n",
    "   - [MDP and Dynamic Programming](#mdp-implementations)\n",
    "   - [Monte Carlo Methods](#mc-implementations)\n",
    "   - [Temporal Difference Methods](#td-implementations)\n",
    "   - [Deep RL Implementations](#deep-rl-implementations)\n",
    "6. [Section 5: Real-World Applications](#section5)\n",
    "   - [Traffic Signal Control](#traffic)\n",
    "   - [Robotics](#robotics)\n",
    "   - [Autonomous Trading](#trading)\n",
    "   - [Recommendation Systems](#recommendations)\n",
    "   - [Healthcare](#healthcare)\n",
    "   - [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "   - [Game Playing](#game-playing)\n",
    "   - [Energy Management](#energy)\n",
    "   - [Chess Environment](#chess)\n",
    "7. [Section 6: Advanced Research & Deployment](#section6)\n",
    "   - [Current Research Trends](#research-trends)\n",
    "   - [Ethical and Safety Considerations](#ethics)\n",
    "   - [Deployment Challenges](#deployment)\n",
    "   - [End-to-End Pipeline](#pipeline)\n",
    "   - [Recent Research](#recent-research)\n",
    "8. [Conclusion and Next Steps](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Setup & Dependencies\n",
    "\n",
    "Before we begin, we need to install the required Python packages. This notebook uses several popular libraries for numerical computation, visualization, and reinforcement learning environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "# Note: You may need to restart the kernel after installation\n",
    "\n",
    "!pip install numpy>=1.21.0\n",
    "!pip install matplotlib>=3.4.0\n",
    "!pip install seaborn>=0.11.0\n",
    "!pip install gym>=0.21.0\n",
    "!pip install torch>=1.10.0\n",
    "!pip install pandas>=1.3.0"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "\n",
    "Now let's import all the libraries we'll be using throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Core numerical and scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Reinforcement Learning environments\n",
    "import gym\n",
    "\n",
    "# Deep Learning framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Installation\n",
    "\n",
    "Let's verify that all packages are installed correctly and check their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verification cell - check all installations\n",
    "import sys\n",
    "\n",
    "def check_package(package_name, import_name=None):\n",
    "    \"\"\"Check if a package is installed and print its version.\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    \n",
    "    try:\n",
    "        module = __import__(import_name)\n",
    "        version = getattr(module, '__version__', 'Unknown')\n",
    "        print(f\"‚úì {package_name:20s} version: {version}\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"‚úó {package_name:20s} NOT INSTALLED\")\n",
    "        return False\n",
    "\n",
    "print(\"Checking package installations...\\n\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\\n\")\n",
    "\n",
    "packages = [\n",
    "    ('numpy', 'numpy'),\n",
    "    ('matplotlib', 'matplotlib'),\n",
    "    ('seaborn', 'seaborn'),\n",
    "    ('gym', 'gym'),\n",
    "    ('torch', 'torch'),\n",
    "    ('pandas', 'pandas')\n",
    "]\n",
    "\n",
    "all_installed = True\n",
    "for package_name, import_name in packages:\n",
    "    if not check_package(package_name, import_name):\n",
    "        all_installed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if all_installed:\n",
    "    print(\"‚úì All packages installed successfully!\")\n",
    "    print(\"You're ready to start learning Reinforcement Learning!\")\n",
    "else:\n",
    "    print(\"‚úó Some packages are missing. Please install them using:\")\n",
    "    print(\"  pip install numpy matplotlib seaborn gym torch pandas\")\n",
    "print(\"=\"*50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Section 1: Foundational Concepts\n",
    "\n",
    "In this section, we'll build a solid foundation in reinforcement learning by exploring core concepts, starting with the simplest problems and gradually increasing complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro-rl'></a>\n",
    "### Introduction to Reinforcement Learning\n",
    "\n",
    "**What is Reinforcement Learning?**\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. Unlike other machine learning paradigms, RL focuses on learning through trial and error, receiving feedback in the form of rewards or penalties.\n",
    "\n",
    "**How RL Differs from Other Machine Learning Paradigms:**\n",
    "\n",
    "1. **Supervised Learning**: \n",
    "   - Learns from labeled examples (input-output pairs)\n",
    "   - The correct answer is provided for each training example\n",
    "   - Example: Image classification, where each image has a label\n",
    "\n",
    "2. **Unsupervised Learning**: \n",
    "   - Learns patterns from unlabeled data\n",
    "   - No explicit feedback or correct answers\n",
    "   - Example: Clustering customers based on purchasing behavior\n",
    "\n",
    "3. **Reinforcement Learning**: \n",
    "   - Learns from interaction with an environment\n",
    "   - Receives delayed rewards/penalties as feedback\n",
    "   - Must discover which actions yield the most reward through exploration\n",
    "   - Example: Teaching a robot to walk, playing chess, or optimizing ad placement\n",
    "\n",
    "**Key Characteristics of RL:**\n",
    "\n",
    "- **Sequential Decision Making**: Actions affect future states and rewards\n",
    "- **Trial and Error**: The agent must explore to discover good strategies\n",
    "- **Delayed Consequences**: Rewards may come long after the actions that caused them\n",
    "- **Trade-offs**: Must balance exploration (trying new things) vs exploitation (using known good strategies)\n",
    "\n",
    "**The RL Loop:**\n",
    "\n",
    "The fundamental interaction pattern in RL is:\n",
    "\n",
    "1. Agent observes the current **state** of the environment\n",
    "2. Agent selects and performs an **action**\n",
    "3. Environment transitions to a new **state**\n",
    "4. Agent receives a **reward** signal\n",
    "5. Repeat\n",
    "\n",
    "Let's see this in action with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simple demonstration of the RL loop\n",
    "# We'll create a basic environment and agent to illustrate the interaction\n",
    "\n",
    "class SimpleEnvironment:\n",
    "    \"\"\"A simple environment where the agent tries to reach a goal.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.position = 0  # Agent starts at position 0\n",
    "        self.goal = 5      # Goal is at position 5\n",
    "        self.max_steps = 10\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state.\"\"\"\n",
    "        self.position = 0\n",
    "        self.current_step = 0\n",
    "        return self.position\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute an action and return (next_state, reward, done).\n",
    "        \n",
    "        Args:\n",
    "            action: 0 = move left, 1 = move right\n",
    "        \n",
    "        Returns:\n",
    "            next_state: The new position\n",
    "            reward: Reward for this transition\n",
    "            done: Whether the episode is finished\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Update position based on action\n",
    "        if action == 0:  # Move left\n",
    "            self.position = max(0, self.position - 1)\n",
    "        else:  # Move right\n",
    "            self.position = min(10, self.position + 1)\n",
    "        \n",
    "        # Calculate reward\n",
    "        if self.position == self.goal:\n",
    "            reward = 10  # Large reward for reaching goal\n",
    "            done = True\n",
    "        elif self.current_step >= self.max_steps:\n",
    "            reward = -5  # Penalty for taking too long\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1  # Small penalty for each step (encourages efficiency)\n",
    "            done = False\n",
    "        \n",
    "        return self.position, reward, done\n",
    "\n",
    "\n",
    "class SimpleAgent:\n",
    "    \"\"\"A simple agent that takes random actions.\"\"\"\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action (randomly for now).\"\"\"\n",
    "        return np.random.choice([0, 1])  # 0 = left, 1 = right\n",
    "\n",
    "\n",
    "# Demonstrate the RL loop\n",
    "print(\"Demonstrating the Reinforcement Learning Loop\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = SimpleEnvironment()\n",
    "agent = SimpleAgent()\n",
    "\n",
    "# Run one episode\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "step = 0\n",
    "\n",
    "print(f\"Initial State: Position = {state}, Goal = {env.goal}\\n\")\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # Agent observes state and selects action\n",
    "    action = agent.select_action(state)\n",
    "    action_name = \"LEFT\" if action == 0 else \"RIGHT\"\n",
    "    \n",
    "    # Environment responds to action\n",
    "    next_state, reward, done = env.step(action)\n",
    "    \n",
    "    # Track cumulative reward\n",
    "    total_reward += reward\n",
    "    step += 1\n",
    "    \n",
    "    # Display the interaction\n",
    "    print(f\"Step {step}:\")\n",
    "    print(f\"  State: {state} ‚Üí Action: {action_name} ‚Üí Next State: {next_state}\")\n",
    "    print(f\"  Reward: {reward:+.0f} | Total Reward: {total_reward:+.0f}\")\n",
    "    \n",
    "    if done:\n",
    "        if next_state == env.goal:\n",
    "            print(f\"\\n‚úì Goal reached in {step} steps!\")\n",
    "        else:\n",
    "            print(f\"\\n‚úó Failed to reach goal within {env.max_steps} steps.\")\n",
    "    print()\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal Total Reward: {total_reward:+.0f}\")\n",
    "print(\"\\nThis demonstrates the core RL loop:\")\n",
    "print(\"  1. Agent observes STATE\")\n",
    "print(\"  2. Agent takes ACTION\")\n",
    "print(\"  3. Environment provides REWARD and new STATE\")\n",
    "print(\"  4. Repeat until episode ends\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "\n",
    "- The agent doesn't know the optimal strategy initially\n",
    "- It must learn through experience which actions lead to higher rewards\n",
    "- The random agent above is inefficient - a learning agent would improve over time\n",
    "- This simple example captures the essence of RL: learning to make good decisions through interaction\n",
    "\n",
    "In the following sections, we'll explore how agents can learn optimal strategies, starting with one of the simplest RL problems: the Multi-Armed Bandit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bandits'></a>\n",
    "### Multi-Armed Bandit Problem\n",
    "\n",
    "**What is the Multi-Armed Bandit Problem?**\n",
    "\n",
    "Imagine you're in a casino facing a row of slot machines (also called \"one-armed bandits\"). Each machine has a different, unknown probability of paying out. You have a limited budget and want to maximize your total winnings. Which machines should you play?\n",
    "\n",
    "This is the **K-Armed Bandit Problem**, one of the simplest yet most fundamental problems in reinforcement learning. It's called \"K-armed\" because there are K different slot machines (or \"arms\") to choose from.\n",
    "\n",
    "**The Exploration-Exploitation Dilemma**\n",
    "\n",
    "The bandit problem perfectly illustrates the core challenge in RL:\n",
    "\n",
    "- **Exploitation**: Play the machine that has given you the best results so far (use your current knowledge)\n",
    "- **Exploration**: Try other machines to see if they might be better (gather more information)\n",
    "\n",
    "If you only exploit, you might miss out on better options you haven't tried enough. If you only explore, you waste time on machines you already know are bad. The key is finding the right balance.\n",
    "\n",
    "**Formal Definition:**\n",
    "\n",
    "- You have K actions (arms) to choose from\n",
    "- Each action has an unknown expected reward (the \"true value\")\n",
    "- When you select an action, you receive a reward drawn from that action's probability distribution\n",
    "- Your goal: maximize the total reward over many time steps\n",
    "\n",
    "Let's implement a simple bandit environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultiArmedBandit:\n",
    "    \"\"\"A K-armed bandit environment.\n",
    "    \n",
    "    Each arm has a true mean reward, and pulling an arm gives a reward\n",
    "    sampled from a normal distribution around that mean.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=10, mean_range=(0, 1), std=1.0):\n",
    "        \"\"\"Initialize the bandit.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of arms\n",
    "            mean_range: Range for true mean rewards\n",
    "            std: Standard deviation of reward distributions\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.std = std\n",
    "        \n",
    "        # True mean reward for each arm (unknown to the agent)\n",
    "        self.true_means = np.random.uniform(mean_range[0], mean_range[1], k)\n",
    "        \n",
    "        # Track which arm is actually best\n",
    "        self.best_arm = np.argmax(self.true_means)\n",
    "        \n",
    "    def pull(self, arm):\n",
    "        \"\"\"Pull an arm and receive a reward.\n",
    "        \n",
    "        Args:\n",
    "            arm: Index of the arm to pull (0 to k-1)\n",
    "            \n",
    "        Returns:\n",
    "            reward: Sampled reward from this arm's distribution\n",
    "        \"\"\"\n",
    "        # Sample reward from normal distribution around true mean\n",
    "        reward = np.random.normal(self.true_means[arm], self.std)\n",
    "        return reward\n",
    "    \n",
    "    def get_optimal_reward(self):\n",
    "        \"\"\"Return the expected reward of the best arm.\"\"\"\n",
    "        return self.true_means[self.best_arm]\n",
    "\n",
    "\n",
    "# Create a 10-armed bandit\n",
    "np.random.seed(42)\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "\n",
    "print(\"Multi-Armed Bandit Environment Created\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of arms: {bandit.k}\")\n",
    "print(f\"\\nTrue mean rewards for each arm:\")\n",
    "for i, mean in enumerate(bandit.true_means):\n",
    "    marker = \" ‚Üê BEST\" if i == bandit.best_arm else \"\"\n",
    "    print(f\"  Arm {i}: {mean:.3f}{marker}\")\n",
    "print(f\"\\nOptimal arm: {bandit.best_arm} (mean reward: {bandit.get_optimal_reward():.3f})\")\n",
    "print(\"\\nNote: The agent doesn't know these true values!\")\n",
    "print(\"      It must learn them through experience.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Greedy Strategy and Its Fatal Flaw\n",
    "\n",
    "**What is the Greedy Strategy?**\n",
    "\n",
    "The simplest approach to the bandit problem is the **greedy strategy**: always choose the action that has the highest estimated value based on your experience so far.\n",
    "\n",
    "**How it works:**\n",
    "1. Keep track of the average reward received from each arm\n",
    "2. Always select the arm with the highest average reward\n",
    "3. Update the average after each pull\n",
    "\n",
    "**The Fatal Flaw:**\n",
    "\n",
    "The greedy strategy can easily get stuck on a suboptimal arm! Here's why:\n",
    "\n",
    "- Suppose you try arm 3 first and get lucky with a high reward\n",
    "- Now arm 3 has the highest estimated value\n",
    "- The greedy strategy will keep choosing arm 3 forever\n",
    "- You'll never discover that arm 7 is actually better!\n",
    "\n",
    "This is called **premature convergence** - the agent stops exploring too early and misses better options.\n",
    "\n",
    "Let's implement a greedy agent and see this problem in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GreedyAgent:\n",
    "    \"\"\"An agent that always selects the arm with highest estimated value.\"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\"Initialize the agent.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of arms\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.q_estimates = np.zeros(k)  # Estimated value of each arm\n",
    "        self.action_counts = np.zeros(k)  # Number of times each arm was pulled\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select the arm with highest estimated value (greedy).\"\"\"\n",
    "        # Break ties randomly\n",
    "        max_value = np.max(self.q_estimates)\n",
    "        best_arms = np.where(self.q_estimates == max_value)[0]\n",
    "        return np.random.choice(best_arms)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimates after receiving a reward.\n",
    "        \n",
    "        Uses incremental average formula:\n",
    "        NewEstimate = OldEstimate + (1/n) * (Reward - OldEstimate)\n",
    "        \"\"\"\n",
    "        self.action_counts[action] += 1\n",
    "        n = self.action_counts[action]\n",
    "        \n",
    "        # Incremental update of average\n",
    "        self.q_estimates[action] += (1/n) * (reward - self.q_estimates[action])\n",
    "\n",
    "\n",
    "def run_experiment(agent, bandit, steps=1000):\n",
    "    \"\"\"Run an experiment with an agent on a bandit.\n",
    "    \n",
    "    Returns:\n",
    "        rewards: Array of rewards received at each step\n",
    "        optimal_actions: Array indicating if optimal action was chosen\n",
    "    \"\"\"\n",
    "    rewards = np.zeros(steps)\n",
    "    optimal_actions = np.zeros(steps)\n",
    "    \n",
    "    for t in range(steps):\n",
    "        # Agent selects action\n",
    "        action = agent.select_action()\n",
    "        \n",
    "        # Environment provides reward\n",
    "        reward = bandit.pull(action)\n",
    "        \n",
    "        # Agent updates its estimates\n",
    "        agent.update(action, reward)\n",
    "        \n",
    "        # Track results\n",
    "        rewards[t] = reward\n",
    "        optimal_actions[t] = 1 if action == bandit.best_arm else 0\n",
    "    \n",
    "    return rewards, optimal_actions\n",
    "\n",
    "\n",
    "# Demonstrate the greedy strategy's failure\n",
    "print(\"Demonstrating the Greedy Strategy\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "greedy_agent = GreedyAgent(k=10)\n",
    "\n",
    "rewards, optimal_actions = run_experiment(greedy_agent, bandit, steps=1000)\n",
    "\n",
    "print(f\"\\nAfter 1000 steps:\")\n",
    "print(f\"\\nArm selection counts:\")\n",
    "for i in range(bandit.k):\n",
    "    count = greedy_agent.action_counts[i]\n",
    "    estimate = greedy_agent.q_estimates[i]\n",
    "    true_value = bandit.true_means[i]\n",
    "    marker = \" ‚Üê OPTIMAL\" if i == bandit.best_arm else \"\"\n",
    "    print(f\"  Arm {i}: pulled {count:4.0f} times | \"\n",
    "          f\"estimated value: {estimate:6.3f} | true value: {true_value:6.3f}{marker}\")\n",
    "\n",
    "optimal_pct = np.mean(optimal_actions) * 100\n",
    "avg_reward = np.mean(rewards)\n",
    "optimal_reward = bandit.get_optimal_reward()\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Optimal action selected: {optimal_pct:.1f}% of the time\")\n",
    "print(f\"  Average reward: {avg_reward:.3f}\")\n",
    "print(f\"  Optimal reward: {optimal_reward:.3f}\")\n",
    "print(f\"  Regret: {optimal_reward - avg_reward:.3f}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Notice: The greedy agent likely got stuck on a suboptimal arm!\")\n",
    "print(f\"    It stopped exploring and missed the best option.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Greedy Strategy's Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run multiple experiments to see the pattern\n",
    "num_experiments = 100\n",
    "steps = 1000\n",
    "\n",
    "all_rewards = np.zeros((num_experiments, steps))\n",
    "all_optimal = np.zeros((num_experiments, steps))\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(num_experiments):\n",
    "    bandit = MultiArmedBandit(k=10)\n",
    "    agent = GreedyAgent(k=10)\n",
    "    rewards, optimal = run_experiment(agent, bandit, steps)\n",
    "    all_rewards[i] = rewards\n",
    "    all_optimal[i] = optimal\n",
    "\n",
    "# Calculate averages across experiments\n",
    "avg_rewards = np.mean(all_rewards, axis=0)\n",
    "avg_optimal = np.mean(all_optimal, axis=0)\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Average reward over time\n",
    "ax1.plot(avg_rewards, linewidth=2, color='red', alpha=0.8)\n",
    "ax1.set_xlabel('Steps', fontsize=12)\n",
    "ax1.set_ylabel('Average Reward', fontsize=12)\n",
    "ax1.set_title('Greedy Strategy: Average Reward Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=np.mean(avg_rewards), color='red', linestyle='--', alpha=0.5, label=f'Mean: {np.mean(avg_rewards):.3f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Percentage of optimal actions\n",
    "ax2.plot(avg_optimal * 100, linewidth=2, color='red', alpha=0.8)\n",
    "ax2.set_xlabel('Steps', fontsize=12)\n",
    "ax2.set_ylabel('% Optimal Action', fontsize=12)\n",
    "ax2.set_title('Greedy Strategy: Percentage of Optimal Actions', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=np.mean(avg_optimal) * 100, color='red', linestyle='--', alpha=0.5, \n",
    "            label=f'Mean: {np.mean(avg_optimal)*100:.1f}%')\n",
    "ax2.legend()\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"   - The greedy strategy quickly settles on an arm (often suboptimal)\")\n",
    "print(\"   - It rarely selects the optimal action because it stopped exploring\")\n",
    "print(\"   - Performance plateaus early and doesn't improve\")\n",
    "print(\"   - This demonstrates why pure exploitation fails!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Epsilon-Greedy Algorithm\n",
    "\n",
    "**A Simple Solution to the Exploration-Exploitation Dilemma**\n",
    "\n",
    "The epsilon-greedy algorithm provides a simple yet effective solution to the greedy strategy's fatal flaw. Instead of always exploiting, it introduces controlled exploration.\n",
    "\n",
    "**How Epsilon-Greedy Works:**\n",
    "\n",
    "With probability $\\epsilon$ (epsilon): Choose a **random** action (explore)\n",
    "\n",
    "With probability $1 - \\epsilon$: Choose the **best known** action (exploit)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "$$\n",
    "A_t = \\begin{cases}\n",
    "\\text{random action} & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_a Q_t(a) & \\text{with probability } 1 - \\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $Q_t(a)$ is the estimated value of action $a$ at time $t$.\n",
    "\n",
    "**Key Parameters:**\n",
    "\n",
    "- $\\epsilon = 0$: Pure exploitation (greedy strategy)\n",
    "- $\\epsilon = 1$: Pure exploration (random selection)\n",
    "- $\\epsilon = 0.1$: A common choice - explore 10% of the time\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to implement\n",
    "- Guarantees all actions are tried infinitely often (in the limit)\n",
    "- Balances exploration and exploitation\n",
    "\n",
    "**Trade-offs:**\n",
    "- Explores uniformly (doesn't prioritize promising actions)\n",
    "- Continues exploring even after finding the best action\n",
    "- Choice of $\\epsilon$ affects performance\n",
    "\n",
    "Let's implement the epsilon-greedy algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class EpsilonGreedyAgent:\n",
    "    \"\"\"An agent that uses epsilon-greedy action selection.\"\"\"\n",
    "    \n",
    "    def __init__(self, k, epsilon=0.1):\n",
    "        \"\"\"Initialize the agent.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of arms\n",
    "            epsilon: Probability of random exploration (0 to 1)\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.q_estimates = np.zeros(k)  # Estimated value of each arm\n",
    "        self.action_counts = np.zeros(k)  # Number of times each arm was pulled\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select action using epsilon-greedy strategy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: choose random action\n",
    "            return np.random.randint(0, self.k)\n",
    "        else:\n",
    "            # Exploit: choose best known action\n",
    "            max_value = np.max(self.q_estimates)\n",
    "            best_arms = np.where(self.q_estimates == max_value)[0]\n",
    "            return np.random.choice(best_arms)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimates after receiving a reward.\"\"\"\n",
    "        self.action_counts[action] += 1\n",
    "        n = self.action_counts[action]\n",
    "        \n",
    "        # Incremental update of average\n",
    "        self.q_estimates[action] += (1/n) * (reward - self.q_estimates[action])\n",
    "\n",
    "\n",
    "# Test epsilon-greedy with different epsilon values\n",
    "print(\"Epsilon-Greedy Algorithm Demonstration\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "\n",
    "epsilon_values = [0.0, 0.01, 0.1, 0.3]\n",
    "results = {}\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    agent = EpsilonGreedyAgent(k=10, epsilon=eps)\n",
    "    rewards, optimal_actions = run_experiment(agent, bandit, steps=1000)\n",
    "    \n",
    "    results[eps] = {\n",
    "        'rewards': rewards,\n",
    "        'optimal': optimal_actions,\n",
    "        'avg_reward': np.mean(rewards),\n",
    "        'optimal_pct': np.mean(optimal_actions) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nŒµ = {eps:.2f}:\")\n",
    "    print(f\"  Average reward: {results[eps]['avg_reward']:.3f}\")\n",
    "    print(f\"  Optimal action: {results[eps]['optimal_pct']:.1f}% of the time\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   - Œµ = 0.0 (greedy) gets stuck on suboptimal actions\")\n",
    "print(\"   - Small Œµ values (0.01-0.1) balance exploration and exploitation well\")\n",
    "print(\"   - Large Œµ values (0.3) explore too much and waste opportunities\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Greedy vs Epsilon-Greedy Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run comprehensive comparison across multiple experiments\n",
    "num_experiments = 200\n",
    "steps = 1000\n",
    "epsilon_values = [0.0, 0.01, 0.1, 0.3]\n",
    "\n",
    "# Store results for each epsilon value\n",
    "all_results = {eps: {'rewards': [], 'optimal': []} for eps in epsilon_values}\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(num_experiments):\n",
    "    bandit = MultiArmedBandit(k=10)\n",
    "    \n",
    "    for eps in epsilon_values:\n",
    "        agent = EpsilonGreedyAgent(k=10, epsilon=eps)\n",
    "        rewards, optimal = run_experiment(agent, bandit, steps)\n",
    "        all_results[eps]['rewards'].append(rewards)\n",
    "        all_results[eps]['optimal'].append(optimal)\n",
    "\n",
    "# Calculate averages\n",
    "avg_results = {}\n",
    "for eps in epsilon_values:\n",
    "    avg_results[eps] = {\n",
    "        'rewards': np.mean(all_results[eps]['rewards'], axis=0),\n",
    "        'optimal': np.mean(all_results[eps]['optimal'], axis=0)\n",
    "    }\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue']\n",
    "labels = [f'Œµ = {eps:.2f}' + (' (Greedy)' if eps == 0 else '') for eps in epsilon_values]\n",
    "\n",
    "# Plot 1: Average reward over time\n",
    "for eps, color, label in zip(epsilon_values, colors, labels):\n",
    "    ax1.plot(avg_results[eps]['rewards'], linewidth=2, color=color, alpha=0.8, label=label)\n",
    "\n",
    "ax1.set_xlabel('Steps', fontsize=12)\n",
    "ax1.set_ylabel('Average Reward', fontsize=12)\n",
    "ax1.set_title('Epsilon-Greedy: Average Reward Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Percentage of optimal actions\n",
    "for eps, color, label in zip(epsilon_values, colors, labels):\n",
    "    ax2.plot(avg_results[eps]['optimal'] * 100, linewidth=2, color=color, alpha=0.8, label=label)\n",
    "\n",
    "ax2.set_xlabel('Steps', fontsize=12)\n",
    "ax2.set_ylabel('% Optimal Action', fontsize=12)\n",
    "ax2.set_title('Epsilon-Greedy: Optimal Action Selection', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nüìä Performance Summary (averaged over {} experiments):\".format(num_experiments))\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Strategy':<20} {'Avg Reward':<15} {'Optimal %':<15} {'Final Optimal %'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    strategy = f\"Œµ = {eps:.2f}\" + (\" (Greedy)\" if eps == 0 else \"\")\n",
    "    avg_reward = np.mean(avg_results[eps]['rewards'])\n",
    "    avg_optimal = np.mean(avg_results[eps]['optimal']) * 100\n",
    "    final_optimal = np.mean(avg_results[eps]['optimal'][-100:]) * 100  # Last 100 steps\n",
    "    \n",
    "    print(f\"{strategy:<20} {avg_reward:<15.3f} {avg_optimal:<15.1f} {final_optimal:.1f}%\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Conclusions:\")\n",
    "print(\"   1. Greedy (Œµ=0) performs poorly due to lack of exploration\")\n",
    "print(\"   2. Small epsilon (0.01-0.1) achieves good balance\")\n",
    "print(\"   3. Œµ=0.1 typically performs best in this setting\")\n",
    "print(\"   4. Too much exploration (Œµ=0.3) wastes opportunities to exploit\")\n",
    "print(\"   5. Epsilon-greedy successfully solves the exploration-exploitation dilemma!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimistic Initial Values: Exploration Through Disappointment\n",
    "\n",
    "**A Clever Alternative to Epsilon-Greedy**\n",
    "\n",
    "The Optimistic Initial Values approach provides a different solution to encourage exploration. Instead of randomly exploring, it uses **disappointment-driven exploration**.\n",
    "\n",
    "**The Key Idea:**\n",
    "\n",
    "Initialize all action-value estimates to be **optimistically high** (higher than any realistic reward). When the agent tries an action and receives a lower-than-expected reward, it becomes \"disappointed\" and tries other actions, naturally encouraging exploration.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "1. Set initial Q-values to a high value (e.g., +5 when true rewards are around 0-1)\n",
    "2. Use a greedy strategy (no epsilon needed!)\n",
    "3. Each action will initially seem promising\n",
    "4. After trying an action, its estimate decreases toward the true value\n",
    "5. The agent naturally tries all actions before settling on the best one\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "\n",
    "If we initialize $Q_0(a) = c$ for all actions where $c$ is large:\n",
    "\n",
    "$$Q_{n+1}(a) = Q_n(a) + \\frac{1}{n+1}(R_n - Q_n(a))$$\n",
    "\n",
    "Since $R_n < Q_n(a)$ initially, the estimate decreases, making other untried actions more attractive.\n",
    "\n",
    "**Advantages:**\n",
    "- No need to tune an epsilon parameter\n",
    "- Exploration happens naturally through the learning process\n",
    "- Simple to implement\n",
    "- Works well for stationary problems\n",
    "\n",
    "**Disadvantages:**\n",
    "- Only explores at the beginning (not suitable for non-stationary problems)\n",
    "- Requires knowing a good initial value\n",
    "- Less flexible than epsilon-greedy\n",
    "\n",
    "Let's implement and compare this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class OptimisticGreedyAgent:\n",
    "    \"\"\"A greedy agent with optimistic initial value estimates.\"\"\"\n",
    "    \n",
    "    def __init__(self, k, initial_value=5.0):\n",
    "        \"\"\"Initialize the agent with optimistic values.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of arms\n",
    "            initial_value: Optimistic initial estimate for all actions\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.initial_value = initial_value\n",
    "        # Initialize all estimates optimistically\n",
    "        self.q_estimates = np.ones(k) * initial_value\n",
    "        self.action_counts = np.zeros(k)\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select action greedily (highest estimated value).\"\"\"\n",
    "        max_value = np.max(self.q_estimates)\n",
    "        best_arms = np.where(self.q_estimates == max_value)[0]\n",
    "        return np.random.choice(best_arms)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimates after receiving a reward.\"\"\"\n",
    "        self.action_counts[action] += 1\n",
    "        n = self.action_counts[action]\n",
    "        \n",
    "        # Incremental update\n",
    "        self.q_estimates[action] += (1/n) * (reward - self.q_estimates[action])\n",
    "\n",
    "\n",
    "# Demonstrate optimistic initial values\n",
    "print(\"Optimistic Initial Values Demonstration\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "\n",
    "print(f\"True reward range: [{bandit.true_means.min():.2f}, {bandit.true_means.max():.2f}]\")\n",
    "print(f\"Optimal arm: {bandit.best_arm} (mean: {bandit.get_optimal_reward():.3f})\\n\")\n",
    "\n",
    "# Test different initial values\n",
    "initial_values = [0.0, 2.0, 5.0, 10.0]\n",
    "optimistic_results = {}\n",
    "\n",
    "for init_val in initial_values:\n",
    "    agent = OptimisticGreedyAgent(k=10, initial_value=init_val)\n",
    "    rewards, optimal_actions = run_experiment(agent, bandit, steps=1000)\n",
    "    \n",
    "    optimistic_results[init_val] = {\n",
    "        'rewards': rewards,\n",
    "        'optimal': optimal_actions,\n",
    "        'avg_reward': np.mean(rewards),\n",
    "        'optimal_pct': np.mean(optimal_actions) * 100,\n",
    "        'agent': agent\n",
    "    }\n",
    "    \n",
    "    print(f\"Initial Value = {init_val:.1f}:\")\n",
    "    print(f\"  Average reward: {optimistic_results[init_val]['avg_reward']:.3f}\")\n",
    "    print(f\"  Optimal action: {optimistic_results[init_val]['optimal_pct']:.1f}% of the time\")\n",
    "    print(f\"  Final estimates: {agent.q_estimates}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   - Initial value = 0: Behaves like standard greedy (poor exploration)\")\n",
    "print(\"   - Initial value = 5-10: Encourages exploration through disappointment\")\n",
    "print(\"   - Higher initial values ‚Üí more initial exploration\")\n",
    "print(\"   - Eventually converges to true values regardless of initialization\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Optimistic Initial Values with Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Comprehensive comparison: Optimistic vs Epsilon-Greedy\n",
    "num_experiments = 200\n",
    "steps = 1000\n",
    "\n",
    "# Strategies to compare\n",
    "strategies = {\n",
    "    'Greedy (Q=0)': {'type': 'optimistic', 'init': 0.0},\n",
    "    'Optimistic (Q=5)': {'type': 'optimistic', 'init': 5.0},\n",
    "    'Œµ-greedy (Œµ=0.1)': {'type': 'epsilon', 'epsilon': 0.1},\n",
    "    'Œµ-greedy (Œµ=0.01)': {'type': 'epsilon', 'epsilon': 0.01}\n",
    "}\n",
    "\n",
    "comparison_results = {name: {'rewards': [], 'optimal': []} for name in strategies.keys()}\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(num_experiments):\n",
    "    bandit = MultiArmedBandit(k=10)\n",
    "    \n",
    "    for name, config in strategies.items():\n",
    "        if config['type'] == 'optimistic':\n",
    "            agent = OptimisticGreedyAgent(k=10, initial_value=config['init'])\n",
    "        else:\n",
    "            agent = EpsilonGreedyAgent(k=10, epsilon=config['epsilon'])\n",
    "        \n",
    "        rewards, optimal = run_experiment(agent, bandit, steps)\n",
    "        comparison_results[name]['rewards'].append(rewards)\n",
    "        comparison_results[name]['optimal'].append(optimal)\n",
    "\n",
    "# Calculate averages\n",
    "avg_comparison = {}\n",
    "for name in strategies.keys():\n",
    "    avg_comparison[name] = {\n",
    "        'rewards': np.mean(comparison_results[name]['rewards'], axis=0),\n",
    "        'optimal': np.mean(comparison_results[name]['optimal'], axis=0)\n",
    "    }\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "colors = ['red', 'purple', 'green', 'orange']\n",
    "linestyles = ['-', '-', '--', '--']\n",
    "\n",
    "# Plot 1: Average reward\n",
    "for (name, color, ls) in zip(strategies.keys(), colors, linestyles):\n",
    "    ax1.plot(avg_comparison[name]['rewards'], linewidth=2, color=color, \n",
    "             linestyle=ls, alpha=0.8, label=name)\n",
    "\n",
    "ax1.set_xlabel('Steps', fontsize=12)\n",
    "ax1.set_ylabel('Average Reward', fontsize=12)\n",
    "ax1.set_title('Optimistic Initial Values vs Epsilon-Greedy: Average Reward', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Optimal action percentage\n",
    "for (name, color, ls) in zip(strategies.keys(), colors, linestyles):\n",
    "    ax2.plot(avg_comparison[name]['optimal'] * 100, linewidth=2, color=color, \n",
    "             linestyle=ls, alpha=0.8, label=name)\n",
    "\n",
    "ax2.set_xlabel('Steps', fontsize=12)\n",
    "ax2.set_ylabel('% Optimal Action', fontsize=12)\n",
    "ax2.set_title('Optimistic Initial Values vs Epsilon-Greedy: Optimal Action Selection', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìä Performance Comparison (averaged over {} experiments):\".format(num_experiments))\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Strategy':<25} {'Avg Reward':<15} {'Optimal %':<15} {'Early (0-100)':<15} {'Late (900-1000)'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name in strategies.keys():\n",
    "    avg_reward = np.mean(avg_comparison[name]['rewards'])\n",
    "    avg_optimal = np.mean(avg_comparison[name]['optimal']) * 100\n",
    "    early_optimal = np.mean(avg_comparison[name]['optimal'][:100]) * 100\n",
    "    late_optimal = np.mean(avg_comparison[name]['optimal'][-100:]) * 100\n",
    "    \n",
    "    print(f\"{name:<25} {avg_reward:<15.3f} {avg_optimal:<15.1f} {early_optimal:<15.1f} {late_optimal:.1f}%\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚úÖ Key Insights:\")\n",
    "print(\"   1. Optimistic initialization explores more early on\")\n",
    "print(\"   2. Epsilon-greedy maintains consistent exploration throughout\")\n",
    "print(\"   3. Optimistic approach eventually stops exploring (greedy after learning)\")\n",
    "print(\"   4. Both methods significantly outperform standard greedy\")\n",
    "print(\"   5. Choice depends on problem: stationary ‚Üí optimistic, non-stationary ‚Üí epsilon-greedy\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upper Confidence Bound (UCB): Uncertainty-Driven Exploration\n",
    "\n",
    "**The Most Sophisticated Bandit Algorithm**\n",
    "\n",
    "The Upper Confidence Bound (UCB) algorithm represents a more principled approach to the exploration-exploitation dilemma. Instead of exploring randomly (epsilon-greedy) or through disappointment (optimistic initialization), UCB explores based on **uncertainty**.\n",
    "\n",
    "**The Core Principle:**\n",
    "\n",
    "\"It's reasonable to be optimistic in the face of uncertainty.\"\n",
    "\n",
    "UCB selects actions based on both:\n",
    "1. **Estimated value** (exploitation)\n",
    "2. **Uncertainty in that estimate** (exploration)\n",
    "\n",
    "Actions that have been tried less often have higher uncertainty, making them more attractive for exploration.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "The UCB action selection rule is:\n",
    "\n",
    "$$A_t = \\arg\\max_a \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]$$\n",
    "\n",
    "where:\n",
    "- $Q_t(a)$ = estimated value of action $a$ at time $t$ (exploitation term)\n",
    "- $c$ = exploration parameter (controls degree of exploration)\n",
    "- $t$ = current time step (total number of actions taken)\n",
    "- $N_t(a)$ = number of times action $a$ has been selected (uncertainty term)\n",
    "- $\\sqrt{\\frac{\\ln t}{N_t(a)}}$ = uncertainty bonus (larger for less-tried actions)\n",
    "\n",
    "**How the Uncertainty Bonus Works:**\n",
    "\n",
    "- Actions tried fewer times have larger $\\sqrt{\\frac{\\ln t}{N_t(a)}}$ (more uncertainty)\n",
    "- As an action is tried more, $N_t(a)$ increases and the bonus decreases\n",
    "- The $\\ln t$ term ensures all actions are eventually tried\n",
    "- The bonus naturally balances exploration and exploitation\n",
    "\n",
    "**Advantages:**\n",
    "- No random exploration - deterministic given the history\n",
    "- Automatically balances exploration and exploitation\n",
    "- Theoretical guarantees on performance (logarithmic regret)\n",
    "- Prioritizes promising actions while ensuring all are tried\n",
    "\n",
    "**Disadvantages:**\n",
    "- More complex to implement\n",
    "- Requires tuning the $c$ parameter\n",
    "- Assumes stationary reward distributions\n",
    "\n",
    "Let's implement UCB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class UCBAgent:\n",
    "    \"\"\"An agent using Upper Confidence Bound action selection.\"\"\"\n",
    "    \n",
    "    def __init__(self, k, c=2.0):\n",
    "        \"\"\"Initialize the UCB agent.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of arms\n",
    "            c: Exploration parameter (typically sqrt(2) or 2)\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.c = c\n",
    "        self.q_estimates = np.zeros(k)\n",
    "        self.action_counts = np.zeros(k)\n",
    "        self.t = 0  # Total time steps\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select action using UCB formula.\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # First, try each action at least once\n",
    "        if self.t <= self.k:\n",
    "            return self.t - 1\n",
    "        \n",
    "        # Calculate UCB values for all actions\n",
    "        ucb_values = np.zeros(self.k)\n",
    "        for a in range(self.k):\n",
    "            if self.action_counts[a] == 0:\n",
    "                # Untried actions get infinite value (shouldn't happen after initial phase)\n",
    "                ucb_values[a] = float('inf')\n",
    "            else:\n",
    "                # UCB formula: Q(a) + c * sqrt(ln(t) / N(a))\n",
    "                exploitation = self.q_estimates[a]\n",
    "                exploration = self.c * np.sqrt(np.log(self.t) / self.action_counts[a])\n",
    "                ucb_values[a] = exploitation + exploration\n",
    "        \n",
    "        # Select action with highest UCB value\n",
    "        max_ucb = np.max(ucb_values)\n",
    "        best_actions = np.where(ucb_values == max_ucb)[0]\n",
    "        return np.random.choice(best_actions)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimates after receiving a reward.\"\"\"\n",
    "        self.action_counts[action] += 1\n",
    "        n = self.action_counts[action]\n",
    "        \n",
    "        # Incremental update\n",
    "        self.q_estimates[action] += (1/n) * (reward - self.q_estimates[action])\n",
    "\n",
    "\n",
    "# Demonstrate UCB algorithm\n",
    "print(\"Upper Confidence Bound (UCB) Algorithm Demonstration\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "\n",
    "print(f\"Optimal arm: {bandit.best_arm} (mean: {bandit.get_optimal_reward():.3f})\\n\")\n",
    "\n",
    "# Test different c values\n",
    "c_values = [0.5, 1.0, 2.0, 4.0]\n",
    "ucb_results = {}\n",
    "\n",
    "for c in c_values:\n",
    "    agent = UCBAgent(k=10, c=c)\n",
    "    rewards, optimal_actions = run_experiment(agent, bandit, steps=1000)\n",
    "    \n",
    "    ucb_results[c] = {\n",
    "        'rewards': rewards,\n",
    "        'optimal': optimal_actions,\n",
    "        'avg_reward': np.mean(rewards),\n",
    "        'optimal_pct': np.mean(optimal_actions) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"c = {c:.1f}:\")\n",
    "    print(f\"  Average reward: {ucb_results[c]['avg_reward']:.3f}\")\n",
    "    print(f\"  Optimal action: {ucb_results[c]['optimal_pct']:.1f}% of the time\")\n",
    "    print(f\"  Action counts: {agent.action_counts.astype(int)}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   - Small c (0.5): Less exploration, may converge faster but risk suboptimal\")\n",
    "print(\"   - Medium c (1-2): Good balance, typical choice\")\n",
    "print(\"   - Large c (4): More exploration, ensures thorough search\")\n",
    "print(\"   - UCB naturally tries all actions but focuses on promising ones\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprehensive Comparison: All Three Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Final comprehensive comparison of all strategies\n",
    "num_experiments = 200\n",
    "steps = 1000\n",
    "\n",
    "# All strategies to compare\n",
    "all_strategies = {\n",
    "    'Greedy': {'type': 'greedy'},\n",
    "    'Œµ-greedy (Œµ=0.1)': {'type': 'epsilon', 'epsilon': 0.1},\n",
    "    'Œµ-greedy (Œµ=0.01)': {'type': 'epsilon', 'epsilon': 0.01},\n",
    "    'Optimistic (Q=5)': {'type': 'optimistic', 'init': 5.0},\n",
    "    'UCB (c=2)': {'type': 'ucb', 'c': 2.0},\n",
    "    'UCB (c=1)': {'type': 'ucb', 'c': 1.0}\n",
    "}\n",
    "\n",
    "final_results = {name: {'rewards': [], 'optimal': []} for name in all_strategies.keys()}\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(num_experiments):\n",
    "    bandit = MultiArmedBandit(k=10)\n",
    "    \n",
    "    for name, config in all_strategies.items():\n",
    "        if config['type'] == 'greedy':\n",
    "            agent = GreedyAgent(k=10)\n",
    "        elif config['type'] == 'epsilon':\n",
    "            agent = EpsilonGreedyAgent(k=10, epsilon=config['epsilon'])\n",
    "        elif config['type'] == 'optimistic':\n",
    "            agent = OptimisticGreedyAgent(k=10, initial_value=config['init'])\n",
    "        else:  # ucb\n",
    "            agent = UCBAgent(k=10, c=config['c'])\n",
    "        \n",
    "        rewards, optimal = run_experiment(agent, bandit, steps)\n",
    "        final_results[name]['rewards'].append(rewards)\n",
    "        final_results[name]['optimal'].append(optimal)\n",
    "\n",
    "# Calculate averages\n",
    "avg_final = {}\n",
    "for name in all_strategies.keys():\n",
    "    avg_final[name] = {\n",
    "        'rewards': np.mean(final_results[name]['rewards'], axis=0),\n",
    "        'optimal': np.mean(final_results[name]['optimal'], axis=0)\n",
    "    }\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 11))\n",
    "\n",
    "colors = ['red', 'green', 'lightgreen', 'purple', 'blue', 'lightblue']\n",
    "linestyles = ['-', '-', '--', '-', '-', '--']\n",
    "linewidths = [2, 2.5, 2, 2, 2.5, 2]\n",
    "\n",
    "# Plot 1: Average reward over time\n",
    "for (name, color, ls, lw) in zip(all_strategies.keys(), colors, linestyles, linewidths):\n",
    "    ax1.plot(avg_final[name]['rewards'], linewidth=lw, color=color, \n",
    "             linestyle=ls, alpha=0.8, label=name)\n",
    "\n",
    "ax1.set_xlabel('Steps', fontsize=13)\n",
    "ax1.set_ylabel('Average Reward', fontsize=13)\n",
    "ax1.set_title('Multi-Armed Bandit: Complete Strategy Comparison - Average Reward', \n",
    "              fontsize=15, fontweight='bold')\n",
    "ax1.legend(loc='lower right', fontsize=11, ncol=2)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Percentage of optimal actions\n",
    "for (name, color, ls, lw) in zip(all_strategies.keys(), colors, linestyles, linewidths):\n",
    "    ax2.plot(avg_final[name]['optimal'] * 100, linewidth=lw, color=color, \n",
    "             linestyle=ls, alpha=0.8, label=name)\n",
    "\n",
    "ax2.set_xlabel('Steps', fontsize=13)\n",
    "ax2.set_ylabel('% Optimal Action', fontsize=13)\n",
    "ax2.set_title('Multi-Armed Bandit: Complete Strategy Comparison - Optimal Action Selection', \n",
    "              fontsize=15, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=11, ncol=2)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed performance table\n",
    "print(\"\\nüìä Final Performance Comparison (averaged over {} experiments):\".format(num_experiments))\n",
    "print(\"=\"*95)\n",
    "print(f\"{'Strategy':<25} {'Avg Reward':<15} {'Total Optimal %':<18} {'Early (0-100)':<18} {'Late (900-1000)'}\")\n",
    "print(\"=\"*95)\n",
    "\n",
    "# Sort by average reward for ranking\n",
    "sorted_strategies = sorted(all_strategies.keys(), \n",
    "                          key=lambda x: np.mean(avg_final[x]['rewards']), \n",
    "                          reverse=True)\n",
    "\n",
    "for rank, name in enumerate(sorted_strategies, 1):\n",
    "    avg_reward = np.mean(avg_final[name]['rewards'])\n",
    "    avg_optimal = np.mean(avg_final[name]['optimal']) * 100\n",
    "    early_optimal = np.mean(avg_final[name]['optimal'][:100]) * 100\n",
    "    late_optimal = np.mean(avg_final[name]['optimal'][-100:]) * 100\n",
    "    \n",
    "    rank_str = f\"#{rank} {name}\"\n",
    "    print(f\"{rank_str:<25} {avg_reward:<15.3f} {avg_optimal:<18.1f} {early_optimal:<18.1f} {late_optimal:.1f}%\")\n",
    "\n",
    "print(\"=\"*95)\n",
    "\n",
    "print(\"\\nüèÜ Final Rankings and Insights:\\n\")\n",
    "print(\"1. UCB (c=2) typically performs best overall\")\n",
    "print(\"   - Principled exploration based on uncertainty\")\n",
    "print(\"   - Strong theoretical guarantees\")\n",
    "print(\"   - No random exploration needed\\n\")\n",
    "\n",
    "print(\"2. Œµ-greedy (Œµ=0.1) is a close second\")\n",
    "print(\"   - Simple and effective\")\n",
    "print(\"   - Works well in non-stationary environments\")\n",
    "print(\"   - Easy to implement and tune\\n\")\n",
    "\n",
    "print(\"3. Optimistic initialization works well early\")\n",
    "print(\"   - Good for stationary problems\")\n",
    "print(\"   - No parameter tuning needed\")\n",
    "print(\"   - Exploration decreases over time\\n\")\n",
    "\n",
    "print(\"4. Pure greedy fails dramatically\")\n",
    "print(\"   - Gets stuck on first good option\")\n",
    "print(\"   - Demonstrates importance of exploration\\n\")\n",
    "\n",
    "print(\"üí° Key Takeaway:\")\n",
    "print(\"   The exploration-exploitation dilemma is fundamental to RL.\")\n",
    "print(\"   Different strategies offer different trade-offs, but all successful\")\n",
    "print(\"   approaches balance trying new things with using what works.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mdp'></a>\n",
    "### Core Terminology and MDP Framework\n",
    "\n",
    "Now that we've explored the multi-armed bandit problem, let's expand our understanding to more complex reinforcement learning scenarios. We'll introduce the fundamental terminology and the Markov Decision Process (MDP) framework that underlies most RL algorithms.\n",
    "\n",
    "#### Fundamental RL Terminology\n",
    "\n",
    "Before diving into MDPs, let's clearly define the core concepts that appear in every RL problem:\n",
    "\n",
    "**1. Agent**\n",
    "- The learner and decision maker\n",
    "- Observes the environment and takes actions\n",
    "- Goal: Learn a policy that maximizes cumulative reward\n",
    "- Example: A robot, a game-playing AI, a trading algorithm\n",
    "\n",
    "**2. Environment**\n",
    "- Everything outside the agent\n",
    "- Responds to the agent's actions\n",
    "- Provides observations and rewards\n",
    "- Example: The physical world, a game board, a stock market\n",
    "\n",
    "**3. State (s)**\n",
    "- A representation of the current situation\n",
    "- Contains all relevant information for decision making\n",
    "- Can be fully observable or partially observable\n",
    "- Example: Robot's position and velocity, chess board configuration, account balance\n",
    "\n",
    "**4. Action (a)**\n",
    "- A choice the agent can make\n",
    "- Can be discrete (finite set) or continuous (infinite range)\n",
    "- Available actions may depend on the current state\n",
    "- Example: Move left/right, place chess piece, buy/sell/hold\n",
    "\n",
    "**5. Reward (r)**\n",
    "- Immediate feedback signal from the environment\n",
    "- Scalar value indicating how good/bad an action was\n",
    "- The agent's goal is to maximize cumulative reward\n",
    "- Example: +1 for reaching goal, -1 for collision, profit/loss amount\n",
    "\n",
    "**The Agent-Environment Interface:**\n",
    "\n",
    "```\n",
    "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ  Agent  ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "          ‚îÇ\n",
    "    action‚îÇ ‚Üì\n",
    "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ Environment ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "          ‚îÇ\n",
    "  state,  ‚îÇ ‚Üë\n",
    "  reward  ‚îÇ\n",
    "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ  Agent  ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "Let's implement a simple environment class to demonstrate these concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GridWorldEnvironment:\n",
    "    \"\"\"A simple grid world environment demonstrating RL concepts.\n",
    "    \n",
    "    The agent navigates a 2D grid to reach a goal while avoiding obstacles.\n",
    "    This demonstrates: states (grid positions), actions (movements),\n",
    "    rewards (goal/obstacle/step), and the agent-environment interaction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=5, goal_pos=(4, 4), obstacles=None):\n",
    "        \"\"\"Initialize the grid world.\n",
    "        \n",
    "        Args:\n",
    "            grid_size: Size of the square grid\n",
    "            goal_pos: (row, col) position of the goal\n",
    "            obstacles: List of (row, col) positions that are obstacles\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.goal_pos = goal_pos\n",
    "        self.obstacles = obstacles if obstacles else [(2, 2), (3, 2)]\n",
    "        \n",
    "        # Action space: 0=up, 1=right, 2=down, 3=left\n",
    "        self.actions = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "        self.action_effects = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        \n",
    "        # Initialize state\n",
    "        self.agent_pos = None\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state.\n",
    "        \n",
    "        Returns:\n",
    "            state: Initial state (agent position)\n",
    "        \"\"\"\n",
    "        # Start at top-left corner\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.agent_pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute an action and return the result.\n",
    "        \n",
    "        Args:\n",
    "            action: Integer 0-3 representing direction\n",
    "            \n",
    "        Returns:\n",
    "            next_state: New agent position\n",
    "            reward: Reward for this transition\n",
    "            done: Whether episode is finished\n",
    "            info: Additional information (dict)\n",
    "        \"\"\"\n",
    "        # Calculate new position\n",
    "        delta = self.action_effects[action]\n",
    "        new_row = self.agent_pos[0] + delta[0]\n",
    "        new_col = self.agent_pos[1] + delta[1]\n",
    "        new_pos = (new_row, new_col)\n",
    "        \n",
    "        # Check if new position is valid\n",
    "        if self._is_valid_position(new_pos):\n",
    "            self.agent_pos = new_pos\n",
    "        # If invalid (wall), agent stays in place\n",
    "        \n",
    "        # Calculate reward and check if done\n",
    "        reward, done, info = self._get_reward_and_done()\n",
    "        \n",
    "        return self.agent_pos, reward, done, info\n",
    "    \n",
    "    def _is_valid_position(self, pos):\n",
    "        \"\"\"Check if position is within bounds and not an obstacle.\"\"\"\n",
    "        row, col = pos\n",
    "        \n",
    "        # Check bounds\n",
    "        if row < 0 or row >= self.grid_size or col < 0 or col >= self.grid_size:\n",
    "            return False\n",
    "        \n",
    "        # Check obstacles\n",
    "        if pos in self.obstacles:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _get_reward_and_done(self):\n",
    "        \"\"\"Calculate reward and check if episode is done.\"\"\"\n",
    "        info = {}\n",
    "        \n",
    "        # Check if reached goal\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            return 10.0, True, {'reason': 'goal_reached'}\n",
    "        \n",
    "        # Check if hit obstacle (shouldn't happen with valid position check)\n",
    "        if self.agent_pos in self.obstacles:\n",
    "            return -10.0, True, {'reason': 'obstacle_hit'}\n",
    "        \n",
    "        # Small negative reward for each step (encourages efficiency)\n",
    "        return -0.1, False, {'reason': 'step'}\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Display the current state of the grid world.\"\"\"\n",
    "        grid = [['.' for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n",
    "        \n",
    "        # Mark obstacles\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs[0]][obs[1]] = 'X'\n",
    "        \n",
    "        # Mark goal\n",
    "        grid[self.goal_pos[0]][self.goal_pos[1]] = 'G'\n",
    "        \n",
    "        # Mark agent\n",
    "        grid[self.agent_pos[0]][self.agent_pos[1]] = 'A'\n",
    "        \n",
    "        # Print grid\n",
    "        print('\\n' + '‚îÄ' * (self.grid_size * 2 + 1))\n",
    "        for row in grid:\n",
    "            print('‚îÇ' + ' '.join(row) + '‚îÇ')\n",
    "        print('‚îÄ' * (self.grid_size * 2 + 1))\n",
    "        print(f\"Agent at: {self.agent_pos}\")\n",
    "\n",
    "\n",
    "# Demonstrate the environment and core concepts\n",
    "print(\"Demonstrating Core RL Concepts with Grid World\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnvironment(grid_size=5)\n",
    "\n",
    "print(\"\\nüåç ENVIRONMENT: 5x5 Grid World\")\n",
    "print(\"   - Goal: Reach position (4,4) marked with 'G'\")\n",
    "print(\"   - Obstacles: Positions marked with 'X'\")\n",
    "print(\"   - Agent: Current position marked with 'A'\")\n",
    "\n",
    "print(\"\\nüìç STATE: Agent's position in the grid (row, col)\")\n",
    "print(f\"   - Initial state: {env.agent_pos}\")\n",
    "print(f\"   - State space size: {env.grid_size * env.grid_size} possible positions\")\n",
    "\n",
    "print(\"\\nüéÆ ACTIONS: Four possible movements\")\n",
    "for i, action_name in enumerate(env.actions):\n",
    "    print(f\"   - Action {i}: {action_name}\")\n",
    "\n",
    "print(\"\\nüéÅ REWARDS:\")\n",
    "print(\"   - Reach goal: +10.0\")\n",
    "print(\"   - Each step: -0.1 (encourages efficiency)\")\n",
    "print(\"   - Hit wall: Agent stays in place\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nInitial State:\")\n",
    "env.render()\n",
    "\n",
    "# Simulate a few steps\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Simulating Agent-Environment Interaction:\\n\")\n",
    "\n",
    "actions_to_take = [1, 1, 2, 2, 1, 1, 2, 2]  # Path to goal\n",
    "total_reward = 0\n",
    "\n",
    "for step, action in enumerate(actions_to_take, 1):\n",
    "    action_name = env.actions[action]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    print(f\"Step {step}:\")\n",
    "    print(f\"  Action: {action_name}\")\n",
    "    print(f\"  New State: {next_state}\")\n",
    "    print(f\"  Reward: {reward:+.1f}\")\n",
    "    print(f\"  Total Reward: {total_reward:+.1f}\")\n",
    "    print(f\"  Done: {done}\")\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\\n‚úì Episode finished: {info['reason']}\")\n",
    "        env.render()\n",
    "        break\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   1. STATE: Represents where the agent is\")\n",
    "print(\"   2. ACTION: What the agent chooses to do\")\n",
    "print(\"   3. REWARD: Feedback on how good the action was\")\n",
    "print(\"   4. ENVIRONMENT: Determines next state and reward\")\n",
    "print(\"   5. AGENT: Would learn which actions to take in each state\")\n",
    "print(\"\\n   This interaction loop is the foundation of all RL!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Agent-Environment Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a visualization of the agent-environment interaction\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_grid_world(env, trajectory=None):\n",
    "    \"\"\"Visualize the grid world and optionally a trajectory.\n",
    "    \n",
    "    Args:\n",
    "        env: GridWorldEnvironment instance\n",
    "        trajectory: List of (state, action) tuples to visualize\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Draw grid\n",
    "    for i in range(env.grid_size + 1):\n",
    "        ax.plot([0, env.grid_size], [i, i], 'k-', linewidth=1)\n",
    "        ax.plot([i, i], [0, env.grid_size], 'k-', linewidth=1)\n",
    "    \n",
    "    # Draw obstacles\n",
    "    for obs in env.obstacles:\n",
    "        rect = patches.Rectangle((obs[1], env.grid_size - obs[0] - 1), 1, 1, \n",
    "                                 linewidth=2, edgecolor='black', facecolor='gray', alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(obs[1] + 0.5, env.grid_size - obs[0] - 0.5, 'X', \n",
    "               ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # Draw goal\n",
    "    goal = env.goal_pos\n",
    "    rect = patches.Rectangle((goal[1], env.grid_size - goal[0] - 1), 1, 1, \n",
    "                             linewidth=2, edgecolor='green', facecolor='lightgreen', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(goal[1] + 0.5, env.grid_size - goal[0] - 0.5, 'G', \n",
    "           ha='center', va='center', fontsize=20, fontweight='bold', color='darkgreen')\n",
    "    \n",
    "    # Draw start position\n",
    "    start = (0, 0)\n",
    "    rect = patches.Rectangle((start[1], env.grid_size - start[0] - 1), 1, 1, \n",
    "                             linewidth=2, edgecolor='blue', facecolor='lightblue', alpha=0.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(start[1] + 0.5, env.grid_size - start[0] - 0.5, 'S', \n",
    "           ha='center', va='center', fontsize=20, fontweight='bold', color='darkblue')\n",
    "    \n",
    "    # Draw trajectory if provided\n",
    "    if trajectory:\n",
    "        for i, (state, action) in enumerate(trajectory):\n",
    "            row, col = state\n",
    "            # Convert to plot coordinates\n",
    "            x = col + 0.5\n",
    "            y = env.grid_size - row - 0.5\n",
    "            \n",
    "            # Draw step number\n",
    "            ax.text(x, y, str(i+1), ha='center', va='center', \n",
    "                   fontsize=12, color='red', fontweight='bold',\n",
    "                   bbox=dict(boxstyle='circle', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Draw arrow for action\n",
    "            if action is not None:\n",
    "                delta = env.action_effects[action]\n",
    "                dx = delta[1] * 0.3\n",
    "                dy = -delta[0] * 0.3  # Negative because y-axis is flipped\n",
    "                ax.arrow(x, y, dx, dy, head_width=0.15, head_length=0.1, \n",
    "                        fc='red', ec='red', alpha=0.6, linewidth=2)\n",
    "    \n",
    "    ax.set_xlim(0, env.grid_size)\n",
    "    ax.set_ylim(0, env.grid_size)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xticks(range(env.grid_size + 1))\n",
    "    ax.set_yticks(range(env.grid_size + 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title('Grid World Environment\\nS=Start, G=Goal, X=Obstacle', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# Visualize the environment\n",
    "env = GridWorldEnvironment(grid_size=5)\n",
    "\n",
    "# Create a sample trajectory\n",
    "trajectory = []\n",
    "state = env.reset()\n",
    "actions = [1, 1, 2, 2, 1, 1, 2, 2]  # Path to goal\n",
    "\n",
    "for action in actions:\n",
    "    trajectory.append((state, action))\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        trajectory.append((state, None))  # Final state, no action\n",
    "        break\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = visualize_grid_world(env, trajectory)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization shows:\")\n",
    "print(\"   - Blue 'S': Starting state\")\n",
    "print(\"   - Green 'G': Goal state\")\n",
    "print(\"   - Gray 'X': Obstacles\")\n",
    "print(\"   - Red numbers: Step sequence\")\n",
    "print(\"   - Red arrows: Actions taken\")\n",
    "print(\"\\nThis illustrates how the agent navigates through states\")\n",
    "print(\"by taking actions to reach the goal!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Decision Processes (MDPs)\n",
    "\n",
    "**The Mathematical Framework for Reinforcement Learning**\n",
    "\n",
    "Now that we understand the basic terminology, let's formalize these concepts using the **Markov Decision Process (MDP)** framework. MDPs provide the mathematical foundation for most reinforcement learning algorithms.\n",
    "\n",
    "**What is an MDP?**\n",
    "\n",
    "A Markov Decision Process is a mathematical model for sequential decision-making under uncertainty. It's defined by a tuple $(S, A, P, R, \\gamma)$:\n",
    "\n",
    "**MDP Components:**\n",
    "\n",
    "1. **$S$: State Space**\n",
    "   - Set of all possible states\n",
    "   - Can be finite (grid positions) or infinite (continuous positions)\n",
    "   - Example: $S = \\{(0,0), (0,1), ..., (4,4)\\}$ for 5√ó5 grid\n",
    "\n",
    "2. **$A$: Action Space**\n",
    "   - Set of all possible actions\n",
    "   - Can be state-dependent: $A(s)$ = actions available in state $s$\n",
    "   - Example: $A = \\{\\text{UP, DOWN, LEFT, RIGHT}\\}$\n",
    "\n",
    "3. **$P$: Transition Probability Function**\n",
    "   - $P(s'|s,a)$ = probability of reaching state $s'$ from state $s$ after taking action $a$\n",
    "   - Defines the dynamics of the environment\n",
    "   - Must satisfy: $\\sum_{s' \\in S} P(s'|s,a) = 1$ for all $s, a$\n",
    "   - Example: In deterministic grid world, $P(s'|s,a) = 1$ for one $s'$ and 0 for others\n",
    "\n",
    "4. **$R$: Reward Function**\n",
    "   - $R(s, a, s')$ = immediate reward for transition from $s$ to $s'$ via action $a$\n",
    "   - Sometimes simplified as $R(s)$ or $R(s,a)$\n",
    "   - Defines the objective the agent should optimize\n",
    "   - Example: $R(s_{goal}) = +10$, $R(s_{other}) = -0.1$\n",
    "\n",
    "5. **$\\gamma$: Discount Factor**\n",
    "   - Value between 0 and 1 that determines importance of future rewards\n",
    "   - $\\gamma = 0$: Only immediate rewards matter (myopic)\n",
    "   - $\\gamma = 1$: All future rewards equally important (far-sighted)\n",
    "   - Typical values: 0.9, 0.95, 0.99\n",
    "\n",
    "**The Markov Property**\n",
    "\n",
    "The \"Markov\" in MDP refers to the **Markov Property** (also called the memoryless property):\n",
    "\n",
    "$P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1} | s_t, a_t)$\n",
    "\n",
    "**In plain English:** The future depends only on the present, not on the past.\n",
    "\n",
    "**Why is the Markov Property Important?**\n",
    "\n",
    "1. **Tractability**: Makes the problem computationally feasible\n",
    "   - Don't need to remember entire history\n",
    "   - State contains all relevant information\n",
    "\n",
    "2. **Simplifies Learning**: Agent only needs to learn from current state\n",
    "   - No need to condition on past states\n",
    "   - Enables dynamic programming and temporal difference learning\n",
    "\n",
    "3. **Theoretical Guarantees**: Most RL theory assumes Markov property\n",
    "   - Convergence proofs rely on it\n",
    "   - Optimal policies exist under this assumption\n",
    "\n",
    "**Example - Chess:**\n",
    "- **Markov**: Current board position is the state (contains all relevant info)\n",
    "- **Non-Markov**: Only knowing the last move (need full game history)\n",
    "\n",
    "**When the Markov Property Doesn't Hold:**\n",
    "\n",
    "In practice, many problems are **Partially Observable MDPs (POMDPs)** where:\n",
    "- Agent doesn't see the full state\n",
    "- Must infer state from observations\n",
    "- Example: Robot with limited sensors, poker (can't see opponent's cards)\n",
    "\n",
    "Let's implement an MDP simulator to demonstrate these concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimpleMDP:\n",
    "    \"\"\"A simple MDP simulator with explicit transition probabilities.\n",
    "    \n",
    "    This class demonstrates the core MDP components:\n",
    "    - State space S\n",
    "    - Action space A  \n",
    "    - Transition probabilities P(s'|s,a)\n",
    "    - Reward function R(s,a,s')\n",
    "    - Discount factor gamma\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, states, actions, transitions, rewards, gamma=0.9):\n",
    "        \"\"\"Initialize the MDP.\n",
    "        \n",
    "        Args:\n",
    "            states: List of state identifiers\n",
    "            actions: List of action identifiers\n",
    "            transitions: Dict mapping (state, action) -> {next_state: probability}\n",
    "            rewards: Dict mapping (state, action, next_state) -> reward\n",
    "            gamma: Discount factor (0 to 1)\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.current_state = None\n",
    "        \n",
    "        # Verify transition probabilities sum to 1\n",
    "        self._verify_transitions()\n",
    "    \n",
    "    def _verify_transitions(self):\n",
    "        \"\"\"Verify that transition probabilities are valid.\"\"\"\n",
    "        for (state, action), next_states in self.transitions.items():\n",
    "            total_prob = sum(next_states.values())\n",
    "            if not np.isclose(total_prob, 1.0):\n",
    "                raise ValueError(\n",
    "                    f\"Transition probabilities for ({state}, {action}) sum to {total_prob}, not 1.0\"\n",
    "                )\n",
    "    \n",
    "    def reset(self, initial_state=None):\n",
    "        \"\"\"Reset to initial state.\n",
    "        \n",
    "        Args:\n",
    "            initial_state: Starting state (random if None)\n",
    "            \n",
    "        Returns:\n",
    "            state: The initial state\n",
    "        \"\"\"\n",
    "        if initial_state is None:\n",
    "            self.current_state = np.random.choice(self.states)\n",
    "        else:\n",
    "            self.current_state = initial_state\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and transition to next state.\n",
    "        \n",
    "        Args:\n",
    "            action: Action to take\n",
    "            \n",
    "        Returns:\n",
    "            next_state: The resulting state\n",
    "            reward: Reward received\n",
    "            info: Additional information\n",
    "        \"\"\"\n",
    "        if self.current_state is None:\n",
    "            raise ValueError(\"Must call reset() before step()\")\n",
    "        \n",
    "        # Get transition probabilities for current state and action\n",
    "        next_state_probs = self.transitions.get((self.current_state, action), {})\n",
    "        \n",
    "        if not next_state_probs:\n",
    "            raise ValueError(f\"No transitions defined for state {self.current_state}, action {action}\")\n",
    "        \n",
    "        # Sample next state according to transition probabilities\n",
    "        next_states = list(next_state_probs.keys())\n",
    "        probabilities = list(next_state_probs.values())\n",
    "        next_state = np.random.choice(next_states, p=probabilities)\n",
    "        \n",
    "        # Get reward\n",
    "        reward = self.rewards.get((self.current_state, action, next_state), 0.0)\n",
    "        \n",
    "        # Update current state\n",
    "        old_state = self.current_state\n",
    "        self.current_state = next_state\n",
    "        \n",
    "        info = {\n",
    "            'old_state': old_state,\n",
    "            'action': action,\n",
    "            'probability': next_state_probs[next_state]\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, info\n",
    "    \n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\"Get P(next_state | state, action).\"\"\"\n",
    "        return self.transitions.get((state, action), {}).get(next_state, 0.0)\n",
    "    \n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Get R(state, action, next_state).\"\"\"\n",
    "        return self.rewards.get((state, action, next_state), 0.0)\n",
    "\n",
    "\n",
    "# Create a simple 2x2 grid world MDP\n",
    "print(\"Simple 2x2 Grid World MDP\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define the MDP components\n",
    "# States: positions in 2x2 grid\n",
    "states = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\n",
    "\n",
    "# Actions: move right or down\n",
    "actions = ['RIGHT', 'DOWN']\n",
    "\n",
    "# Transitions: P(s'|s,a)\n",
    "# In this simple example, actions are deterministic\n",
    "transitions = {\n",
    "    ('(0,0)', 'RIGHT'): {'(0,1)': 1.0},\n",
    "    ('(0,0)', 'DOWN'): {'(1,0)': 1.0},\n",
    "    ('(0,1)', 'RIGHT'): {'(0,1)': 1.0},  # Hit wall, stay in place\n",
    "    ('(0,1)', 'DOWN'): {'(1,1)': 1.0},\n",
    "    ('(1,0)', 'RIGHT'): {'(1,1)': 1.0},\n",
    "    ('(1,0)', 'DOWN'): {'(1,0)': 1.0},  # Hit wall, stay in place\n",
    "    ('(1,1)', 'RIGHT'): {'(1,1)': 1.0},  # Goal state, stay\n",
    "    ('(1,1)', 'DOWN'): {'(1,1)': 1.0},   # Goal state, stay\n",
    "}\n",
    "\n",
    "# Rewards: R(s,a,s')\n",
    "rewards = {\n",
    "    ('(0,0)', 'RIGHT', '(0,1)'): -1,\n",
    "    ('(0,0)', 'DOWN', '(1,0)'): -1,\n",
    "    ('(0,1)', 'RIGHT', '(0,1)'): -1,\n",
    "    ('(0,1)', 'DOWN', '(1,1)'): 10,  # Reaching goal\n",
    "    ('(1,0)', 'RIGHT', '(1,1)'): 10,  # Reaching goal\n",
    "    ('(1,0)', 'DOWN', '(1,0)'): -1,\n",
    "    ('(1,1)', 'RIGHT', '(1,1)'): 0,  # At goal\n",
    "    ('(1,1)', 'DOWN', '(1,1)'): 0,   # At goal\n",
    "}\n",
    "\n",
    "# Create MDP\n",
    "mdp = SimpleMDP(states, actions, transitions, rewards, gamma=0.9)\n",
    "\n",
    "print(\"MDP Components:\")\n",
    "print(f\"\\n1. State Space S: {states}\")\n",
    "print(f\"   |S| = {len(states)} states\")\n",
    "\n",
    "print(f\"\\n2. Action Space A: {actions}\")\n",
    "print(f\"   |A| = {len(actions)} actions\")\n",
    "\n",
    "print(f\"\\n3. Discount Factor Œ≥: {mdp.gamma}\")\n",
    "\n",
    "print(\"\\n4. Transition Function P(s'|s,a):\")\n",
    "print(\"   Example: P((0,1) | (0,0), RIGHT) =\", mdp.get_transition_prob('(0,0)', 'RIGHT', '(0,1)'))\n",
    "print(\"   Example: P((1,0) | (0,0)', DOWN) =\", mdp.get_transition_prob('(0,0)', 'DOWN', '(1,0)'))\n",
    "\n",
    "print(\"\\n5. Reward Function R(s,a,s'):\")\n",
    "print(\"   Example: R((0,0), RIGHT, (0,1)) =\", mdp.get_reward('(0,0)', 'RIGHT', '(0,1)'))\n",
    "print(\"   Example: R((0,1), DOWN, (1,1)) =\", mdp.get_reward('(0,1)', 'DOWN', '(1,1)'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nGrid Layout:\")\n",
    "print(\"  (0,0) ‚Üí (0,1)\")\n",
    "print(\"    ‚Üì       ‚Üì\")\n",
    "print(\"  (1,0) ‚Üí (1,1) [GOAL]\")\n",
    "print(\"\\nGoal: Reach (1,1) from (0,0)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulate episodes in the MDP\n",
    "print(\"Simulating MDP Episodes\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run a few episodes with random actions\n",
    "num_episodes = 3\n",
    "max_steps = 5\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    print(f\"\\nEpisode {episode}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    state = mdp.reset(initial_state='(0,0)')\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(f\"Initial state: {state}\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Random action selection\n",
    "        action = np.random.choice(mdp.actions)\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, info = mdp.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        print(f\"  Step {step+1}: {info['old_state']} --[{action}]--> {next_state}\")\n",
    "        print(f\"           Reward: {reward:+.0f}, Total: {total_reward:+.0f}\")\n",
    "        \n",
    "        # Check if reached goal\n",
    "        if next_state == '(1,1)':\n",
    "            print(f\"\\n  ‚úì Reached goal in {step+1} steps!\")\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    if state != '(1,1)':\n",
    "        print(f\"\\n  ‚úó Did not reach goal in {max_steps} steps\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Key MDP Concepts Demonstrated:\")\n",
    "print(\"   1. States: Discrete positions in the grid\")\n",
    "print(\"   2. Actions: RIGHT and DOWN movements\")\n",
    "print(\"   3. Transitions: Deterministic (probability = 1.0)\")\n",
    "print(\"   4. Rewards: Negative for steps, positive for goal\")\n",
    "print(\"   5. Markov Property: Next state depends only on current state and action\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing State Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the MDP as a state transition diagram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def visualize_mdp_transitions(mdp):\n",
    "    \"\"\"Create a visualization of MDP state transitions.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Define state positions for visualization\n",
    "    state_positions = {\n",
    "        '(0,0)': (1, 3),\n",
    "        '(0,1)': (3, 3),\n",
    "        '(1,0)': (1, 1),\n",
    "        '(1,1)': (3, 1)\n",
    "    }\n",
    "    \n",
    "    # Draw states\n",
    "    for state, (x, y) in state_positions.items():\n",
    "        if state == '(1,1)':\n",
    "            # Goal state - green\n",
    "            circle = plt.Circle((x, y), 0.3, color='lightgreen', ec='darkgreen', linewidth=3)\n",
    "            ax.add_patch(circle)\n",
    "            ax.text(x, y, state + '\\nGOAL', ha='center', va='center', \n",
    "                   fontsize=11, fontweight='bold')\n",
    "        elif state == '(0,0)':\n",
    "            # Start state - blue\n",
    "            circle = plt.Circle((x, y), 0.3, color='lightblue', ec='darkblue', linewidth=3)\n",
    "            ax.add_patch(circle)\n",
    "            ax.text(x, y, state + '\\nSTART', ha='center', va='center', \n",
    "                   fontsize=11, fontweight='bold')\n",
    "        else:\n",
    "            # Regular state - white\n",
    "            circle = plt.Circle((x, y), 0.3, color='white', ec='black', linewidth=2)\n",
    "            ax.add_patch(circle)\n",
    "            ax.text(x, y, state, ha='center', va='center', \n",
    "                   fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Draw transitions\n",
    "    for (state, action), next_states in mdp.transitions.items():\n",
    "        for next_state, prob in next_states.items():\n",
    "            if state == next_state:\n",
    "                # Self-loop (hitting wall or at goal)\n",
    "                continue\n",
    "            \n",
    "            x1, y1 = state_positions[state]\n",
    "            x2, y2 = state_positions[next_state]\n",
    "            \n",
    "            # Calculate arrow position\n",
    "            dx = x2 - x1\n",
    "            dy = y2 - y1\n",
    "            length = np.sqrt(dx**2 + dy**2)\n",
    "            \n",
    "            # Normalize and shorten to account for circle radius\n",
    "            dx_norm = dx / length\n",
    "            dy_norm = dy / length\n",
    "            \n",
    "            start_x = x1 + dx_norm * 0.35\n",
    "            start_y = y1 + dy_norm * 0.35\n",
    "            end_x = x2 - dx_norm * 0.35\n",
    "            end_y = y2 - dy_norm * 0.35\n",
    "            \n",
    "            # Get reward for this transition\n",
    "            reward = mdp.get_reward(state, action, next_state)\n",
    "            \n",
    "            # Color based on action\n",
    "            color = 'blue' if action == 'RIGHT' else 'red'\n",
    "            \n",
    "            # Draw arrow\n",
    "            ax.annotate('', xy=(end_x, end_y), xytext=(start_x, start_y),\n",
    "                       arrowprops=dict(arrowstyle='->', lw=2, color=color, alpha=0.7))\n",
    "            \n",
    "            # Add label\n",
    "            mid_x = (start_x + end_x) / 2\n",
    "            mid_y = (start_y + end_y) / 2\n",
    "            label = f\"{action}\\nR={reward:+.0f}\"\n",
    "            ax.text(mid_x, mid_y, label, ha='center', va='center',\n",
    "                   fontsize=9, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Add legend\n",
    "    right_patch = mpatches.Patch(color='blue', label='RIGHT action')\n",
    "    down_patch = mpatches.Patch(color='red', label='DOWN action')\n",
    "    ax.legend(handles=[right_patch, down_patch], loc='upper right', fontsize=11)\n",
    "    \n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('MDP State Transition Diagram\\n(Arrows show actions and rewards)', \n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = visualize_mdp_transitions(mdp)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Transition Diagram shows:\")\n",
    "print(\"   - Blue arrows: RIGHT actions\")\n",
    "print(\"   - Red arrows: DOWN actions\")\n",
    "print(\"   - Labels show: Action name and Reward\")\n",
    "print(\"   - Green circle: Goal state (1,1)\")\n",
    "print(\"   - Blue circle: Start state (0,0)\")\n",
    "print(\"\\nThis visualizes the complete MDP structure:\")\n",
    "print(\"how states connect through actions and what rewards are received!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discounted Return and Value Functions\n",
    "\n",
    "**From Immediate Rewards to Long-Term Value**\n",
    "\n",
    "In reinforcement learning, we don't just care about immediate rewards - we want to maximize the **total reward over time**. This leads us to the concepts of return and value functions.\n",
    "\n",
    "**The Return (Cumulative Reward)**\n",
    "\n",
    "The **return** $G_t$ at time $t$ is the total discounted reward from that point forward:\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ...$\n",
    "\n",
    "Or more compactly:\n",
    "\n",
    "$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
    "\n",
    "**The Discount Factor Œ≥ (Gamma)**\n",
    "\n",
    "The discount factor $\\gamma \\in [0, 1]$ determines how much we value future rewards:\n",
    "\n",
    "- **$\\gamma = 0$**: Only immediate reward matters (myopic)\n",
    "  - $G_t = R_{t+1}$\n",
    "  - Agent is short-sighted\n",
    "\n",
    "- **$\\gamma = 1$**: All future rewards equally important (far-sighted)\n",
    "  - $G_t = R_{t+1} + R_{t+2} + R_{t+3} + ...$\n",
    "  - Can lead to infinite returns in continuing tasks\n",
    "\n",
    "- **$\\gamma \\in (0, 1)$**: Balance between immediate and future rewards\n",
    "  - Typical values: 0.9, 0.95, 0.99\n",
    "  - Ensures finite returns even in infinite horizons\n",
    "\n",
    "**Why Discount Future Rewards?**\n",
    "\n",
    "1. **Mathematical Convenience**: Ensures convergence for infinite horizons\n",
    "2. **Uncertainty**: Future is uncertain, so future rewards are less reliable\n",
    "3. **Preference**: Often prefer rewards sooner rather than later\n",
    "4. **Computational**: Makes the problem tractable\n",
    "\n",
    "**Example - Effect of Gamma:**\n",
    "\n",
    "Suppose we receive rewards: [1, 1, 1, 1, 1]\n",
    "\n",
    "- $\\gamma = 0.0$: $G = 1$ (only first reward)\n",
    "- $\\gamma = 0.5$: $G = 1 + 0.5 + 0.25 + 0.125 + 0.0625 = 1.9375$\n",
    "- $\\gamma = 0.9$: $G = 1 + 0.9 + 0.81 + 0.729 + 0.6561 = 4.0951$\n",
    "- $\\gamma = 1.0$: $G = 5$ (all rewards equally)\n",
    "\n",
    "Let's implement a function to calculate discounted returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_discounted_return(rewards, gamma):\n",
    "    \"\"\"Calculate the discounted return for a sequence of rewards.\n",
    "    \n",
    "    Args:\n",
    "        rewards: List or array of rewards [r1, r2, r3, ...]\n",
    "        gamma: Discount factor (0 to 1)\n",
    "        \n",
    "    Returns:\n",
    "        G: The discounted return\n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    for t, reward in enumerate(rewards):\n",
    "        G += (gamma ** t) * reward\n",
    "    return G\n",
    "\n",
    "\n",
    "def calculate_returns_to_go(rewards, gamma):\n",
    "    \"\"\"Calculate return-to-go for each time step.\n",
    "    \n",
    "    Return-to-go at time t is the discounted sum of rewards from t onward.\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards [r1, r2, r3, ...]\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        returns: List of returns-to-go [G0, G1, G2, ...]\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    \n",
    "    # Calculate backwards for efficiency\n",
    "    for reward in reversed(rewards):\n",
    "        G = reward + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    return returns\n",
    "\n",
    "\n",
    "# Demonstrate discounted return calculation\n",
    "print(\"Discounted Return Calculation\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example reward sequence\n",
    "rewards = [1, 1, 1, 1, 1]\n",
    "print(f\"Reward sequence: {rewards}\\n\")\n",
    "\n",
    "# Calculate for different gamma values\n",
    "gamma_values = [0.0, 0.5, 0.9, 0.99, 1.0]\n",
    "\n",
    "print(\"Effect of Discount Factor Œ≥:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Œ≥':<10} {'Discounted Return':<20} {'Interpretation'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    G = calculate_discounted_return(rewards, gamma)\n",
    "    \n",
    "    if gamma == 0.0:\n",
    "        interp = \"Only immediate reward\"\n",
    "    elif gamma == 1.0:\n",
    "        interp = \"All rewards equally\"\n",
    "    elif gamma < 0.5:\n",
    "        interp = \"Very myopic\"\n",
    "    elif gamma < 0.9:\n",
    "        interp = \"Moderately far-sighted\"\n",
    "    else:\n",
    "        interp = \"Very far-sighted\"\n",
    "    \n",
    "    print(f\"{gamma:<10.2f} {G:<20.4f} {interp}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Example with varying rewards\n",
    "print(\"\\nExample with Varying Rewards:\\n\")\n",
    "rewards2 = [1, 2, 3, 4, 5]\n",
    "gamma = 0.9\n",
    "\n",
    "print(f\"Rewards: {rewards2}\")\n",
    "print(f\"Œ≥ = {gamma}\\n\")\n",
    "\n",
    "G = calculate_discounted_return(rewards2, gamma)\n",
    "print(f\"Total discounted return: {G:.4f}\")\n",
    "\n",
    "# Show the calculation step by step\n",
    "print(\"\\nStep-by-step calculation:\")\n",
    "print(f\"G = {rewards2[0]} + {gamma}√ó{rewards2[1]} + {gamma}¬≤√ó{rewards2[2]} + {gamma}¬≥√ó{rewards2[3]} + {gamma}‚Å¥√ó{rewards2[4]}\")\n",
    "print(f\"G = {rewards2[0]} + {gamma*rewards2[1]:.2f} + {gamma**2*rewards2[2]:.2f} + {gamma**3*rewards2[3]:.2f} + {gamma**4*rewards2[4]:.2f}\")\n",
    "print(f\"G = {G:.4f}\")\n",
    "\n",
    "# Calculate returns-to-go\n",
    "returns_to_go = calculate_returns_to_go(rewards2, gamma)\n",
    "print(\"\\nReturns-to-go at each time step:\")\n",
    "for t, (r, G_t) in enumerate(zip(rewards2, returns_to_go)):\n",
    "    print(f\"  t={t}: Reward={r}, Return-to-go G_{t}={G_t:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Functions: Estimating Long-Term Value\n",
    "\n",
    "**From Returns to Value Functions**\n",
    "\n",
    "While the return $G_t$ tells us the actual cumulative reward from a specific trajectory, **value functions** tell us the **expected** return from a state or state-action pair.\n",
    "\n",
    "**State-Value Function V(s)**\n",
    "\n",
    "The **state-value function** $V^\\pi(s)$ is the expected return starting from state $s$ and following policy $\\pi$:\n",
    "\n",
    "$V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$\n",
    "\n",
    "$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]$\n",
    "\n",
    "**Interpretation:**\n",
    "- \"How good is it to be in state $s$?\"\n",
    "- Expected cumulative reward if we start in $s$ and follow policy $\\pi$\n",
    "- Depends on the policy being followed\n",
    "\n",
    "**Action-Value Function Q(s,a)**\n",
    "\n",
    "The **action-value function** $Q^\\pi(s,a)$ is the expected return starting from state $s$, taking action $a$, then following policy $\\pi$:\n",
    "\n",
    "$Q^\\pi(s,a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]$\n",
    "\n",
    "$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a\\right]$\n",
    "\n",
    "**Interpretation:**\n",
    "- \"How good is it to take action $a$ in state $s$?\"\n",
    "- Expected cumulative reward if we start in $s$, take action $a$, then follow $\\pi$\n",
    "- Also called Q-values (hence \"Q-learning\")\n",
    "\n",
    "**Relationship Between V and Q:**\n",
    "\n",
    "The state-value is the expected action-value under the policy:\n",
    "\n",
    "$V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a)$\n",
    "\n",
    "For a deterministic policy that always chooses action $a^*$ in state $s$:\n",
    "\n",
    "$V^\\pi(s) = Q^\\pi(s, a^*)$\n",
    "\n",
    "**Optimal Value Functions:**\n",
    "\n",
    "The **optimal state-value function** $V^*(s)$ is the maximum value achievable in state $s$:\n",
    "\n",
    "$V^*(s) = \\max_\\pi V^\\pi(s)$\n",
    "\n",
    "The **optimal action-value function** $Q^*(s,a)$ is the maximum value achievable by taking action $a$ in state $s$:\n",
    "\n",
    "$Q^*(s,a) = \\max_\\pi Q^\\pi(s,a)$\n",
    "\n",
    "**Key Insight:**\n",
    "If we know $Q^*(s,a)$ for all states and actions, we can act optimally by choosing:\n",
    "\n",
    "$\\pi^*(s) = \\arg\\max_a Q^*(s,a)$\n",
    "\n",
    "This is why Q-learning is so powerful - it learns $Q^*$ directly!\n",
    "\n",
    "Let's demonstrate these concepts with examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Demonstrate value functions with simple examples\n",
    "print(\"Value Functions: V(s) and Q(s,a)\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simple example: 3-state chain\n",
    "# States: S0 -> S1 -> S2 (terminal)\n",
    "# Actions: FORWARD (deterministic)\n",
    "# Rewards: 0, 0, +10 (only at terminal)\n",
    "\n",
    "print(\"Example: Simple 3-State Chain\")\n",
    "print(\"\\nStates: S0 ‚Üí S1 ‚Üí S2 (terminal)\")\n",
    "print(\"Action: FORWARD (deterministic)\")\n",
    "print(\"Rewards: R(S0‚ÜíS1)=0, R(S1‚ÜíS2)=10\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "gamma = 0.9\n",
    "print(f\"\\nDiscount factor Œ≥ = {gamma}\")\n",
    "\n",
    "# Calculate V(s) for each state\n",
    "# V(S2) = 0 (terminal state, no future rewards)\n",
    "# V(S1) = 0 + Œ≥ * 10 = 9.0\n",
    "# V(S0) = 0 + Œ≥ * V(S1) = 0 + 0.9 * 9.0 = 8.1\n",
    "\n",
    "V_S2 = 0\n",
    "V_S1 = 0 + gamma * 10\n",
    "V_S0 = 0 + gamma * V_S1\n",
    "\n",
    "print(\"\\nState-Value Function V(s):\")\n",
    "print(f\"  V(S0) = {V_S0:.2f}  (2 steps to reward)\")\n",
    "print(f\"  V(S1) = {V_S1:.2f}  (1 step to reward)\")\n",
    "print(f\"  V(S2) = {V_S2:.2f}  (terminal state)\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - V(S0) < V(S1) because S0 is farther from the reward\")\n",
    "print(\"   - Each step away reduces value by factor of Œ≥\")\n",
    "print(\"   - V(s) tells us 'how good' each state is\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Example with multiple actions\n",
    "print(\"\\nExample: Grid World with Multiple Actions\\n\")\n",
    "print(\"Consider state S with two actions:\")\n",
    "print(\"  - Action A1: Leads to goal (reward +10) with prob 0.8\")\n",
    "print(\"  - Action A2: Leads to goal (reward +10) with prob 0.3\")\n",
    "print(\"\\nBoth actions give -1 reward if they don't reach goal\")\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "# Q(S, A1) = 0.8 * 10 + 0.2 * (-1) = 7.8\n",
    "# Q(S, A2) = 0.3 * 10 + 0.7 * (-1) = 2.3\n",
    "\n",
    "Q_S_A1 = 0.8 * 10 + 0.2 * (-1)\n",
    "Q_S_A2 = 0.3 * 10 + 0.7 * (-1)\n",
    "\n",
    "print(\"\\nAction-Value Function Q(s,a):\")\n",
    "print(f\"  Q(S, A1) = {Q_S_A1:.2f}  (high success rate)\")\n",
    "print(f\"  Q(S, A2) = {Q_S_A2:.2f}  (low success rate)\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - Q(S, A1) > Q(S, A2) because A1 is more likely to succeed\")\n",
    "print(\"   - Optimal action: A1 (higher Q-value)\")\n",
    "print(\"   - Q(s,a) tells us 'how good' each action is in each state\")\n",
    "\n",
    "# If following a policy that chooses A1 with prob 0.7 and A2 with prob 0.3\n",
    "V_S = 0.7 * Q_S_A1 + 0.3 * Q_S_A2\n",
    "print(f\"\\nIf policy œÄ(A1|S)=0.7, œÄ(A2|S)=0.3:\")\n",
    "print(f\"  V(S) = 0.7 √ó Q(S,A1) + 0.3 √ó Q(S,A2)\")\n",
    "print(f\"  V(S) = 0.7 √ó {Q_S_A1:.2f} + 0.3 √ó {Q_S_A2:.2f}\")\n",
    "print(f\"  V(S) = {V_S:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüéØ Key Takeaways:\")\n",
    "print(\"   1. V(s): Expected return from state s\")\n",
    "print(\"   2. Q(s,a): Expected return from taking action a in state s\")\n",
    "print(\"   3. V(s) = Œ£ œÄ(a|s) Q(s,a) (weighted average over actions)\")\n",
    "print(\"   4. Optimal policy: Choose action with highest Q-value\")\n",
    "print(\"   5. Value functions are the foundation of RL algorithms!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Effect of Discount Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize how discount factor affects returns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a reward sequence\n",
    "num_steps = 20\n",
    "rewards = np.ones(num_steps)  # Constant reward of 1 at each step\n",
    "\n",
    "# Calculate returns for different gamma values\n",
    "gamma_values = [0.5, 0.7, 0.9, 0.95, 0.99]\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Discount weights over time\n",
    "time_steps = np.arange(num_steps)\n",
    "for gamma, color in zip(gamma_values, colors):\n",
    "    weights = gamma ** time_steps\n",
    "    ax1.plot(time_steps, weights, linewidth=2, color=color, \n",
    "            label=f'Œ≥ = {gamma}', marker='o', markersize=4, alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Time Steps into Future', fontsize=12)\n",
    "ax1.set_ylabel('Discount Weight (Œ≥·µó)', fontsize=12)\n",
    "ax1.set_title('How Discount Factor Weights Future Rewards', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper right', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 1.1])\n",
    "\n",
    "# Plot 2: Total discounted return\n",
    "returns = []\n",
    "for gamma in gamma_values:\n",
    "    G = calculate_discounted_return(rewards, gamma)\n",
    "    returns.append(G)\n",
    "\n",
    "bars = ax2.bar(range(len(gamma_values)), returns, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax2.set_xlabel('Discount Factor Œ≥', fontsize=12)\n",
    "ax2.set_ylabel('Total Discounted Return', fontsize=12)\n",
    "ax2.set_title('Total Return for Constant Reward Sequence', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(range(len(gamma_values)))\n",
    "ax2.set_xticklabels([f'{g}' for g in gamma_values])\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, returns):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization Insights:\")\n",
    "print(\"\\nLeft Plot - Discount Weights:\")\n",
    "print(\"   - Shows how much each future reward is weighted\")\n",
    "print(\"   - Lower Œ≥: Future rewards decay quickly\")\n",
    "print(\"   - Higher Œ≥: Future rewards remain important longer\")\n",
    "print(\"\\nRight Plot - Total Returns:\")\n",
    "print(\"   - Shows cumulative effect of discounting\")\n",
    "print(\"   - Œ≥=0.5: Only considers ~2 steps ahead effectively\")\n",
    "print(\"   - Œ≥=0.99: Considers ~100 steps ahead effectively\")\n",
    "print(\"\\nüí° Rule of thumb: Effective horizon ‚âà 1/(1-Œ≥) steps\")\n",
    "for gamma in gamma_values:\n",
    "    horizon = 1 / (1 - gamma)\n",
    "    print(f\"   Œ≥={gamma}: ~{horizon:.0f} steps\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policies: Mapping States to Actions\n",
    "\n",
    "**What is a Policy?**\n",
    "\n",
    "A **policy** $\\pi$ is a strategy that defines how the agent behaves - it maps states to actions. The policy is what the agent learns in reinforcement learning.\n",
    "\n",
    "**Types of Policies:**\n",
    "\n",
    "**1. Deterministic Policy**\n",
    "\n",
    "A deterministic policy $\\pi: S \\rightarrow A$ maps each state to a single action:\n",
    "\n",
    "$a = \\pi(s)$\n",
    "\n",
    "**Example:**\n",
    "- In grid world: \"Always move RIGHT in state (0,0)\"\n",
    "- In chess: \"Always make the move that captures the most valuable piece\"\n",
    "\n",
    "**2. Stochastic Policy**\n",
    "\n",
    "A stochastic policy $\\pi(a|s)$ gives a probability distribution over actions for each state:\n",
    "\n",
    "$\\pi(a|s) = P(A_t = a | S_t = s)$\n",
    "\n",
    "where $\\sum_a \\pi(a|s) = 1$ for all states $s$\n",
    "\n",
    "**Example:**\n",
    "- In grid world: \"Move RIGHT with 70% probability, DOWN with 30% in state (0,0)\"\n",
    "- Epsilon-greedy: \"Take best action with probability 1-Œµ, random action with probability Œµ\"\n",
    "\n",
    "**Why Stochastic Policies?**\n",
    "\n",
    "1. **Exploration**: Randomness helps explore the environment\n",
    "2. **Partial Observability**: When state is uncertain, randomization can help\n",
    "3. **Game Theory**: In competitive settings, randomization prevents exploitation\n",
    "4. **Continuous Actions**: Natural representation for continuous action spaces\n",
    "\n",
    "**Optimal Policy**\n",
    "\n",
    "The **optimal policy** $\\pi^*$ maximizes the expected return from every state:\n",
    "\n",
    "$\\pi^* = \\arg\\max_\\pi V^\\pi(s) \\text{ for all } s \\in S$\n",
    "\n",
    "**Key Theorem:** For any MDP, there exists an optimal deterministic policy!\n",
    "\n",
    "This means we can always find a policy that doesn't need randomness to be optimal (though stochastic policies are still useful during learning).\n",
    "\n",
    "Let's implement policy representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DeterministicPolicy:\n",
    "    \"\"\"A deterministic policy that maps states to actions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize empty policy.\"\"\"\n",
    "        self.policy = {}  # state -> action mapping\n",
    "    \n",
    "    def set_action(self, state, action):\n",
    "        \"\"\"Set the action for a given state.\"\"\"\n",
    "        self.policy[state] = action\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Get the action for a given state.\"\"\"\n",
    "        return self.policy.get(state, None)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"DeterministicPolicy({len(self.policy)} states)\"\n",
    "\n",
    "\n",
    "class StochasticPolicy:\n",
    "    \"\"\"A stochastic policy that gives probability distributions over actions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize empty policy.\"\"\"\n",
    "        self.policy = {}  # state -> {action: probability} mapping\n",
    "    \n",
    "    def set_action_probs(self, state, action_probs):\n",
    "        \"\"\"Set action probabilities for a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state\n",
    "            action_probs: Dict mapping actions to probabilities\n",
    "        \"\"\"\n",
    "        # Verify probabilities sum to 1\n",
    "        total = sum(action_probs.values())\n",
    "        if not np.isclose(total, 1.0):\n",
    "            raise ValueError(f\"Action probabilities must sum to 1, got {total}\")\n",
    "        self.policy[state] = action_probs.copy()\n",
    "    \n",
    "    def get_action_prob(self, state, action):\n",
    "        \"\"\"Get probability of taking action in state.\"\"\"\n",
    "        return self.policy.get(state, {}).get(action, 0.0)\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"Sample an action according to the policy.\"\"\"\n",
    "        action_probs = self.policy.get(state, {})\n",
    "        if not action_probs:\n",
    "            return None\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"StochasticPolicy({len(self.policy)} states)\"\n",
    "\n",
    "\n",
    "# Demonstrate policy representations\n",
    "print(\"Policy Representations\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example 1: Deterministic policy for 2x2 grid\n",
    "print(\"Example 1: Deterministic Policy for 2x2 Grid\\n\")\n",
    "\n",
    "det_policy = DeterministicPolicy()\n",
    "det_policy.set_action('(0,0)', 'RIGHT')\n",
    "det_policy.set_action('(0,1)', 'DOWN')\n",
    "det_policy.set_action('(1,0)', 'RIGHT')\n",
    "det_policy.set_action('(1,1)', 'STAY')  # Goal state\n",
    "\n",
    "print(\"Deterministic Policy œÄ(s):\")\n",
    "for state in ['(0,0)', '(0,1)', '(1,0)', '(1,1)']:\n",
    "    action = det_policy.get_action(state)\n",
    "    print(f\"  œÄ({state}) = {action}\")\n",
    "\n",
    "print(\"\\nüí° This policy always takes the same action in each state\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Example 2: Stochastic policy\n",
    "print(\"\\nExample 2: Stochastic Policy (Exploration)\\n\")\n",
    "\n",
    "stoch_policy = StochasticPolicy()\n",
    "\n",
    "# State (0,0): Prefer RIGHT but sometimes go DOWN\n",
    "stoch_policy.set_action_probs('(0,0)', {'RIGHT': 0.7, 'DOWN': 0.3})\n",
    "\n",
    "# State (0,1): Prefer DOWN\n",
    "stoch_policy.set_action_probs('(0,1)', {'RIGHT': 0.1, 'DOWN': 0.9})\n",
    "\n",
    "# State (1,0): Prefer RIGHT\n",
    "stoch_policy.set_action_probs('(1,0)', {'RIGHT': 0.9, 'DOWN': 0.1})\n",
    "\n",
    "print(\"Stochastic Policy œÄ(a|s):\")\n",
    "for state in ['(0,0)', '(0,1)', '(1,0)']:\n",
    "    print(f\"\\n  State {state}:\")\n",
    "    for action in ['RIGHT', 'DOWN']:\n",
    "        prob = stoch_policy.get_action_prob(state, action)\n",
    "        if prob > 0:\n",
    "            print(f\"    œÄ({action}|{state}) = {prob:.1f}\")\n",
    "\n",
    "print(\"\\nüí° This policy has randomness - different actions with different probabilities\")\n",
    "\n",
    "# Sample actions from stochastic policy\n",
    "print(\"\\nSampling 10 actions from state (0,0):\")\n",
    "samples = [stoch_policy.sample_action('(0,0)') for _ in range(10)]\n",
    "print(f\"  Actions: {samples}\")\n",
    "right_count = samples.count('RIGHT')\n",
    "down_count = samples.count('DOWN')\n",
    "print(f\"  RIGHT: {right_count}/10 ({right_count*10}%), DOWN: {down_count}/10 ({down_count*10}%)\")\n",
    "print(f\"  Expected: RIGHT: 70%, DOWN: 30%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüéØ Key Points:\")\n",
    "print(\"   1. Deterministic: œÄ(s) ‚Üí single action\")\n",
    "print(\"   2. Stochastic: œÄ(a|s) ‚Üí probability distribution\")\n",
    "print(\"   3. Optimal policies can be deterministic\")\n",
    "print(\"   4. Stochastic policies useful for exploration during learning\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equations: The Foundation of RL Algorithms\n",
    "\n",
    "**The Bellman Equations**\n",
    "\n",
    "The **Bellman equations** are fundamental recursive relationships that express value functions in terms of themselves. They are the mathematical foundation for most RL algorithms.\n",
    "\n",
    "**Bellman Equation for V(s):**\n",
    "\n",
    "The value of a state equals the expected immediate reward plus the discounted value of the next state:\n",
    "\n",
    "$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V^\\pi(s')\\right]$\n",
    "\n",
    "**In words:**\n",
    "1. Consider all possible actions under policy $\\pi$\n",
    "2. For each action, consider all possible next states\n",
    "3. Sum up: immediate reward + discounted value of next state\n",
    "4. Weight by probabilities\n",
    "\n",
    "**Bellman Equation for Q(s,a):**\n",
    "\n",
    "$Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a')\\right]$\n",
    "\n",
    "**Bellman Optimality Equations:**\n",
    "\n",
    "For the optimal value functions:\n",
    "\n",
    "$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V^*(s')\\right]$\n",
    "\n",
    "$Q^*(s,a) = \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')\\right]$\n",
    "\n",
    "**Why Are Bellman Equations Important?**\n",
    "\n",
    "1. **Recursive Structure**: Break down long-term value into immediate reward + future value\n",
    "2. **Dynamic Programming**: Enable iterative computation of value functions\n",
    "3. **Temporal Difference Learning**: Basis for TD learning and Q-learning\n",
    "4. **Optimality**: Optimal policies satisfy the Bellman optimality equations\n",
    "\n",
    "**The Bellman Deadlock**\n",
    "\n",
    "The Bellman equations create a system of equations where:\n",
    "- Each value depends on other values\n",
    "- We have $|S|$ equations with $|S|$ unknowns (for V)\n",
    "- Or $|S| \\times |A|$ equations with $|S| \\times |A|$ unknowns (for Q)\n",
    "\n",
    "**The Problem:**\n",
    "- Can't solve directly because values are defined in terms of each other\n",
    "- This circular dependency is called the \"Bellman deadlock\"\n",
    "\n",
    "**Solutions:**\n",
    "1. **Iterative Methods**: Dynamic Programming (policy evaluation, value iteration)\n",
    "2. **Sampling Methods**: Monte Carlo, Temporal Difference learning\n",
    "3. **Function Approximation**: Neural networks for large state spaces\n",
    "\n",
    "**The Curse of Dimensionality**\n",
    "\n",
    "As the state space grows, computational requirements explode:\n",
    "\n",
    "- **Tabular Methods**: Need to store value for every state\n",
    "  - 10 binary features ‚Üí $2^{10} = 1,024$ states\n",
    "  - 20 binary features ‚Üí $2^{20} = 1,048,576$ states\n",
    "  - 30 binary features ‚Üí $2^{30} = 1,073,741,824$ states\n",
    "\n",
    "- **Continuous States**: Infinite states (e.g., robot position)\n",
    "\n",
    "**Addressing the Curse:**\n",
    "1. **Function Approximation**: Learn V(s) or Q(s,a) with neural networks\n",
    "2. **Sampling**: Don't visit all states, learn from experience\n",
    "3. **Generalization**: Use features to generalize across similar states\n",
    "4. **Hierarchical Methods**: Break problem into subproblems\n",
    "\n",
    "Let's demonstrate the Bellman equations with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Demonstrate Bellman equations with policy evaluation\n",
    "print(\"Bellman Equations: Policy Evaluation Example\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use our 2x2 grid MDP from earlier\n",
    "states = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\n",
    "gamma = 0.9\n",
    "\n",
    "# Define a simple policy: always go RIGHT from (0,0) and (1,0), DOWN from (0,1)\n",
    "policy = {\n",
    "    '(0,0)': {'RIGHT': 1.0},\n",
    "    '(0,1)': {'DOWN': 1.0},\n",
    "    '(1,0)': {'RIGHT': 1.0},\n",
    "    '(1,1)': {'RIGHT': 1.0}  # Terminal, doesn't matter\n",
    "}\n",
    "\n",
    "# Transitions and rewards (from earlier MDP)\n",
    "transitions = {\n",
    "    ('(0,0)', 'RIGHT'): {'(0,1)': 1.0},\n",
    "    ('(0,1)', 'DOWN'): {'(1,1)': 1.0},\n",
    "    ('(1,0)', 'RIGHT'): {'(1,1)': 1.0},\n",
    "    ('(1,1)', 'RIGHT'): {'(1,1)': 1.0},\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    ('(0,0)', 'RIGHT', '(0,1)'): -1,\n",
    "    ('(0,1)', 'DOWN', '(1,1)'): 10,\n",
    "    ('(1,0)', 'RIGHT', '(1,1)'): 10,\n",
    "    ('(1,1)', 'RIGHT', '(1,1)'): 0,\n",
    "}\n",
    "\n",
    "print(\"MDP Setup:\")\n",
    "print(\"  States: (0,0) ‚Üí (0,1) ‚Üí (1,1) [GOAL]\")\n",
    "print(\"           ‚Üì       ‚Üì\")\n",
    "print(\"         (1,0) ‚Üí (1,1) [GOAL]\")\n",
    "print(f\"\\n  Discount factor Œ≥ = {gamma}\")\n",
    "print(\"\\n  Policy œÄ:\")\n",
    "for state, actions in policy.items():\n",
    "    for action, prob in actions.items():\n",
    "        if prob > 0:\n",
    "            print(f\"    œÄ({state}) = {action}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nApplying Bellman Equation: V(s) = Œ£ œÄ(a|s) Œ£ P(s'|s,a)[R + Œ≥V(s')]\\n\")\n",
    "\n",
    "# Iterative policy evaluation\n",
    "V = {s: 0.0 for s in states}  # Initialize values to 0\n",
    "V['(1,1)'] = 0.0  # Terminal state\n",
    "\n",
    "print(\"Iteration 0 (Initial):\")\n",
    "for state in states:\n",
    "    print(f\"  V({state}) = {V[state]:.2f}\")\n",
    "\n",
    "# Perform a few iterations\n",
    "for iteration in range(1, 6):\n",
    "    V_new = V.copy()\n",
    "    \n",
    "    for state in states:\n",
    "        if state == '(1,1)':  # Terminal state\n",
    "            continue\n",
    "        \n",
    "        # Apply Bellman equation\n",
    "        v = 0.0\n",
    "        for action, action_prob in policy[state].items():\n",
    "            # Get transitions for this state-action pair\n",
    "            next_states = transitions.get((state, action), {})\n",
    "            \n",
    "            for next_state, trans_prob in next_states.items():\n",
    "                reward = rewards.get((state, action, next_state), 0.0)\n",
    "                # Bellman equation: R + Œ≥ * V(s')\n",
    "                v += action_prob * trans_prob * (reward + gamma * V[next_state])\n",
    "        \n",
    "        V_new[state] = v\n",
    "    \n",
    "    V = V_new\n",
    "    \n",
    "    print(f\"\\nIteration {iteration}:\")\n",
    "    for state in states:\n",
    "        print(f\"  V({state}) = {V[state]:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   1. Values converge through iterative application of Bellman equation\")\n",
    "print(\"   2. V(1,1) = 0 (terminal state, no future rewards)\")\n",
    "print(\"   3. V(0,1) and V(1,0) are high (one step from goal)\")\n",
    "print(\"   4. V(0,0) is lower (two steps from goal, more discounting)\")\n",
    "print(\"   5. Each iteration uses previous values to compute new values\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüéØ Key Insights:\")\n",
    "print(\"   1. Bellman equations express values recursively\")\n",
    "print(\"   2. Can't solve directly (circular dependency = Bellman deadlock)\")\n",
    "print(\"   3. Iterative methods converge to true values\")\n",
    "print(\"   4. This is the foundation of Dynamic Programming!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the curse of dimensionality\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate state space sizes for different scenarios\n",
    "# Store as (name, exponent) to avoid overflow\n",
    "scenarios = [\n",
    "    ('Grid 5√ó5', np.log10(25)),\n",
    "    ('Grid 10√ó10', np.log10(100)),\n",
    "    ('Grid 20√ó20', np.log10(400)),\n",
    "    ('10 binary features', np.log10(2**10)),\n",
    "    ('15 binary features', np.log10(2**15)),\n",
    "    ('20 binary features', np.log10(2**20)),\n",
    "    ('Chess (approx)', 43),\n",
    "    ('Go (approx)', 170)\n",
    "]\n",
    "\n",
    "names = [s[0] for s in scenarios]\n",
    "log_sizes = [s[1] for s in scenarios]\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Use log scale for y-axis\n",
    "y_pos = np.arange(len(names))\n",
    "colors = ['green', 'green', 'yellow', 'yellow', 'orange', 'red', 'darkred', 'darkred']\n",
    "\n",
    "bars = ax.barh(y_pos, log_sizes, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(names, fontsize=11)\n",
    "ax.set_xlabel('Number of States (log‚ÇÅ‚ÇÄ scale)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('The Curse of Dimensionality in Reinforcement Learning', \n",
    "            fontsize=15, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, log_size) in enumerate(zip(bars, log_sizes)):\n",
    "    exp = int(log_size)\n",
    "    label = f'10^{exp}'\n",
    "    \n",
    "    ax.text(log_size, bar.get_y() + bar.get_height()/2, f'  {label}',\n",
    "           va='center', ha='left', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Add annotations\n",
    "ax.text(0.02, 0.98, 'Tractable with\\ntabular methods', \n",
    "       transform=ax.transAxes, fontsize=11, va='top',\n",
    "       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "ax.text(0.02, 0.50, 'Need function\\napproximation', \n",
    "       transform=ax.transAxes, fontsize=11, va='top',\n",
    "       bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "\n",
    "ax.text(0.02, 0.15, 'Extremely\\nchallenging', \n",
    "       transform=ax.transAxes, fontsize=11, va='top',\n",
    "       bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä The Curse of Dimensionality:\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nState Space Growth:\")\n",
    "# Recreate actual sizes for printing\n",
    "actual_scenarios = [\n",
    "    ('Grid 5√ó5', 25),\n",
    "    ('Grid 10√ó10', 100),\n",
    "    ('Grid 20√ó20', 400),\n",
    "    ('10 binary features', 2**10),\n",
    "    ('15 binary features', 2**15),\n",
    "    ('20 binary features', 2**20),\n",
    "    ('Chess (approx)', 43),\n",
    "    ('Go (approx)', 170)\n",
    "]\n",
    "for name, size in actual_scenarios:\n",
    "    if isinstance(size, int) and size < 10**10:\n",
    "        print(f\"  {name:<25} {size:>20,} states\")\n",
    "    else:\n",
    "        if isinstance(size, int) and size >= 10**10:\n",
    "            exp = int(np.log10(size))\n",
    "        else:\n",
    "            exp = size\n",
    "        print(f\"  {name:<25} ~10^{exp} states\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   1. State space grows exponentially with features\")\n",
    "print(\"   2. Tabular methods only work for small state spaces\")\n",
    "print(\"   3. Real-world problems need function approximation\")\n",
    "print(\"   4. Deep RL uses neural networks to handle large spaces\")\n",
    "print(\"   5. Sampling and generalization are essential!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüéØ Summary of MDP Framework:\")\n",
    "print(\"\\n   We've covered the complete MDP framework:\")\n",
    "print(\"   ‚úì Core terminology (agent, environment, state, action, reward)\")\n",
    "print(\"   ‚úì MDP components (S, A, P, R, Œ≥)\")\n",
    "print(\"   ‚úì Markov Property and its importance\")\n",
    "print(\"   ‚úì Discounted returns and value functions\")\n",
    "print(\"   ‚úì Policies (deterministic and stochastic)\")\n",
    "print(\"   ‚úì Bellman equations (foundation of RL algorithms)\")\n",
    "print(\"   ‚úì Challenges (Bellman deadlock, curse of dimensionality)\")\n",
    "print(\"\\n   Next: We'll learn algorithms to solve MDPs!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dynamic-programming'></a>\n",
    "### Dynamic Programming: Solving MDPs with Perfect Knowledge\n",
    "\n",
    "**From Theory to Algorithms**\n",
    "\n",
    "Now that we understand the Bellman equations, we can use them to solve MDPs! **Dynamic Programming (DP)** methods provide exact solutions when we have perfect knowledge of the environment's dynamics.\n",
    "\n",
    "**What is Dynamic Programming?**\n",
    "\n",
    "Dynamic Programming is a general approach to solving complex problems by:\n",
    "1. Breaking them into simpler subproblems\n",
    "2. Solving each subproblem once\n",
    "3. Storing solutions to avoid recomputation\n",
    "4. Combining solutions to solve the original problem\n",
    "\n",
    "In RL, DP uses the Bellman equations to iteratively compute value functions.\n",
    "\n",
    "**Key Assumptions for DP:**\n",
    "\n",
    "1. **Perfect Model**: We know $P(s'|s,a)$ and $R(s,a,s')$ for all states and actions\n",
    "2. **Finite State/Action Spaces**: Can enumerate all states and actions\n",
    "3. **Markov Property**: Future depends only on current state\n",
    "\n",
    "**Two Main DP Algorithms:**\n",
    "\n",
    "1. **Policy Evaluation**: Compute $V^\\pi(s)$ for a given policy $\\pi$\n",
    "2. **Policy Improvement**: Find a better policy given $V^\\pi(s)$\n",
    "\n",
    "Combining these gives us **Policy Iteration** and **Value Iteration** algorithms.\n",
    "\n",
    "**Why Study DP?**\n",
    "\n",
    "Even though DP requires perfect knowledge (rarely available in practice), it's important because:\n",
    "- Provides theoretical foundation for RL\n",
    "- Many RL algorithms are approximate DP methods\n",
    "- Helps understand convergence and optimality\n",
    "- Works well for planning problems (e.g., robotics with simulators)\n",
    "\n",
    "Let's start with Policy Evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Evaluation: Computing the Value Function\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "Given a policy $\\pi$, compute the state-value function $V^\\pi(s)$ for all states.\n",
    "\n",
    "**The Bellman Equation for Policy Evaluation:**\n",
    "\n",
    "$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V^\\pi(s')\\right]$\n",
    "\n",
    "This is a system of $|S|$ linear equations with $|S|$ unknowns. We could solve it directly, but for large state spaces, we use an **iterative approach**.\n",
    "\n",
    "**Iterative Policy Evaluation Algorithm:**\n",
    "\n",
    "1. Initialize $V(s) = 0$ for all states (or any arbitrary values)\n",
    "2. Repeat until convergence:\n",
    "   - For each state $s$:\n",
    "     - $V_{k+1}(s) \\leftarrow \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V_k(s')\\right]$\n",
    "3. Stop when $\\max_s |V_{k+1}(s) - V_k(s)| < \\theta$ (small threshold)\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "- Each iteration applies the Bellman equation as an update rule\n",
    "- Uses old values $V_k(s')$ to compute new values $V_{k+1}(s)$\n",
    "- Guaranteed to converge to $V^\\pi$ as $k \\rightarrow \\infty$\n",
    "- Called \"bootstrapping\" - using estimates to update estimates\n",
    "\n",
    "**Two Variants:**\n",
    "\n",
    "1. **Synchronous**: Update all states using old values, then replace all at once\n",
    "2. **Asynchronous**: Update states one at a time, using most recent values\n",
    "\n",
    "Asynchronous often converges faster in practice.\n",
    "\n",
    "Let's implement policy evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def policy_evaluation(mdp, policy, gamma=0.9, theta=0.0001, max_iterations=1000):\n",
    "    \"\"\"Evaluate a policy using iterative policy evaluation.\n",
    "    \n",
    "    Args:\n",
    "        mdp: MDP object with states, actions, transitions, rewards\n",
    "        policy: Dict mapping state -> {action: probability}\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold\n",
    "        max_iterations: Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        V: Dict mapping state -> value\n",
    "        iterations: Number of iterations until convergence\n",
    "    \"\"\"\n",
    "    # Initialize value function to zero\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0  # Track maximum change in value\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        # Update value for each state\n",
    "        for state in mdp.states:\n",
    "            v = 0.0\n",
    "            \n",
    "            # Sum over all actions according to policy\n",
    "            for action, action_prob in policy.get(state, {}).items():\n",
    "                # Sum over all possible next states\n",
    "                next_states = mdp.transitions.get((state, action), {})\n",
    "                \n",
    "                for next_state, trans_prob in next_states.items():\n",
    "                    reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                    # Bellman equation\n",
    "                    v += action_prob * trans_prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            V_new[state] = v\n",
    "            delta = max(delta, abs(V_new[state] - V[state]))\n",
    "        \n",
    "        V = V_new\n",
    "        \n",
    "        # Check for convergence\n",
    "        if delta < theta:\n",
    "            print(f\"Policy evaluation converged in {iteration + 1} iterations\")\n",
    "            return V, iteration + 1\n",
    "    \n",
    "    print(f\"Policy evaluation reached max iterations ({max_iterations})\")\n",
    "    return V, max_iterations\n",
    "\n",
    "\n",
    "# Demonstrate policy evaluation on our 2x2 grid MDP\n",
    "print(\"Policy Evaluation Demonstration\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Recreate the 2x2 grid MDP\n",
    "states = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\n",
    "actions = ['RIGHT', 'DOWN']\n",
    "\n",
    "transitions = {\n",
    "    ('(0,0)', 'RIGHT'): {'(0,1)': 1.0},\n",
    "    ('(0,0)', 'DOWN'): {'(1,0)': 1.0},\n",
    "    ('(0,1)', 'RIGHT'): {'(0,1)': 1.0},\n",
    "    ('(0,1)', 'DOWN'): {'(1,1)': 1.0},\n",
    "    ('(1,0)', 'RIGHT'): {'(1,1)': 1.0},\n",
    "    ('(1,0)', 'DOWN'): {'(1,0)': 1.0},\n",
    "    ('(1,1)', 'RIGHT'): {'(1,1)': 1.0},\n",
    "    ('(1,1)', 'DOWN'): {'(1,1)': 1.0},\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    ('(0,0)', 'RIGHT', '(0,1)'): -1,\n",
    "    ('(0,0)', 'DOWN', '(1,0)'): -1,\n",
    "    ('(0,1)', 'RIGHT', '(0,1)'): -1,\n",
    "    ('(0,1)', 'DOWN', '(1,1)'): 10,\n",
    "    ('(1,0)', 'RIGHT', '(1,1)'): 10,\n",
    "    ('(1,0)', 'DOWN', '(1,0)'): -1,\n",
    "    ('(1,1)', 'RIGHT', '(1,1)'): 0,\n",
    "    ('(1,1)', 'DOWN', '(1,1)'): 0,\n",
    "}\n",
    "\n",
    "mdp = SimpleMDP(states, actions, transitions, rewards, gamma=0.9)\n",
    "\n",
    "# Define a policy: always go RIGHT from (0,0) and (1,0), DOWN from (0,1)\n",
    "policy = {\n",
    "    '(0,0)': {'RIGHT': 1.0},\n",
    "    '(0,1)': {'DOWN': 1.0},\n",
    "    '(1,0)': {'RIGHT': 1.0},\n",
    "    '(1,1)': {'RIGHT': 1.0}\n",
    "}\n",
    "\n",
    "print(\"MDP: 2x2 Grid World\")\n",
    "print(\"  (0,0) ‚Üí (0,1)\")\n",
    "print(\"    ‚Üì       ‚Üì\")\n",
    "print(\"  (1,0) ‚Üí (1,1) [GOAL]\")\n",
    "print(f\"\\nDiscount factor Œ≥ = {mdp.gamma}\")\n",
    "\n",
    "print(\"\\nPolicy œÄ:\")\n",
    "for state, actions_dict in policy.items():\n",
    "    for action, prob in actions_dict.items():\n",
    "        if prob > 0:\n",
    "            print(f\"  œÄ({state}) = {action}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nRunning Policy Evaluation...\\n\")\n",
    "\n",
    "# Evaluate the policy\n",
    "V, num_iterations = policy_evaluation(mdp, policy, gamma=0.9, theta=0.0001)\n",
    "\n",
    "print(\"\\nFinal Value Function V^œÄ(s):\")\n",
    "print(\"-\"*60)\n",
    "for state in states:\n",
    "    print(f\"  V^œÄ({state}) = {V[state]:7.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(f\"   - V^œÄ(1,1) = {V['(1,1)']:.4f} (terminal state, no future rewards)\")\n",
    "print(f\"   - V^œÄ(0,1) = {V['(0,1)']:.4f} (one step from goal via DOWN)\")\n",
    "print(f\"   - V^œÄ(1,0) = {V['(1,0)']:.4f} (one step from goal via RIGHT)\")\n",
    "print(f\"   - V^œÄ(0,0) = {V['(0,0)']:.4f} (two steps from goal)\")\n",
    "print(\"\\n   Values reflect expected cumulative reward following policy œÄ\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Policy Evaluation Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize how values converge during policy evaluation\n",
    "def policy_evaluation_with_history(mdp, policy, gamma=0.9, theta=0.0001, max_iterations=1000):\n",
    "    \"\"\"Policy evaluation that tracks value history for visualization.\"\"\"\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "    history = {s: [0.0] for s in mdp.states}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for state in mdp.states:\n",
    "            v = 0.0\n",
    "            for action, action_prob in policy.get(state, {}).items():\n",
    "                next_states = mdp.transitions.get((state, action), {})\n",
    "                for next_state, trans_prob in next_states.items():\n",
    "                    reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                    v += action_prob * trans_prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            V_new[state] = v\n",
    "            history[state].append(v)\n",
    "            delta = max(delta, abs(V_new[state] - V[state]))\n",
    "        \n",
    "        V = V_new\n",
    "        \n",
    "        if delta < theta:\n",
    "            return V, history, iteration + 1\n",
    "    \n",
    "    return V, history, max_iterations\n",
    "\n",
    "\n",
    "# Run policy evaluation with history tracking\n",
    "V, history, num_iters = policy_evaluation_with_history(mdp, policy, gamma=0.9, theta=0.0001)\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "markers = ['o', 's', '^', 'd']\n",
    "\n",
    "for state, color, marker in zip(states, colors, markers):\n",
    "    iterations = range(len(history[state]))\n",
    "    values = history[state]\n",
    "    ax.plot(iterations, values, linewidth=2.5, color=color, marker=marker,\n",
    "           markersize=6, markevery=max(1, len(iterations)//10), \n",
    "           label=f'V({state})', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Iteration', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Value V(s)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Policy Evaluation: Value Function Convergence', \n",
    "            fontsize=15, fontweight='bold', pad=15)\n",
    "ax.legend(loc='right', fontsize=12, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "# Add convergence annotation\n",
    "ax.axvline(x=num_iters, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "ax.text(num_iters, ax.get_ylim()[1]*0.9, f'Converged\\n(iter {num_iters})',\n",
    "       ha='center', fontsize=11, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Convergence Analysis:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nConverged in {num_iters} iterations\")\n",
    "print(\"\\nFinal values:\")\n",
    "for state in states:\n",
    "    print(f\"  V({state}) = {V[state]:7.4f}\")\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   1. Values start at 0 and converge to true values\")\n",
    "print(\"   2. Terminal state (1,1) stays at 0\")\n",
    "print(\"   3. States closer to goal converge to higher values\")\n",
    "print(\"   4. Convergence is exponentially fast\")\n",
    "print(\"   5. Each iteration uses Bellman equation as update rule\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrating Policy Evaluation on a Larger Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a larger 4x4 grid world for more interesting demonstration\n",
    "def create_grid_world_mdp(size=4, goal=(3, 3), obstacles=None, gamma=0.9):\n",
    "    \"\"\"Create a grid world MDP.\n",
    "    \n",
    "    Args:\n",
    "        size: Grid size (size x size)\n",
    "        goal: Goal position (row, col)\n",
    "        obstacles: List of obstacle positions\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        mdp: SimpleMDP object\n",
    "    \"\"\"\n",
    "    if obstacles is None:\n",
    "        obstacles = []\n",
    "    \n",
    "    # Generate all states\n",
    "    states = []\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if (i, j) not in obstacles:\n",
    "                states.append(f'({i},{j})')\n",
    "    \n",
    "    actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    action_effects = {'UP': (-1, 0), 'DOWN': (1, 0), 'LEFT': (0, -1), 'RIGHT': (0, 1)}\n",
    "    \n",
    "    transitions = {}\n",
    "    rewards = {}\n",
    "    \n",
    "    for state_str in states:\n",
    "        # Parse state\n",
    "        state = eval(state_str)\n",
    "        \n",
    "        for action in actions:\n",
    "            # Calculate next state\n",
    "            delta = action_effects[action]\n",
    "            next_state = (state[0] + delta[0], state[1] + delta[1])\n",
    "            \n",
    "            # Check if next state is valid\n",
    "            if (0 <= next_state[0] < size and 0 <= next_state[1] < size and \n",
    "                next_state not in obstacles):\n",
    "                next_state_str = f'({next_state[0]},{next_state[1]})'\n",
    "            else:\n",
    "                # Hit wall or obstacle, stay in place\n",
    "                next_state_str = state_str\n",
    "            \n",
    "            transitions[(state_str, action)] = {next_state_str: 1.0}\n",
    "            \n",
    "            # Set rewards\n",
    "            if next_state == goal:\n",
    "                rewards[(state_str, action, next_state_str)] = 10.0\n",
    "            elif next_state_str == state_str and state != goal:\n",
    "                rewards[(state_str, action, next_state_str)] = -1.0  # Hit wall\n",
    "            else:\n",
    "                rewards[(state_str, action, next_state_str)] = -0.1  # Step cost\n",
    "    \n",
    "    return SimpleMDP(states, actions, transitions, rewards, gamma)\n",
    "\n",
    "\n",
    "# Create 4x4 grid world\n",
    "print(\"Policy Evaluation on 4x4 Grid World\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "grid_mdp = create_grid_world_mdp(size=4, goal=(3, 3), obstacles=[(1, 1), (2, 2)], gamma=0.9)\n",
    "\n",
    "print(\"Grid World: 4x4 with obstacles at (1,1) and (2,2)\")\n",
    "print(\"Goal: (3,3)\")\n",
    "print(f\"States: {len(grid_mdp.states)} states\")\n",
    "print(f\"Discount factor: Œ≥ = {grid_mdp.gamma}\")\n",
    "\n",
    "# Create a simple policy: move towards goal (right and down preferred)\n",
    "grid_policy = {}\n",
    "for state_str in grid_mdp.states:\n",
    "    state = eval(state_str)\n",
    "    \n",
    "    if state == (3, 3):  # Goal state\n",
    "        grid_policy[state_str] = {'RIGHT': 0.25, 'DOWN': 0.25, 'LEFT': 0.25, 'UP': 0.25}\n",
    "    else:\n",
    "        # Prefer moving towards goal\n",
    "        if state[0] < 3 and state[1] < 3:\n",
    "            grid_policy[state_str] = {'RIGHT': 0.4, 'DOWN': 0.4, 'LEFT': 0.1, 'UP': 0.1}\n",
    "        elif state[0] < 3:\n",
    "            grid_policy[state_str] = {'DOWN': 0.7, 'RIGHT': 0.1, 'LEFT': 0.1, 'UP': 0.1}\n",
    "        elif state[1] < 3:\n",
    "            grid_policy[state_str] = {'RIGHT': 0.7, 'DOWN': 0.1, 'LEFT': 0.1, 'UP': 0.1}\n",
    "        else:\n",
    "            grid_policy[state_str] = {'RIGHT': 0.25, 'DOWN': 0.25, 'LEFT': 0.25, 'UP': 0.25}\n",
    "\n",
    "print(\"\\nPolicy: Stochastic policy favoring movement towards goal\")\n",
    "print(\"\\nRunning policy evaluation...\\n\")\n",
    "\n",
    "# Evaluate policy\n",
    "V_grid, num_iters = policy_evaluation(grid_mdp, grid_policy, gamma=0.9, theta=0.001)\n",
    "\n",
    "# Visualize value function as a grid\n",
    "print(\"\\nValue Function V^œÄ(s) as Grid:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "value_grid = np.full((4, 4), np.nan)\n",
    "for state_str, value in V_grid.items():\n",
    "    state = eval(state_str)\n",
    "    value_grid[state[0], state[1]] = value\n",
    "\n",
    "# Print as formatted grid\n",
    "print(\"\\n     Col 0    Col 1    Col 2    Col 3\")\n",
    "print(\"   \" + \"-\"*42)\n",
    "for i in range(4):\n",
    "    row_str = f\"Row {i} |\"  \n",
    "    for j in range(4):\n",
    "        if np.isnan(value_grid[i, j]):\n",
    "            row_str += \"   XXX   \"\n",
    "        else:\n",
    "            row_str += f\" {value_grid[i, j]:6.2f}  \"\n",
    "    print(row_str)\n",
    "\n",
    "print(\"\\n(XXX = obstacle)\")\n",
    "\n",
    "# Create heatmap visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "# Mask obstacles\n",
    "masked_grid = np.ma.masked_where(np.isnan(value_grid), value_grid)\n",
    "\n",
    "im = ax.imshow(masked_grid, cmap='RdYlGn', aspect='auto', interpolation='nearest')\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_yticks(range(4))\n",
    "ax.set_xticklabels(range(4), fontsize=12)\n",
    "ax.set_yticklabels(range(4), fontsize=12)\n",
    "ax.set_xlabel('Column', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Row', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Value Function V^œÄ(s) Heatmap\\n(Brighter = Higher Value)', \n",
    "            fontsize=15, fontweight='bold', pad=15)\n",
    "\n",
    "# Add value labels\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if not np.isnan(value_grid[i, j]):\n",
    "            text = ax.text(j, i, f'{value_grid[i, j]:.2f}',\n",
    "                         ha=\"center\", va=\"center\", color=\"black\", \n",
    "                         fontsize=11, fontweight='bold')\n",
    "        else:\n",
    "            text = ax.text(j, i, 'X',\n",
    "                         ha=\"center\", va=\"center\", color=\"white\", \n",
    "                         fontsize=20, fontweight='bold')\n",
    "\n",
    "# Mark goal\n",
    "ax.add_patch(plt.Rectangle((2.5, 2.5), 1, 1, fill=False, edgecolor='blue', linewidth=4))\n",
    "ax.text(3, 3.8, 'GOAL', ha='center', fontsize=12, fontweight='bold', color='blue')\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   1. Goal state (3,3) has highest value\")\n",
    "print(\"   2. Values decrease with distance from goal\")\n",
    "print(\"   3. Obstacles create 'shadows' in value function\")\n",
    "print(\"   4. Policy evaluation successfully computed V^œÄ for all states\")\n",
    "print(\"   5. This tells us how good each state is under the given policy\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Improvement: Finding Better Policies\n",
    "\n",
    "**From Evaluation to Improvement**\n",
    "\n",
    "Now that we can evaluate a policy, the natural question is: **Can we find a better policy?**\n",
    "\n",
    "The answer is yes, using the **Policy Improvement Theorem**!\n",
    "\n",
    "**Policy Improvement Theorem:**\n",
    "\n",
    "Given a policy $\\pi$ and its value function $V^\\pi$, we can create an improved policy $\\pi'$ by acting greedily with respect to $V^\\pi$:\n",
    "\n",
    "$\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V^\\pi(s')\\right]$\n",
    "\n",
    "Or equivalently, using Q-values:\n",
    "\n",
    "$\\pi'(s) = \\arg\\max_a Q^\\pi(s,a)$\n",
    "\n",
    "**The theorem guarantees:** $V^{\\pi'}(s) \\geq V^\\pi(s)$ for all states $s$\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "- We have $V^\\pi(s)$ telling us how good each state is under policy $\\pi$\n",
    "- For each state, we look one step ahead and choose the action that leads to the best expected value\n",
    "- This greedy policy must be at least as good as $\\pi$\n",
    "\n",
    "**Generalized Policy Iteration (GPI)**\n",
    "\n",
    "Combining policy evaluation and policy improvement gives us a powerful framework:\n",
    "\n",
    "```\n",
    "1. Initialize policy œÄ arbitrarily\n",
    "2. Repeat:\n",
    "   a. Policy Evaluation: Compute V^œÄ\n",
    "   b. Policy Improvement: œÄ' ‚Üê greedy(V^œÄ)\n",
    "   c. If œÄ' = œÄ, stop (optimal policy found)\n",
    "   d. œÄ ‚Üê œÄ'\n",
    "```\n",
    "\n",
    "This is called **Policy Iteration** and is guaranteed to converge to the optimal policy $\\pi^*$!\n",
    "\n",
    "**Why GPI Works:**\n",
    "\n",
    "- Evaluation makes the value function consistent with the current policy\n",
    "- Improvement makes the policy greedy with respect to the current value function\n",
    "- These two processes work together, pushing towards optimality\n",
    "- Convergence is guaranteed for finite MDPs\n",
    "\n",
    "Let's implement policy improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def policy_improvement(mdp, V, gamma=0.9):\n",
    "    \"\"\"Improve a policy by acting greedily with respect to value function.\n",
    "    \n",
    "    Args:\n",
    "        mdp: MDP object\n",
    "        V: Value function (dict: state -> value)\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        new_policy: Improved policy (dict: state -> {action: probability})\n",
    "        policy_stable: Boolean indicating if policy changed\n",
    "    \"\"\"\n",
    "    new_policy = {}\n",
    "    policy_stable = True\n",
    "    \n",
    "    for state in mdp.states:\n",
    "        # Calculate Q(s,a) for all actions\n",
    "        q_values = {}\n",
    "        \n",
    "        for action in mdp.actions:\n",
    "            q = 0.0\n",
    "            next_states = mdp.transitions.get((state, action), {})\n",
    "            \n",
    "            for next_state, trans_prob in next_states.items():\n",
    "                reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                q += trans_prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            q_values[action] = q\n",
    "        \n",
    "        # Choose action(s) with maximum Q-value\n",
    "        if q_values:\n",
    "            max_q = max(q_values.values())\n",
    "            best_actions = [a for a, q in q_values.items() if np.isclose(q, max_q)]\n",
    "            \n",
    "            # Create deterministic policy (or uniform over best actions if tie)\n",
    "            new_policy[state] = {a: 1.0/len(best_actions) for a in best_actions}\n",
    "    \n",
    "    return new_policy, policy_stable\n",
    "\n",
    "\n",
    "def policy_iteration(mdp, gamma=0.9, theta=0.0001, max_iterations=100):\n",
    "    \"\"\"Find optimal policy using policy iteration.\n",
    "    \n",
    "    Args:\n",
    "        mdp: MDP object\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold for policy evaluation\n",
    "        max_iterations: Maximum number of policy iterations\n",
    "        \n",
    "    Returns:\n",
    "        policy: Optimal policy\n",
    "        V: Optimal value function\n",
    "        num_iterations: Number of iterations\n",
    "    \"\"\"\n",
    "    # Initialize with random policy\n",
    "    policy = {}\n",
    "    for state in mdp.states:\n",
    "        # Uniform random policy\n",
    "        policy[state] = {a: 1.0/len(mdp.actions) for a in mdp.actions}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Policy Evaluation\n",
    "        V, _ = policy_evaluation(mdp, policy, gamma, theta, max_iterations=1000)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        new_policy, policy_stable = policy_improvement(mdp, V, gamma)\n",
    "        \n",
    "        # Check if policy has converged\n",
    "        if policies_equal(policy, new_policy):\n",
    "            print(f\"\\nPolicy iteration converged in {iteration + 1} iterations\")\n",
    "            return new_policy, V, iteration + 1\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    print(f\"\\nPolicy iteration reached max iterations ({max_iterations})\")\n",
    "    return policy, V, max_iterations\n",
    "\n",
    "\n",
    "def policies_equal(policy1, policy2):\n",
    "    \"\"\"Check if two policies are equal.\"\"\"\n",
    "    if set(policy1.keys()) != set(policy2.keys()):\n",
    "        return False\n",
    "    \n",
    "    for state in policy1:\n",
    "        actions1 = policy1[state]\n",
    "        actions2 = policy2.get(state, {})\n",
    "        \n",
    "        if set(actions1.keys()) != set(actions2.keys()):\n",
    "            return False\n",
    "        \n",
    "        for action in actions1:\n",
    "            if not np.isclose(actions1[action], actions2.get(action, 0)):\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# Demonstrate policy iteration on 2x2 grid\n",
    "print(\"Policy Iteration Demonstration\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Finding optimal policy for 2x2 Grid World...\\n\")\n",
    "\n",
    "# Run policy iteration\n",
    "optimal_policy, optimal_V, num_iters = policy_iteration(mdp, gamma=0.9, theta=0.0001)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nOptimal Policy œÄ*:\")\n",
    "print(\"-\"*60)\n",
    "for state in states:\n",
    "    actions_str = \", \".join([f\"{a}({p:.2f})\" for a, p in optimal_policy[state].items() if p > 0])\n",
    "    print(f\"  œÄ*({state}) = {actions_str}\")\n",
    "\n",
    "print(\"\\nOptimal Value Function V*:\")\n",
    "print(\"-\"*60)\n",
    "for state in states:\n",
    "    print(f\"  V*({state}) = {optimal_V[state]:7.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - Policy iteration found the optimal policy\")\n",
    "print(\"   - From (0,0): Go RIGHT to (0,1)\")\n",
    "print(\"   - From (0,1): Go DOWN to goal (1,1)\")\n",
    "print(\"   - From (1,0): Go RIGHT to goal (1,1)\")\n",
    "print(\"   - This is the shortest path to the goal!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration: A More Efficient Approach\n",
    "\n",
    "**Combining Evaluation and Improvement**\n",
    "\n",
    "Policy iteration works well but can be slow because it fully evaluates each policy. **Value iteration** provides a more efficient alternative by combining evaluation and improvement into a single update.\n",
    "\n",
    "**Value Iteration Algorithm:**\n",
    "\n",
    "Instead of alternating between full policy evaluation and improvement, value iteration updates values using the Bellman optimality equation:\n",
    "\n",
    "$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V_k(s')\\right]$\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "```\n",
    "1. Initialize V(s) = 0 for all states\n",
    "2. Repeat until convergence:\n",
    "   For each state s:\n",
    "     V(s) ‚Üê max_a Œ£ P(s'|s,a)[R(s,a,s') + Œ≥V(s')]\n",
    "3. Extract policy: œÄ(s) = argmax_a Œ£ P(s'|s,a)[R(s,a,s') + Œ≥V(s')]\n",
    "```\n",
    "\n",
    "**Key Differences from Policy Iteration:**\n",
    "\n",
    "1. **No explicit policy**: Works directly with value function\n",
    "2. **Single update**: Combines evaluation and improvement\n",
    "3. **Faster convergence**: Often requires fewer iterations\n",
    "4. **Simpler implementation**: No need to track policy during iteration\n",
    "\n",
    "**Why Value Iteration Works:**\n",
    "\n",
    "- Each update moves V closer to V*\n",
    "- The max operator implicitly improves the policy\n",
    "- Guaranteed to converge to V* (and thus œÄ*)\n",
    "- Convergence rate is exponential in Œ≥\n",
    "\n",
    "**Relationship to Policy Iteration:**\n",
    "\n",
    "Value iteration is like policy iteration with just one sweep of policy evaluation per iteration. Both converge to the same optimal solution, but value iteration is often faster in practice.\n",
    "\n",
    "Let's implement value iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def value_iteration(mdp, gamma=0.9, theta=0.0001, max_iterations=1000):\n",
    "    \"\"\"Find optimal value function and policy using value iteration.\n",
    "    \n",
    "    Args:\n",
    "        mdp: MDP object\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold\n",
    "        max_iterations: Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        V: Optimal value function\n",
    "        policy: Optimal policy\n",
    "        num_iterations: Number of iterations\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        # Update each state\n",
    "        for state in mdp.states:\n",
    "            # Calculate max over actions\n",
    "            action_values = []\n",
    "            \n",
    "            for action in mdp.actions:\n",
    "                q = 0.0\n",
    "                next_states = mdp.transitions.get((state, action), {})\n",
    "                \n",
    "                for next_state, trans_prob in next_states.items():\n",
    "                    reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                    q += trans_prob * (reward + gamma * V[next_state])\n",
    "                \n",
    "                action_values.append(q)\n",
    "            \n",
    "            # Bellman optimality update\n",
    "            if action_values:\n",
    "                V_new[state] = max(action_values)\n",
    "                delta = max(delta, abs(V_new[state] - V[state]))\n",
    "        \n",
    "        V = V_new\n",
    "        \n",
    "        # Check convergence\n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged in {iteration + 1} iterations\")\n",
    "            \n",
    "            # Extract optimal policy\n",
    "            policy = {}\n",
    "            for state in mdp.states:\n",
    "                q_values = {}\n",
    "                for action in mdp.actions:\n",
    "                    q = 0.0\n",
    "                    next_states = mdp.transitions.get((state, action), {})\n",
    "                    for next_state, trans_prob in next_states.items():\n",
    "                        reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                        q += trans_prob * (reward + gamma * V[next_state])\n",
    "                    q_values[action] = q\n",
    "                \n",
    "                # Greedy policy\n",
    "                if q_values:\n",
    "                    max_q = max(q_values.values())\n",
    "                    best_actions = [a for a, q in q_values.items() if np.isclose(q, max_q)]\n",
    "                    policy[state] = {a: 1.0/len(best_actions) for a in best_actions}\n",
    "            \n",
    "            return V, policy, iteration + 1\n",
    "    \n",
    "    print(f\"Value iteration reached max iterations ({max_iterations})\")\n",
    "    \n",
    "    # Extract policy even if not converged\n",
    "    policy = {}\n",
    "    for state in mdp.states:\n",
    "        q_values = {}\n",
    "        for action in mdp.actions:\n",
    "            q = 0.0\n",
    "            next_states = mdp.transitions.get((state, action), {})\n",
    "            for next_state, trans_prob in next_states.items():\n",
    "                reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                q += trans_prob * (reward + gamma * V[next_state])\n",
    "            q_values[action] = q\n",
    "        \n",
    "        if q_values:\n",
    "            max_q = max(q_values.values())\n",
    "            best_actions = [a for a, q in q_values.items() if np.isclose(q, max_q)]\n",
    "            policy[state] = {a: 1.0/len(best_actions) for a in best_actions}\n",
    "    \n",
    "    return V, policy, max_iterations\n",
    "\n",
    "\n",
    "# Demonstrate value iteration\n",
    "print(\"Value Iteration Demonstration\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Finding optimal policy for 2x2 Grid World using Value Iteration...\\n\")\n",
    "\n",
    "# Run value iteration\n",
    "V_opt, policy_opt, num_iters_vi = value_iteration(mdp, gamma=0.9, theta=0.0001)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nOptimal Policy œÄ* (from Value Iteration):\")\n",
    "print(\"-\"*60)\n",
    "for state in states:\n",
    "    actions_str = \", \".join([f\"{a}({p:.2f})\" for a, p in policy_opt[state].items() if p > 0])\n",
    "    print(f\"  œÄ*({state}) = {actions_str}\")\n",
    "\n",
    "print(\"\\nOptimal Value Function V*:\")\n",
    "print(\"-\"*60)\n",
    "for state in states:\n",
    "    print(f\"  V*({state}) = {V_opt[state]:7.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Comparison: Policy Iteration vs Value Iteration\")\n",
    "print(\"\\n  Both methods found the same optimal solution!\")\n",
    "print(f\"  Policy Iteration: {num_iters} iterations\")\n",
    "print(f\"  Value Iteration: {num_iters_vi} iterations\")\n",
    "print(\"\\n  Value iteration is often faster because it doesn't\")\n",
    "print(\"  fully evaluate each intermediate policy.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Value Iteration on 4x4 Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply value iteration to the larger 4x4 grid world\n",
    "print(\"Value Iteration on 4x4 Grid World\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Running value iteration on 4x4 grid with obstacles...\\n\")\n",
    "\n",
    "# Run value iteration\n",
    "V_grid_opt, policy_grid_opt, num_iters_grid = value_iteration(grid_mdp, gamma=0.9, theta=0.001)\n",
    "\n",
    "# Visualize optimal value function\n",
    "value_grid_opt = np.full((4, 4), np.nan)\n",
    "for state_str, value in V_grid_opt.items():\n",
    "    state = eval(state_str)\n",
    "    value_grid_opt[state[0], state[1]] = value\n",
    "\n",
    "# Create visualization with optimal policy arrows\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot 1: Optimal Value Function\n",
    "masked_grid = np.ma.masked_where(np.isnan(value_grid_opt), value_grid_opt)\n",
    "im1 = ax1.imshow(masked_grid, cmap='RdYlGn', aspect='auto', interpolation='nearest')\n",
    "ax1.set_xticks(range(4))\n",
    "ax1.set_yticks(range(4))\n",
    "ax1.set_xlabel('Column', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Row', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Optimal Value Function V*(s)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if not np.isnan(value_grid_opt[i, j]):\n",
    "            ax1.text(j, i, f'{value_grid_opt[i, j]:.2f}',\n",
    "                    ha=\"center\", va=\"center\", color=\"black\", \n",
    "                    fontsize=10, fontweight='bold')\n",
    "        else:\n",
    "            ax1.text(j, i, 'X', ha=\"center\", va=\"center\", \n",
    "                    color=\"white\", fontsize=18, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im1, ax=ax1, label='Value')\n",
    "\n",
    "# Plot 2: Optimal Policy\n",
    "ax2.set_xlim(-0.5, 3.5)\n",
    "ax2.set_ylim(-0.5, 3.5)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_xticks(range(4))\n",
    "ax2.set_yticks(range(4))\n",
    "ax2.set_xlabel('Column', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Row', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Optimal Policy œÄ*(s)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# Draw policy arrows\n",
    "arrow_map = {'UP': (0, -0.3), 'DOWN': (0, 0.3), 'LEFT': (-0.3, 0), 'RIGHT': (0.3, 0)}\n",
    "\n",
    "for state_str, actions in policy_grid_opt.items():\n",
    "    state = eval(state_str)\n",
    "    i, j = state\n",
    "    \n",
    "    # Skip obstacles\n",
    "    if (i, j) in [(1, 1), (2, 2)]:\n",
    "        ax2.add_patch(plt.Rectangle((j-0.4, i-0.4), 0.8, 0.8, \n",
    "                                    fill=True, facecolor='gray', edgecolor='black', linewidth=2))\n",
    "        ax2.text(j, i, 'X', ha='center', va='center', \n",
    "                color='white', fontsize=18, fontweight='bold')\n",
    "        continue\n",
    "    \n",
    "    # Draw arrows for best action(s)\n",
    "    for action, prob in actions.items():\n",
    "        if prob > 0.1:  # Only draw if significant probability\n",
    "            dx, dy = arrow_map.get(action, (0, 0))\n",
    "            ax2.arrow(j, i, dx, dy, head_width=0.15, head_length=0.1,\n",
    "                     fc='blue', ec='blue', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Mark goal\n",
    "ax2.add_patch(plt.Circle((3, 3), 0.3, fill=True, facecolor='gold', \n",
    "                         edgecolor='darkgreen', linewidth=3))\n",
    "ax2.text(3, 3, 'G', ha='center', va='center', \n",
    "        color='darkgreen', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nConverged in {num_iters_grid} iterations\")\n",
    "print(\"\\nOptimal policy shows the best action in each state\")\n",
    "print(\"Arrows point towards the goal, avoiding obstacles\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   1. Value iteration found the optimal policy efficiently\")\n",
    "print(\"   2. Policy directs agent towards goal from any state\")\n",
    "print(\"   3. Obstacles are naturally avoided\")\n",
    "print(\"   4. V*(s) reflects optimal expected return from each state\")\n",
    "print(\"   5. This is the foundation for solving MDPs!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-Based vs Model-Free Reinforcement Learning\n",
    "\n",
    "**A Fundamental Distinction in RL**\n",
    "\n",
    "Now that we've seen Dynamic Programming in action, it's important to understand a fundamental distinction in reinforcement learning: **model-based** vs **model-free** approaches.\n",
    "\n",
    "**Model-Based Reinforcement Learning**\n",
    "\n",
    "**Definition:** The agent has (or learns) a model of the environment's dynamics.\n",
    "\n",
    "**What is a \"model\"?**\n",
    "- Transition probabilities: $P(s'|s,a)$\n",
    "- Reward function: $R(s,a,s')$\n",
    "- Essentially, knowledge of how the environment works\n",
    "\n",
    "**Examples:**\n",
    "- Dynamic Programming (what we just learned!)\n",
    "- Planning algorithms\n",
    "- Simulators (e.g., chess, Go, robotics simulators)\n",
    "- Learned models (agent learns $P$ and $R$ from experience)\n",
    "\n",
    "**Advantages of Model-Based RL:**\n",
    "\n",
    "1. **Sample Efficiency**: Can plan without interacting with environment\n",
    "   - Simulate many trajectories mentally\n",
    "   - No need to try every action in every state\n",
    "   - Particularly valuable when real-world interactions are expensive\n",
    "\n",
    "2. **Faster Learning**: Can use planning algorithms\n",
    "   - Dynamic Programming guarantees optimal solution\n",
    "   - Can reason about consequences before acting\n",
    "   - Update values for all states simultaneously\n",
    "\n",
    "3. **Generalization**: Model can be used for multiple tasks\n",
    "   - Same model, different reward functions\n",
    "   - Transfer learning across related problems\n",
    "   - What-if analysis and counterfactual reasoning\n",
    "\n",
    "4. **Interpretability**: Can understand and debug the model\n",
    "   - Inspect transition probabilities\n",
    "   - Verify model correctness\n",
    "   - Explain agent's reasoning\n",
    "\n",
    "**Disadvantages of Model-Based RL:**\n",
    "\n",
    "1. **Model Errors**: If model is wrong, policy will be suboptimal\n",
    "   - \"All models are wrong, but some are useful\"\n",
    "   - Model errors compound over long horizons\n",
    "   - Difficult to model complex, stochastic environments\n",
    "\n",
    "2. **Computational Cost**: Planning can be expensive\n",
    "   - Need to solve Bellman equations\n",
    "   - Scales poorly with state/action space size\n",
    "   - Curse of dimensionality\n",
    "\n",
    "3. **Model Learning**: Learning accurate models is hard\n",
    "   - Requires lots of data\n",
    "   - High-dimensional state spaces are challenging\n",
    "   - Stochastic environments are difficult to model\n",
    "\n",
    "4. **Availability**: Many real-world problems lack good models\n",
    "   - Human behavior is hard to model\n",
    "   - Complex physical systems\n",
    "   - Unknown or changing dynamics\n",
    "\n",
    "**Model-Free Reinforcement Learning**\n",
    "\n",
    "**Definition:** The agent learns directly from experience without a model.\n",
    "\n",
    "**What does \"model-free\" mean?**\n",
    "- No knowledge of $P(s'|s,a)$ or $R(s,a,s')$\n",
    "- Learns value functions or policies directly\n",
    "- Trial-and-error learning\n",
    "\n",
    "**Examples:**\n",
    "- Monte Carlo methods\n",
    "- Temporal Difference learning (TD, SARSA, Q-learning)\n",
    "- Policy gradient methods\n",
    "- Deep RL (DQN, A3C, PPO)\n",
    "\n",
    "**Advantages of Model-Free RL:**\n",
    "\n",
    "1. **No Model Required**: Works when model is unknown or complex\n",
    "   - Don't need to know environment dynamics\n",
    "   - Can handle black-box environments\n",
    "   - Robust to model misspecification\n",
    "\n",
    "2. **Simpler**: Often easier to implement\n",
    "   - Direct learning from experience\n",
    "   - No need to learn or maintain a model\n",
    "   - Fewer components to debug\n",
    "\n",
    "3. **Scalability**: Can use function approximation\n",
    "   - Neural networks for large state spaces\n",
    "   - Generalization across states\n",
    "   - Handles continuous state/action spaces\n",
    "\n",
    "4. **Robustness**: Less sensitive to model errors\n",
    "   - Learns from actual experience\n",
    "   - Adapts to environment changes\n",
    "   - No compounding of model errors\n",
    "\n",
    "**Disadvantages of Model-Free RL:**\n",
    "\n",
    "1. **Sample Inefficiency**: Requires many interactions\n",
    "   - Must try actions to learn their value\n",
    "   - Can't simulate or plan ahead\n",
    "   - Expensive in real-world applications\n",
    "\n",
    "2. **Slower Learning**: No planning capability\n",
    "   - Must experience each state-action pair\n",
    "   - Can't reason about consequences\n",
    "   - Updates are local (one state at a time)\n",
    "\n",
    "3. **Exploration Challenges**: Must balance exploration/exploitation\n",
    "   - Risk of getting stuck in local optima\n",
    "   - May miss better strategies\n",
    "   - Requires careful exploration strategy\n",
    "\n",
    "4. **No Generalization Across Tasks**: Learns for specific reward\n",
    "   - Must relearn if reward function changes\n",
    "   - Limited transfer learning\n",
    "   - Task-specific knowledge\n",
    "\n",
    "**Why Model-Free Methods Are Needed**\n",
    "\n",
    "Despite the advantages of model-based RL, model-free methods are essential because:\n",
    "\n",
    "1. **Real-World Complexity**: Most real-world environments are too complex to model accurately\n",
    "   - Human interactions, market dynamics, weather patterns\n",
    "   - High-dimensional, stochastic, non-stationary\n",
    "\n",
    "2. **Unknown Dynamics**: Often we don't know how the environment works\n",
    "   - Black-box systems\n",
    "   - Proprietary or inaccessible internals\n",
    "\n",
    "3. **Model Errors Are Costly**: Wrong models lead to wrong policies\n",
    "   - Model-free methods learn from ground truth\n",
    "   - More robust in practice\n",
    "\n",
    "4. **Scalability**: Model-free + function approximation handles large spaces\n",
    "   - Deep RL successes (Atari, Go, robotics)\n",
    "   - Continuous control problems\n",
    "\n",
    "5. **Simplicity**: Easier to implement and debug\n",
    "   - Fewer moving parts\n",
    "   - Direct optimization of objective\n",
    "\n",
    "**The Spectrum: Hybrid Approaches**\n",
    "\n",
    "Modern RL often combines both approaches:\n",
    "\n",
    "- **Dyna**: Model-free learning + model-based planning\n",
    "- **Model-Based RL with Learned Models**: Learn model from data, use for planning\n",
    "- **Imagination-Augmented Agents**: Use model for auxiliary predictions\n",
    "- **World Models**: Learn compact model, train policy in model\n",
    "\n",
    "**When to Use Each Approach?**\n",
    "\n",
    "**Use Model-Based RL when:**\n",
    "- You have an accurate model (simulator, known dynamics)\n",
    "- Sample efficiency is critical (expensive interactions)\n",
    "- State space is small enough for planning\n",
    "- You need interpretability\n",
    "- Multiple tasks with same dynamics\n",
    "\n",
    "**Use Model-Free RL when:**\n",
    "- Model is unknown or too complex\n",
    "- Environment is high-dimensional\n",
    "- You have abundant data/interactions\n",
    "- Robustness to model errors is important\n",
    "- Simplicity is preferred\n",
    "\n",
    "Let's visualize this distinction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a comparison visualization\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Model-Based RL Diagram\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Model-Based Reinforcement Learning', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Agent\n",
    "agent_box = mpatches.FancyBboxPatch((1, 7), 2, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                    edgecolor='blue', facecolor='lightblue', linewidth=3)\n",
    "ax1.add_patch(agent_box)\n",
    "ax1.text(2, 7.75, 'Agent', ha='center', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Model\n",
    "model_box = mpatches.FancyBboxPatch((4, 7), 2, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                    edgecolor='green', facecolor='lightgreen', linewidth=3)\n",
    "ax1.add_patch(model_box)\n",
    "ax1.text(5, 7.75, 'Model\\nP(s\\'|s,a)\\nR(s,a,s\\')', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Environment\n",
    "env_box = mpatches.FancyBboxPatch((7, 7), 2, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                  edgecolor='red', facecolor='lightcoral', linewidth=3)\n",
    "ax1.add_patch(env_box)\n",
    "ax1.text(8, 7.75, 'Environment', ha='center', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Arrows\n",
    "ax1.annotate('', xy=(4, 7.75), xytext=(3, 7.75), \n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "ax1.text(3.5, 8.2, 'Query', ha='center', fontsize=10)\n",
    "\n",
    "ax1.annotate('', xy=(7, 7.75), xytext=(6, 7.75), \n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "ax1.text(6.5, 8.2, 'Action', ha='center', fontsize=10)\n",
    "\n",
    "ax1.annotate('', xy=(8, 7), xytext=(8, 6.5), \n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "ax1.annotate('', xy=(2, 7), xytext=(2, 6.5), \n",
    "            arrowprops=dict(arrowstyle='<-', lw=2, color='black'))\n",
    "ax1.text(5, 6.2, 'State, Reward', ha='center', fontsize=10)\n",
    "\n",
    "# Planning\n",
    "plan_box = mpatches.FancyBboxPatch((1, 4), 5, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                   edgecolor='purple', facecolor='plum', linewidth=3, linestyle='--')\n",
    "ax1.add_patch(plan_box)\n",
    "ax1.text(3.5, 4.75, 'Planning\\n(DP, Value Iteration)', ha='center', va='center', \n",
    "        fontsize=12, fontweight='bold')\n",
    "\n",
    "ax1.annotate('', xy=(3.5, 5.5), xytext=(3.5, 7), \n",
    "            arrowprops=dict(arrowstyle='<->', lw=2, color='purple', linestyle='--'))\n",
    "\n",
    "# Advantages/Disadvantages\n",
    "ax1.text(5, 2.5, 'Advantages:', fontsize=12, fontweight='bold', color='green')\n",
    "ax1.text(5, 2, '‚Ä¢ Sample efficient', fontsize=10, ha='center')\n",
    "ax1.text(5, 1.6, '‚Ä¢ Can plan ahead', fontsize=10, ha='center')\n",
    "ax1.text(5, 1.2, '‚Ä¢ Fast learning', fontsize=10, ha='center')\n",
    "\n",
    "ax1.text(5, 0.5, 'Disadvantages:', fontsize=12, fontweight='bold', color='red')\n",
    "ax1.text(5, 0, '‚Ä¢ Requires accurate model', fontsize=10, ha='center')\n",
    "ax1.text(5, -0.4, '‚Ä¢ Model errors compound', fontsize=10, ha='center')\n",
    "\n",
    "# Model-Free RL Diagram\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Model-Free Reinforcement Learning', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Agent\n",
    "agent_box2 = mpatches.FancyBboxPatch((2, 7), 2.5, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                     edgecolor='blue', facecolor='lightblue', linewidth=3)\n",
    "ax2.add_patch(agent_box2)\n",
    "ax2.text(3.25, 7.75, 'Agent\\n(Q-learning,\\nPolicy Gradient)', ha='center', va='center', \n",
    "        fontsize=11, fontweight='bold')\n",
    "\n",
    "# Environment\n",
    "env_box2 = mpatches.FancyBboxPatch((5.5, 7), 2.5, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                   edgecolor='red', facecolor='lightcoral', linewidth=3)\n",
    "ax2.add_patch(env_box2)\n",
    "ax2.text(6.75, 7.75, 'Environment', ha='center', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Direct interaction arrows\n",
    "ax2.annotate('', xy=(5.5, 8), xytext=(4.5, 8), \n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='black'))\n",
    "ax2.text(5, 8.5, 'Action', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax2.annotate('', xy=(4.5, 7.5), xytext=(5.5, 7.5), \n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='black'))\n",
    "ax2.text(5, 7, 'State, Reward', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Direct learning\n",
    "learn_box = mpatches.FancyBboxPatch((2, 4.5), 6, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                    edgecolor='orange', facecolor='lightyellow', linewidth=3)\n",
    "ax2.add_patch(learn_box)\n",
    "ax2.text(5, 5.25, 'Direct Learning from Experience\\n(No Model)', ha='center', va='center', \n",
    "        fontsize=12, fontweight='bold')\n",
    "\n",
    "ax2.annotate('', xy=(5, 6), xytext=(5, 7), \n",
    "            arrowprops=dict(arrowstyle='<->', lw=2, color='orange'))\n",
    "\n",
    "# Advantages/Disadvantages\n",
    "ax2.text(5, 2.5, 'Advantages:', fontsize=12, fontweight='bold', color='green')\n",
    "ax2.text(5, 2, '‚Ä¢ No model required', fontsize=10, ha='center')\n",
    "ax2.text(5, 1.6, '‚Ä¢ Robust to model errors', fontsize=10, ha='center')\n",
    "ax2.text(5, 1.2, '‚Ä¢ Simpler implementation', fontsize=10, ha='center')\n",
    "\n",
    "ax2.text(5, 0.5, 'Disadvantages:', fontsize=12, fontweight='bold', color='red')\n",
    "ax2.text(5, 0, '‚Ä¢ Sample inefficient', fontsize=10, ha='center')\n",
    "ax2.text(5, -0.4, '‚Ä¢ Slower learning', fontsize=10, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Model-Based vs Model-Free RL\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nModel-Based RL (e.g., Dynamic Programming):\")\n",
    "print(\"  ‚Ä¢ Uses model of environment (P, R)\")\n",
    "print(\"  ‚Ä¢ Can plan without interacting\")\n",
    "print(\"  ‚Ä¢ Sample efficient but requires accurate model\")\n",
    "print(\"  ‚Ä¢ Examples: DP, Dyna, Model-based planning\")\n",
    "\n",
    "print(\"\\nModel-Free RL (e.g., Q-learning):\")\n",
    "print(\"  ‚Ä¢ Learns directly from experience\")\n",
    "print(\"  ‚Ä¢ No model of environment needed\")\n",
    "print(\"  ‚Ä¢ Sample inefficient but robust\")\n",
    "print(\"  ‚Ä¢ Examples: Monte Carlo, TD, Q-learning, Policy Gradients\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüéØ Key Takeaway:\")\n",
    "print(\"\\n   Dynamic Programming is model-based - it requires perfect\")\n",
    "print(\"   knowledge of the environment. In the next sections, we'll\")\n",
    "print(\"   learn model-free methods that work without this knowledge!\")\n",
    "print(\"\\n   Model-free methods are essential for real-world RL where\")\n",
    "print(\"   we don't have access to perfect models.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary: Dynamic Programming and Learning Paradigms**\n",
    "\n",
    "In this section, we've covered:\n",
    "\n",
    "1. **Policy Evaluation**: Computing $V^\\pi(s)$ for a given policy using iterative Bellman updates\n",
    "2. **Policy Improvement**: Finding better policies by acting greedily with respect to value functions\n",
    "3. **Value Iteration**: Efficiently finding optimal policies by combining evaluation and improvement\n",
    "4. **Model-Based vs Model-Free**: Understanding when we need models and when we can learn without them\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "- Dynamic Programming provides exact solutions when we have perfect models\n",
    "- Policy iteration and value iteration both converge to optimal policies\n",
    "- The Bellman equations are the foundation for all these algorithms\n",
    "- Model-based methods are sample-efficient but require accurate models\n",
    "- Model-free methods are more practical for real-world problems\n",
    "\n",
    "**What's Next:**\n",
    "\n",
    "In the following sections, we'll explore model-free methods that learn directly from experience:\n",
    "- Monte Carlo methods (learn from complete episodes)\n",
    "- Temporal Difference learning (learn from every step)\n",
    "- Q-learning (off-policy TD control)\n",
    "- Deep RL (handling large state spaces)\n",
    "\n",
    "These methods form the foundation of modern reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Section 2: Core Algorithms\n",
    "\n",
    "In this section, we'll explore the fundamental algorithms that enable agents to learn optimal policies. We'll start with Monte Carlo methods, which learn from complete episodes, then progress to Temporal Difference methods that can learn from individual steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='monte-carlo'></a>\n",
    "### Monte Carlo Methods\n",
    "\n",
    "**Learning from Complete Episodes**\n",
    "\n",
    "Monte Carlo (MC) methods are a class of reinforcement learning algorithms that learn by averaging sample returns from complete episodes. Unlike Dynamic Programming, MC methods don't require a model of the environment - they learn directly from experience.\n",
    "\n",
    "**The Core Principle:**\n",
    "\n",
    "Monte Carlo methods estimate value functions by **averaging the actual returns** observed after visiting states. The key insight is:\n",
    "\n",
    "*\"The value of a state is the expected return starting from that state. If we experience many episodes and average the returns, we'll get a good estimate of the true value.\"*\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "1. **Episode-Based Learning**: Must wait until episode ends to update values\n",
    "2. **Model-Free**: Don't need to know transition probabilities or rewards\n",
    "3. **Sample-Based**: Learn from actual experience, not from a model\n",
    "4. **Unbiased Estimates**: Returns are actual outcomes, not bootstrapped estimates\n",
    "5. **High Variance**: Individual returns can vary significantly\n",
    "\n",
    "**When to Use Monte Carlo Methods:**\n",
    "\n",
    "- ‚úì Episodic tasks (games, simulations with clear endings)\n",
    "- ‚úì When you don't have a model of the environment\n",
    "- ‚úì When you can simulate or experience complete episodes\n",
    "- ‚úó Continuing tasks (no natural episode boundaries)\n",
    "- ‚úó When episodes are very long (slow learning)\n",
    "\n",
    "**Comparison with Dynamic Programming:**\n",
    "\n",
    "| Aspect | Dynamic Programming | Monte Carlo |\n",
    "|--------|-------------------|-------------|\n",
    "| Model Required | Yes (need P and R) | No (model-free) |\n",
    "| Learning | From model | From experience |\n",
    "| Updates | Every state | Only visited states |\n",
    "| Bootstrapping | Yes (use estimates) | No (use actual returns) |\n",
    "| Variance | Low | High |\n",
    "| Bias | Depends on initialization | Unbiased |\n",
    "\n",
    "Let's explore Monte Carlo methods in detail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Prediction: Estimating Value Functions\n",
    "\n",
    "**The Goal:** Estimate the state-value function $V^\\pi(s)$ for a given policy $\\pi$.\n",
    "\n",
    "**The Approach:** \n",
    "1. Follow policy $\\pi$ to generate episodes\n",
    "2. For each state visited, record the return (cumulative discounted reward)\n",
    "3. Average the returns to estimate the value\n",
    "\n",
    "**Two Variants: First-Visit vs Every-Visit MC**\n",
    "\n",
    "**First-Visit Monte Carlo:**\n",
    "- Only count the **first time** a state is visited in an episode\n",
    "- Average returns from first visits only\n",
    "- Theoretically guaranteed to converge to true value\n",
    "- More commonly used in practice\n",
    "\n",
    "**Every-Visit Monte Carlo:**\n",
    "- Count **every time** a state is visited in an episode\n",
    "- Average returns from all visits\n",
    "- Also converges to true value\n",
    "- Can learn faster in some cases\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "For a state $s$ visited at time $t$ in an episode:\n",
    "\n",
    "**Return from that visit:**\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
    "\n",
    "**Value estimate (after $n$ visits):**\n",
    "$V(s) = \\frac{1}{n} \\sum_{i=1}^{n} G_i(s)$\n",
    "\n",
    "where $G_i(s)$ is the return following the $i$-th visit to state $s$.\n",
    "\n",
    "**Incremental Update Formula:**\n",
    "\n",
    "Instead of storing all returns and averaging, we can update incrementally:\n",
    "\n",
    "$V(s) \\leftarrow V(s) + \\frac{1}{N(s)} [G - V(s)]$\n",
    "\n",
    "where:\n",
    "- $N(s)$ = number of times state $s$ has been visited\n",
    "- $G$ = return observed from this visit\n",
    "- $\\frac{1}{N(s)}$ = step size (learning rate)\n",
    "\n",
    "This is equivalent to:\n",
    "$V(s) \\leftarrow V(s) + \\alpha [G - V(s)]$\n",
    "\n",
    "where $\\alpha$ is a constant step size (useful for non-stationary problems).\n",
    "\n",
    "Let's implement both first-visit and every-visit Monte Carlo prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_episode(env, policy, max_steps=100):\n",
    "    \"\"\"Generate an episode following a given policy.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment with reset() and step() methods\n",
    "        policy: Function that takes state and returns action\n",
    "        max_steps: Maximum steps per episode\n",
    "        \n",
    "    Returns:\n",
    "        episode: List of (state, action, reward) tuples\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = policy(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        episode.append((state, action, reward))\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return episode\n",
    "\n",
    "\n",
    "def calculate_returns(episode, gamma=0.9):\n",
    "    \"\"\"Calculate returns for each step in an episode.\n",
    "    \n",
    "    Args:\n",
    "        episode: List of (state, action, reward) tuples\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        returns: List of returns, one for each step\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    \n",
    "    # Calculate returns backwards from end of episode\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = reward + gamma * G\n",
    "        returns.insert(0, G)  # Insert at beginning to maintain order\n",
    "    \n",
    "    return returns\n",
    "\n",
    "\n",
    "def mc_prediction_first_visit(env, policy, num_episodes=1000, gamma=0.9):\n",
    "    \"\"\"First-visit Monte Carlo prediction.\n",
    "    \n",
    "    Estimates V(s) by averaging returns from first visits to each state.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        policy: Policy to evaluate (function: state -> action)\n",
    "        num_episodes: Number of episodes to generate\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        V: Dictionary mapping states to estimated values\n",
    "        returns_history: List of returns for each episode (for visualization)\n",
    "    \"\"\"\n",
    "    # Initialize value function and visit counts\n",
    "    V = defaultdict(float)\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    returns_history = []\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode\n",
    "        episode = generate_episode(env, policy)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Track total return for this episode\n",
    "        returns_history.append(returns[0] if returns else 0)\n",
    "        \n",
    "        # Track which states we've seen in this episode (for first-visit)\n",
    "        visited_states = set()\n",
    "        \n",
    "        # Update value estimates\n",
    "        for t, (state, action, reward) in enumerate(episode):\n",
    "            # First-visit: only update if this is the first time seeing this state\n",
    "            if state not in visited_states:\n",
    "                visited_states.add(state)\n",
    "                \n",
    "                # Add return to sum\n",
    "                returns_sum[state] += returns[t]\n",
    "                returns_count[state] += 1\n",
    "                \n",
    "                # Update value estimate (average of returns)\n",
    "                V[state] = returns_sum[state] / returns_count[state]\n",
    "    \n",
    "    return dict(V), returns_history\n",
    "\n",
    "\n",
    "def mc_prediction_every_visit(env, policy, num_episodes=1000, gamma=0.9):\n",
    "    \"\"\"Every-visit Monte Carlo prediction.\n",
    "    \n",
    "    Estimates V(s) by averaging returns from all visits to each state.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        policy: Policy to evaluate (function: state -> action)\n",
    "        num_episodes: Number of episodes to generate\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        V: Dictionary mapping states to estimated values\n",
    "        returns_history: List of returns for each episode (for visualization)\n",
    "    \"\"\"\n",
    "    # Initialize value function and visit counts\n",
    "    V = defaultdict(float)\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    returns_history = []\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode\n",
    "        episode = generate_episode(env, policy)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Track total return for this episode\n",
    "        returns_history.append(returns[0] if returns else 0)\n",
    "        \n",
    "        # Update value estimates\n",
    "        for t, (state, action, reward) in enumerate(episode):\n",
    "            # Every-visit: update for every occurrence of the state\n",
    "            returns_sum[state] += returns[t]\n",
    "            returns_count[state] += 1\n",
    "            \n",
    "            # Update value estimate (average of returns)\n",
    "            V[state] = returns_sum[state] / returns_count[state]\n",
    "    \n",
    "    return dict(V), returns_history\n",
    "\n",
    "\n",
    "print(\"Monte Carlo Prediction Implementation\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nImplemented:\")\n",
    "print(\"  ‚úì First-Visit MC Prediction\")\n",
    "print(\"  ‚úì Every-Visit MC Prediction\")\n",
    "print(\"  ‚úì Episode generation\")\n",
    "print(\"  ‚úì Return calculation\")\n",
    "print(\"\\nReady to evaluate policies on episodic environments!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrating Monte Carlo Prediction on Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a simple policy for the grid world\n",
    "def random_policy(state):\n",
    "    \"\"\"Random policy: choose actions uniformly at random.\"\"\"\n",
    "    return np.random.randint(0, 4)  # 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "\n",
    "\n",
    "def greedy_policy(state):\n",
    "    \"\"\"Greedy policy: always move toward goal (4,4).\"\"\"\n",
    "    row, col = state\n",
    "    goal_row, goal_col = 4, 4\n",
    "    \n",
    "    # Move right if not at rightmost column\n",
    "    if col < goal_col:\n",
    "        return 1  # RIGHT\n",
    "    # Move down if not at bottom row\n",
    "    elif row < goal_row:\n",
    "        return 2  # DOWN\n",
    "    # Otherwise move randomly\n",
    "    else:\n",
    "        return np.random.randint(0, 4)\n",
    "\n",
    "\n",
    "print(\"Evaluating Policies with Monte Carlo Prediction\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnvironment(grid_size=5)\n",
    "\n",
    "print(\"\\nEnvironment: 5x5 Grid World\")\n",
    "print(f\"  Start: (0,0)\")\n",
    "print(f\"  Goal: {env.goal_pos}\")\n",
    "print(f\"  Obstacles: {env.obstacles}\")\n",
    "\n",
    "# Evaluate random policy\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Evaluating Random Policy with First-Visit MC...\")\n",
    "V_random_fv, returns_random_fv = mc_prediction_first_visit(\n",
    "    env, random_policy, num_episodes=5000, gamma=0.9\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating Random Policy with Every-Visit MC...\")\n",
    "V_random_ev, returns_random_ev = mc_prediction_every_visit(\n",
    "    env, random_policy, num_episodes=5000, gamma=0.9\n",
    ")\n",
    "\n",
    "# Evaluate greedy policy\n",
    "print(\"\\nEvaluating Greedy Policy with First-Visit MC...\")\n",
    "V_greedy_fv, returns_greedy_fv = mc_prediction_first_visit(\n",
    "    env, greedy_policy, num_episodes=5000, gamma=0.9\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating Greedy Policy with Every-Visit MC...\")\n",
    "V_greedy_ev, returns_greedy_ev = mc_prediction_every_visit(\n",
    "    env, greedy_policy, num_episodes=5000, gamma=0.9\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚úì Evaluation complete!\")\n",
    "print(f\"\\nRandom Policy:\")\n",
    "print(f\"  States evaluated: {len(V_random_fv)}\")\n",
    "print(f\"  Start state value (First-Visit): {V_random_fv.get((0,0), 0):.2f}\")\n",
    "print(f\"  Start state value (Every-Visit): {V_random_ev.get((0,0), 0):.2f}\")\n",
    "\n",
    "print(f\"\\nGreedy Policy:\")\n",
    "print(f\"  States evaluated: {len(V_greedy_fv)}\")\n",
    "print(f\"  Start state value (First-Visit): {V_greedy_fv.get((0,0), 0):.2f}\")\n",
    "print(f\"  Start state value (Every-Visit): {V_greedy_ev.get((0,0), 0):.2f}\")\n",
    "\n",
    "print(\"\\nüí° Observation: Greedy policy has higher value (reaches goal faster)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Value Function Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize how value estimates converge over episodes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Random Policy - Returns over episodes\n",
    "ax = axes[0, 0]\n",
    "window = 100\n",
    "smoothed_random_fv = np.convolve(returns_random_fv, np.ones(window)/window, mode='valid')\n",
    "smoothed_random_ev = np.convolve(returns_random_ev, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax.plot(smoothed_random_fv, label='First-Visit', linewidth=2, alpha=0.8)\n",
    "ax.plot(smoothed_random_ev, label='Every-Visit', linewidth=2, alpha=0.8)\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Average Return', fontsize=11)\n",
    "ax.set_title('Random Policy: Return Convergence', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Greedy Policy - Returns over episodes\n",
    "ax = axes[0, 1]\n",
    "smoothed_greedy_fv = np.convolve(returns_greedy_fv, np.ones(window)/window, mode='valid')\n",
    "smoothed_greedy_ev = np.convolve(returns_greedy_ev, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax.plot(smoothed_greedy_fv, label='First-Visit', linewidth=2, alpha=0.8, color='green')\n",
    "ax.plot(smoothed_greedy_ev, label='Every-Visit', linewidth=2, alpha=0.8, color='lightgreen')\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Average Return', fontsize=11)\n",
    "ax.set_title('Greedy Policy: Return Convergence', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Value function heatmap for Random Policy (First-Visit)\n",
    "ax = axes[1, 0]\n",
    "value_grid = np.zeros((5, 5))\n",
    "for (row, col), value in V_random_fv.items():\n",
    "    value_grid[row, col] = value\n",
    "\n",
    "im = ax.imshow(value_grid, cmap='RdYlGn', aspect='auto')\n",
    "ax.set_title('Random Policy: Value Function (First-Visit)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Column', fontsize=11)\n",
    "ax.set_ylabel('Row', fontsize=11)\n",
    "\n",
    "# Add value labels\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        text = ax.text(j, i, f'{value_grid[i, j]:.1f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 4: Value function heatmap for Greedy Policy (First-Visit)\n",
    "ax = axes[1, 1]\n",
    "value_grid = np.zeros((5, 5))\n",
    "for (row, col), value in V_greedy_fv.items():\n",
    "    value_grid[row, col] = value\n",
    "\n",
    "im = ax.imshow(value_grid, cmap='RdYlGn', aspect='auto')\n",
    "ax.set_title('Greedy Policy: Value Function (First-Visit)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Column', fontsize=11)\n",
    "ax.set_ylabel('Row', fontsize=11)\n",
    "\n",
    "# Add value labels\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        text = ax.text(j, i, f'{value_grid[i, j]:.1f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization Insights:\")\n",
    "print(\"\\n1. Top Row: Returns converge as more episodes are sampled\")\n",
    "print(\"   - First-visit and every-visit produce similar estimates\")\n",
    "print(\"   - Greedy policy achieves higher returns than random\")\n",
    "\n",
    "print(\"\\n2. Bottom Row: Value function heatmaps\")\n",
    "print(\"   - Brighter colors = higher values (closer to goal)\")\n",
    "print(\"   - Values increase as we approach goal state (4,4)\")\n",
    "print(\"   - Greedy policy has higher values overall\")\n",
    "\n",
    "print(\"\\n3. Key Takeaway:\")\n",
    "print(\"   Monte Carlo successfully estimates state values from experience!\")\n",
    "print(\"   No model required - just sample episodes and average returns.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On-Policy Monte Carlo Control\n",
    "\n",
    "**From Prediction to Control: Learning Optimal Policies**\n",
    "\n",
    "So far, we've used Monte Carlo methods for **prediction** - evaluating a given policy. Now we'll use MC for **control** - finding the optimal policy.\n",
    "\n",
    "**The Control Problem:**\n",
    "\n",
    "Given: An environment (MDP without model)\n",
    "Goal: Find the optimal policy $\\pi^*$ that maximizes expected return\n",
    "\n",
    "**On-Policy Learning:**\n",
    "\n",
    "In **on-policy** learning, we learn about and improve the same policy that we're using to generate behavior. The agent:\n",
    "1. Acts according to policy $\\pi$\n",
    "2. Learns from that experience\n",
    "3. Improves policy $\\pi$\n",
    "4. Repeats\n",
    "\n",
    "This contrasts with **off-policy** learning (covered later), where the agent learns about one policy while following another.\n",
    "\n",
    "**The Algorithm: Monte Carlo Control with Epsilon-Greedy**\n",
    "\n",
    "We can't use a purely greedy policy (always exploit) because we need exploration. The solution: **epsilon-greedy exploration**.\n",
    "\n",
    "**Key Idea:** Instead of learning $V(s)$, we learn $Q(s,a)$ (action-values), which tells us the value of taking action $a$ in state $s$.\n",
    "\n",
    "**Algorithm Steps:**\n",
    "\n",
    "1. **Initialize**: \n",
    "   - $Q(s,a) = 0$ for all states and actions\n",
    "   - $\\pi$ = epsilon-greedy policy based on $Q$\n",
    "\n",
    "2. **Repeat** for many episodes:\n",
    "   - **Generate episode** following $\\pi$: $S_0, A_0, R_1, S_1, A_1, R_2, ..., S_T$\n",
    "   - **For each** state-action pair $(s,a)$ in the episode:\n",
    "     - Calculate return $G$ following first visit to $(s,a)$\n",
    "     - Update: $Q(s,a) \\leftarrow \\text{average of returns following } (s,a)$\n",
    "   - **Improve policy**: $\\pi \\leftarrow$ epsilon-greedy with respect to $Q$\n",
    "\n",
    "**Epsilon-Greedy Policy:**\n",
    "\n",
    "$\\pi(a|s) = \\begin{cases}\n",
    "1 - \\epsilon + \\frac{\\epsilon}{|A(s)|} & \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\\n",
    "\\frac{\\epsilon}{|A(s)|} & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "**Why This Works:**\n",
    "\n",
    "1. **Exploration**: Epsilon-greedy ensures all actions are tried\n",
    "2. **Exploitation**: Mostly choose actions with highest Q-values\n",
    "3. **Improvement**: Policy gets better as Q-values become more accurate\n",
    "4. **Convergence**: Under certain conditions, converges to optimal epsilon-greedy policy\n",
    "\n",
    "**Generalized Policy Iteration (GPI):**\n",
    "\n",
    "MC Control is an instance of GPI:\n",
    "- **Policy Evaluation**: Estimate $Q^\\pi$ using MC sampling\n",
    "- **Policy Improvement**: Make policy greedy with respect to $Q$\n",
    "- **Iterate**: These two processes work together to find $\\pi^*$\n",
    "\n",
    "Let's implement on-policy MC control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "    \"\"\"Epsilon-greedy policy based on Q-values.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon=0.1, num_actions=4):\n",
    "        \"\"\"Initialize epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            epsilon: Probability of random action\n",
    "            num_actions: Number of possible actions\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.num_actions = num_actions\n",
    "        self.Q = defaultdict(lambda: np.zeros(num_actions))\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy strategy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            \n",
    "        Returns:\n",
    "            action: Selected action\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return np.random.randint(0, self.num_actions)\n",
    "        else:\n",
    "            # Exploit: best action according to Q\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        \"\"\"Get the greedy action (for evaluation).\"\"\"\n",
    "        return np.argmax(self.Q[state])\n",
    "\n",
    "\n",
    "def mc_control_on_policy(env, num_episodes=10000, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"On-policy Monte Carlo control with epsilon-greedy exploration.\n",
    "    \n",
    "    Learns optimal policy by:\n",
    "    1. Generating episodes with epsilon-greedy policy\n",
    "    2. Updating Q-values from returns\n",
    "    3. Improving policy to be greedy w.r.t. Q\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        num_episodes: Number of episodes to run\n",
    "        gamma: Discount factor\n",
    "        epsilon: Exploration probability\n",
    "        \n",
    "    Returns:\n",
    "        policy: Learned epsilon-greedy policy\n",
    "        Q: Learned action-value function\n",
    "        stats: Dictionary with learning statistics\n",
    "    \"\"\"\n",
    "    # Initialize policy\n",
    "    policy = EpsilonGreedyPolicy(epsilon=epsilon, num_actions=4)\n",
    "    \n",
    "    # Track returns for each state-action pair\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    \n",
    "    # Statistics for tracking progress\n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode using current policy\n",
    "        episode = generate_episode(env, policy.get_action, max_steps=100)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Track statistics\n",
    "        episode_returns.append(returns[0] if returns else 0)\n",
    "        episode_lengths.append(len(episode))\n",
    "        \n",
    "        # Track visited state-action pairs (for first-visit)\n",
    "        visited_pairs = set()\n",
    "        \n",
    "        # Update Q-values\n",
    "        for t, (state, action, reward) in enumerate(episode):\n",
    "            pair = (state, action)\n",
    "            \n",
    "            # First-visit MC\n",
    "            if pair not in visited_pairs:\n",
    "                visited_pairs.add(pair)\n",
    "                \n",
    "                # Update return statistics\n",
    "                returns_sum[pair] += returns[t]\n",
    "                returns_count[pair] += 1\n",
    "                \n",
    "                # Update Q-value (average of returns)\n",
    "                policy.Q[state][action] = returns_sum[pair] / returns_count[pair]\n",
    "        \n",
    "        # Policy improvement happens automatically through epsilon-greedy\n",
    "        # (policy always acts epsilon-greedy w.r.t. current Q)\n",
    "    \n",
    "    stats = {\n",
    "        'episode_returns': episode_returns,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'states_visited': len(policy.Q)\n",
    "    }\n",
    "    \n",
    "    return policy, dict(policy.Q), stats\n",
    "\n",
    "\n",
    "print(\"On-Policy Monte Carlo Control Implementation\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nImplemented:\")\n",
    "print(\"  ‚úì Epsilon-greedy policy class\")\n",
    "print(\"  ‚úì On-policy MC control algorithm\")\n",
    "print(\"  ‚úì Q-value learning from episodes\")\n",
    "print(\"  ‚úì Policy improvement through GPI\")\n",
    "print(\"\\nReady to learn optimal policies!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Optimal Policy in Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Learn optimal policy using MC control\n",
    "print(\"Learning Optimal Policy with On-Policy MC Control\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnvironment(grid_size=5)\n",
    "\n",
    "print(\"\\nEnvironment: 5x5 Grid World\")\n",
    "print(f\"  Start: (0,0)\")\n",
    "print(f\"  Goal: {env.goal_pos}\")\n",
    "print(f\"  Obstacles: {env.obstacles}\")\n",
    "print(f\"  Actions: {env.actions}\")\n",
    "\n",
    "print(\"\\nTraining agent with MC control...\")\n",
    "print(\"  Episodes: 10,000\")\n",
    "print(\"  Epsilon: 0.1\")\n",
    "print(\"  Gamma: 0.9\")\n",
    "\n",
    "# Train agent\n",
    "policy, Q, stats = mc_control_on_policy(\n",
    "    env, \n",
    "    num_episodes=10000, \n",
    "    gamma=0.9, \n",
    "    epsilon=0.1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")\n",
    "print(f\"\\nLearning Statistics:\")\n",
    "print(f\"  States visited: {stats['states_visited']}\")\n",
    "print(f\"  Final average return: {np.mean(stats['episode_returns'][-100:]):.2f}\")\n",
    "print(f\"  Final average episode length: {np.mean(stats['episode_lengths'][-100:]):.1f}\")\n",
    "\n",
    "# Show learned policy for some key states\n",
    "print(\"\\nLearned Policy (greedy actions):\")\n",
    "print(\"-\" * 40)\n",
    "key_states = [(0,0), (0,1), (1,0), (2,0), (3,3), (4,3)]\n",
    "for state in key_states:\n",
    "    if state in Q:\n",
    "        action = policy.get_greedy_action(state)\n",
    "        action_name = env.actions[action]\n",
    "        q_values = Q[state]\n",
    "        print(f\"  State {state}: {action_name} (Q-values: {q_values})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Policy Improvement Over Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize learning progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Episode returns over time\n",
    "ax = axes[0, 0]\n",
    "window = 100\n",
    "smoothed_returns = np.convolve(stats['episode_returns'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(smoothed_returns, linewidth=2, color='blue', alpha=0.8)\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Average Return', fontsize=11)\n",
    "ax.set_title('Learning Progress: Episode Returns', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=np.mean(smoothed_returns[-100:]), color='red', linestyle='--', \n",
    "           alpha=0.5, label=f'Final: {np.mean(smoothed_returns[-100:]):.1f}')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Episode lengths over time\n",
    "ax = axes[0, 1]\n",
    "smoothed_lengths = np.convolve(stats['episode_lengths'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(smoothed_lengths, linewidth=2, color='green', alpha=0.8)\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Episode Length (steps)', fontsize=11)\n",
    "ax.set_title('Learning Progress: Episode Lengths', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=np.mean(smoothed_lengths[-100:]), color='red', linestyle='--', \n",
    "           alpha=0.5, label=f'Final: {np.mean(smoothed_lengths[-100:]):.1f}')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Learned Q-values heatmap (max Q for each state)\n",
    "ax = axes[1, 0]\n",
    "q_grid = np.zeros((5, 5))\n",
    "for (row, col), q_values in Q.items():\n",
    "    q_grid[row, col] = np.max(q_values)\n",
    "\n",
    "im = ax.imshow(q_grid, cmap='RdYlGn', aspect='auto')\n",
    "ax.set_title('Learned Q-Values (max over actions)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Column', fontsize=11)\n",
    "ax.set_ylabel('Row', fontsize=11)\n",
    "\n",
    "# Add value labels\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        text = ax.text(j, i, f'{q_grid[i, j]:.1f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 4: Learned policy visualization\n",
    "ax = axes[1, 1]\n",
    "policy_grid = np.full((5, 5), -1)\n",
    "for (row, col), q_values in Q.items():\n",
    "    policy_grid[row, col] = np.argmax(q_values)\n",
    "\n",
    "# Create custom colormap for actions\n",
    "from matplotlib.colors import ListedColormap\n",
    "colors = ['white', 'lightblue', 'lightgreen', 'lightyellow', 'lightcoral']\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "im = ax.imshow(policy_grid, cmap=cmap, aspect='auto', vmin=-1, vmax=3)\n",
    "ax.set_title('Learned Policy (Greedy Actions)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Column', fontsize=11)\n",
    "ax.set_ylabel('Row', fontsize=11)\n",
    "\n",
    "# Add action arrows\n",
    "arrow_map = {0: '‚Üë', 1: '‚Üí', 2: '‚Üì', 3: '‚Üê', -1: ''}\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        action = int(policy_grid[i, j])\n",
    "        arrow = arrow_map.get(action, '')\n",
    "        ax.text(j, i, arrow, ha=\"center\", va=\"center\", \n",
    "               fontsize=20, fontweight='bold', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization Insights:\")\n",
    "print(\"\\n1. Top Left: Returns increase as policy improves\")\n",
    "print(\"   - Agent learns to reach goal more efficiently\")\n",
    "print(\"   - Converges to near-optimal performance\")\n",
    "\n",
    "print(\"\\n2. Top Right: Episode lengths decrease\")\n",
    "print(\"   - Shorter episodes = more efficient paths to goal\")\n",
    "print(\"   - Agent learns to avoid obstacles and reach goal quickly\")\n",
    "\n",
    "print(\"\\n3. Bottom Left: Q-values show state quality\")\n",
    "print(\"   - Higher values near goal (green)\")\n",
    "print(\"   - Lower values far from goal (red)\")\n",
    "\n",
    "print(\"\\n4. Bottom Right: Learned policy\")\n",
    "print(\"   - Arrows show best action in each state\")\n",
    "print(\"   - Policy guides agent toward goal\")\n",
    "print(\"   - Successfully learned from experience!\")\n",
    "\n",
    "print(\"\\n‚úÖ On-Policy MC Control Success:\")\n",
    "print(\"   The agent learned an effective policy without any model!\")\n",
    "print(\"   Just from trial and error with epsilon-greedy exploration.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Off-Policy Learning with Importance Sampling\n",
    "\n",
    "**Learning About One Policy While Following Another**\n",
    "\n",
    "So far, we've used **on-policy** learning where we learn about the same policy we're following. But what if we want to:\n",
    "- Learn an optimal (deterministic) policy while exploring (stochastic behavior)?\n",
    "- Learn from data generated by a different policy (e.g., human demonstrations)?\n",
    "- Reuse old experience even after the policy has changed?\n",
    "\n",
    "This is where **off-policy** learning comes in!\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "**Target Policy ($\\pi$):**\n",
    "- The policy we want to learn about\n",
    "- Often deterministic and greedy\n",
    "- Example: Always take the best action\n",
    "\n",
    "**Behavior Policy ($b$):**\n",
    "- The policy we actually follow to generate experience\n",
    "- Must be exploratory (try all actions)\n",
    "- Example: Epsilon-greedy or random policy\n",
    "\n",
    "**The Challenge:**\n",
    "\n",
    "Episodes are generated by $b$, but we want to estimate values for $\\pi$. The returns we observe are from $b$'s distribution, not $\\pi$'s!\n",
    "\n",
    "**The Solution: Importance Sampling**\n",
    "\n",
    "Importance sampling is a technique from statistics that allows us to estimate expectations under one distribution using samples from another.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "We want to estimate $\\mathbb{E}_\\pi[G_t]$ (expected return under $\\pi$), but we only have samples from $b$.\n",
    "\n",
    "**Importance Sampling Ratio:**\n",
    "\n",
    "For a trajectory $\\tau = (S_t, A_t, S_{t+1}, A_{t+1}, ..., S_T)$, the importance sampling ratio is:\n",
    "\n",
    "$\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}$\n",
    "\n",
    "This ratio weights the return to correct for the difference between policies.\n",
    "\n",
    "**Intuition:**\n",
    "- If $\\pi$ is more likely to take the actions than $b$: ratio > 1 (weight up)\n",
    "- If $\\pi$ is less likely to take the actions than $b$: ratio < 1 (weight down)\n",
    "- If $\\pi$ would never take an action that $b$ took: ratio = 0 (ignore)\n",
    "\n",
    "**Off-Policy MC Prediction with Importance Sampling:**\n",
    "\n",
    "$V(s) = \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1} G_t}{|\\mathcal{T}(s)|}$\n",
    "\n",
    "where $\\mathcal{T}(s)$ is the set of all time steps where state $s$ was visited.\n",
    "\n",
    "**Coverage Assumption:**\n",
    "\n",
    "For off-policy learning to work, we need:\n",
    "$\\pi(a|s) > 0 \\implies b(a|s) > 0$\n",
    "\n",
    "In words: The behavior policy must try all actions that the target policy might take.\n",
    "\n",
    "**Advantages of Off-Policy Learning:**\n",
    "1. Can learn optimal policy while exploring\n",
    "2. Can learn from observational data\n",
    "3. Can reuse experience from old policies\n",
    "4. More flexible than on-policy methods\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Higher variance (importance sampling ratios can be large)\n",
    "2. Slower convergence\n",
    "3. Requires more data\n",
    "4. Can be unstable if policies are very different\n",
    "\n",
    "Let's implement off-policy MC with importance sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_importance_sampling_ratio(episode, target_policy, behavior_policy):\n",
    "    \"\"\"Compute importance sampling ratio for an episode.\n",
    "    \n",
    "    Args:\n",
    "        episode: List of (state, action, reward) tuples\n",
    "        target_policy: Function that returns probability of action given state\n",
    "        behavior_policy: Function that returns probability of action given state\n",
    "        \n",
    "    Returns:\n",
    "        ratios: List of cumulative importance sampling ratios for each step\n",
    "    \"\"\"\n",
    "    ratios = []\n",
    "    cumulative_ratio = 1.0\n",
    "    \n",
    "    for state, action, reward in episode:\n",
    "        # Get probabilities under both policies\n",
    "        pi_prob = target_policy(action, state)\n",
    "        b_prob = behavior_policy(action, state)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if b_prob == 0:\n",
    "            cumulative_ratio = 0\n",
    "            break\n",
    "        \n",
    "        # Update cumulative ratio\n",
    "        cumulative_ratio *= (pi_prob / b_prob)\n",
    "        ratios.append(cumulative_ratio)\n",
    "    \n",
    "    return ratios\n",
    "\n",
    "\n",
    "def mc_prediction_off_policy(env, target_policy_func, behavior_policy_func,\n",
    "                             target_policy_probs, behavior_policy_probs,\n",
    "                             num_episodes=5000, gamma=0.9):\n",
    "    \"\"\"Off-policy Monte Carlo prediction with ordinary importance sampling.\n",
    "    \n",
    "    Learns value function for target policy using episodes from behavior policy.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        target_policy_func: Function that selects actions for target policy\n",
    "        behavior_policy_func: Function that selects actions for behavior policy\n",
    "        target_policy_probs: Function that returns P(a|s) for target policy\n",
    "        behavior_policy_probs: Function that returns P(a|s) for behavior policy\n",
    "        num_episodes: Number of episodes to generate\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        V: Estimated value function for target policy\n",
    "        stats: Learning statistics\n",
    "    \"\"\"\n",
    "    V = defaultdict(float)\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    \n",
    "    episode_returns = []\n",
    "    importance_ratios = []\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode using behavior policy\n",
    "        episode = generate_episode(env, behavior_policy_func, max_steps=100)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Calculate importance sampling ratios\n",
    "        ratios = compute_importance_sampling_ratio(\n",
    "            episode, target_policy_probs, behavior_policy_probs\n",
    "        )\n",
    "        \n",
    "        # Track statistics\n",
    "        if returns:\n",
    "            episode_returns.append(returns[0])\n",
    "        if ratios:\n",
    "            importance_ratios.append(ratios[-1])  # Final ratio\n",
    "        \n",
    "        # Update value estimates (first-visit)\n",
    "        visited_states = set()\n",
    "        \n",
    "        for t, (state, action, reward) in enumerate(episode):\n",
    "            if state not in visited_states and t < len(ratios):\n",
    "                visited_states.add(state)\n",
    "                \n",
    "                # Weight return by importance sampling ratio\n",
    "                weighted_return = ratios[t] * returns[t]\n",
    "                \n",
    "                returns_sum[state] += weighted_return\n",
    "                returns_count[state] += 1\n",
    "                \n",
    "                # Update value estimate\n",
    "                V[state] = returns_sum[state] / returns_count[state]\n",
    "    \n",
    "    stats = {\n",
    "        'episode_returns': episode_returns,\n",
    "        'importance_ratios': importance_ratios,\n",
    "        'states_visited': len(V)\n",
    "    }\n",
    "    \n",
    "    return dict(V), stats\n",
    "\n",
    "\n",
    "print(\"Off-Policy Monte Carlo with Importance Sampling\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nImplemented:\")\n",
    "print(\"  ‚úì Importance sampling ratio computation\")\n",
    "print(\"  ‚úì Off-policy MC prediction\")\n",
    "print(\"  ‚úì Target vs behavior policy separation\")\n",
    "print(\"\\nReady to learn from off-policy data!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrating Off-Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define target and behavior policies\n",
    "print(\"Off-Policy Learning Demonstration\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnvironment(grid_size=5)\n",
    "\n",
    "# Target policy: Greedy (deterministic)\n",
    "def target_policy_action(state):\n",
    "    \"\"\"Greedy policy - always move toward goal.\"\"\"\n",
    "    row, col = state\n",
    "    goal_row, goal_col = 4, 4\n",
    "    \n",
    "    if col < goal_col:\n",
    "        return 1  # RIGHT\n",
    "    elif row < goal_row:\n",
    "        return 2  # DOWN\n",
    "    else:\n",
    "        return 1  # Default\n",
    "\n",
    "def target_policy_prob(action, state):\n",
    "    \"\"\"Probability of action under target policy (deterministic).\"\"\"\n",
    "    return 1.0 if action == target_policy_action(state) else 0.0\n",
    "\n",
    "# Behavior policy: Epsilon-greedy (exploratory)\n",
    "epsilon_behavior = 0.3\n",
    "\n",
    "def behavior_policy_action(state):\n",
    "    \"\"\"Epsilon-greedy behavior policy.\"\"\"\n",
    "    if np.random.random() < epsilon_behavior:\n",
    "        return np.random.randint(0, 4)  # Random\n",
    "    else:\n",
    "        return target_policy_action(state)  # Greedy\n",
    "\n",
    "def behavior_policy_prob(action, state):\n",
    "    \"\"\"Probability of action under behavior policy (epsilon-greedy).\"\"\"\n",
    "    greedy_action = target_policy_action(state)\n",
    "    \n",
    "    if action == greedy_action:\n",
    "        return 1.0 - epsilon_behavior + epsilon_behavior / 4.0\n",
    "    else:\n",
    "        return epsilon_behavior / 4.0\n",
    "\n",
    "print(\"\\nPolicies:\")\n",
    "print(\"  Target Policy: Greedy (deterministic, optimal)\")\n",
    "print(\"  Behavior Policy: Œµ-greedy with Œµ=0.3 (exploratory)\")\n",
    "\n",
    "print(\"\\nLearning value function for target policy...\")\n",
    "print(\"  (using episodes generated by behavior policy)\")\n",
    "\n",
    "# Learn off-policy\n",
    "V_off_policy, stats_off = mc_prediction_off_policy(\n",
    "    env,\n",
    "    target_policy_action,\n",
    "    behavior_policy_action,\n",
    "    target_policy_prob,\n",
    "    behavior_policy_prob,\n",
    "    num_episodes=5000,\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "# For comparison, learn on-policy with target policy\n",
    "print(\"\\nFor comparison, learning with on-policy (target policy)...\")\n",
    "V_on_policy, returns_on = mc_prediction_first_visit(\n",
    "    env, target_policy_action, num_episodes=5000, gamma=0.9\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nResults:\")\n",
    "print(f\"\\nOff-Policy Learning:\")\n",
    "print(f\"  States visited: {stats_off['states_visited']}\")\n",
    "print(f\"  Start state value: {V_off_policy.get((0,0), 0):.2f}\")\n",
    "print(f\"  Average importance ratio: {np.mean(stats_off['importance_ratios']):.3f}\")\n",
    "print(f\"  Max importance ratio: {np.max(stats_off['importance_ratios']):.3f}\")\n",
    "\n",
    "print(f\"\\nOn-Policy Learning (for comparison):\")\n",
    "print(f\"  States visited: {len(V_on_policy)}\")\n",
    "print(f\"  Start state value: {V_on_policy.get((0,0), 0):.2f}\")\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   - Off-policy successfully learns target policy values\")\n",
    "print(\"   - Uses exploratory behavior policy for data collection\")\n",
    "print(\"   - Importance ratios correct for policy difference\")\n",
    "print(\"   - Values should be similar to on-policy estimates\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Importance Sampling: Reducing Variance\n",
    "\n",
    "**The Variance Problem with Ordinary Importance Sampling**\n",
    "\n",
    "Ordinary importance sampling (what we just implemented) has a significant problem: **high variance**.\n",
    "\n",
    "**Why High Variance?**\n",
    "\n",
    "The importance sampling ratio $\\rho = \\prod \\frac{\\pi(a|s)}{b(a|s)}$ can become very large:\n",
    "- If the trajectory is long, many ratios multiply together\n",
    "- If policies differ significantly, individual ratios can be large\n",
    "- A single large ratio can dominate the average\n",
    "\n",
    "**Example:**\n",
    "- Suppose we have 100 episodes with ratio ‚âà 1.0\n",
    "- And 1 episode with ratio = 100\n",
    "- The single outlier heavily skews the estimate!\n",
    "\n",
    "**The Solution: Weighted Importance Sampling**\n",
    "\n",
    "Instead of a simple average, use a **weighted average** where the weights are the importance sampling ratios themselves.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "**Ordinary Importance Sampling:**\n",
    "$V(s) = \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1} G_t}{|\\mathcal{T}(s)|}$\n",
    "\n",
    "**Weighted Importance Sampling:**\n",
    "$V(s) = \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1} G_t}{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1}}$\n",
    "\n",
    "**Key Difference:**\n",
    "- Ordinary: Divide by number of visits\n",
    "- Weighted: Divide by sum of importance ratios\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "Weighted IS gives more weight to returns with larger importance ratios, but normalizes by the total weight. This:\n",
    "- Reduces the impact of extreme ratios\n",
    "- Provides more stable estimates\n",
    "- Converges faster in practice\n",
    "\n",
    "**Bias-Variance Trade-off:**\n",
    "\n",
    "| Method | Bias | Variance | Convergence |\n",
    "|--------|------|----------|-------------|\n",
    "| Ordinary IS | Unbiased | High | Slower |\n",
    "| Weighted IS | Biased (initially) | Low | Faster |\n",
    "\n",
    "**Important Note:**\n",
    "- Weighted IS is biased for finite samples\n",
    "- But the bias goes to zero as samples increase\n",
    "- In practice, lower variance usually wins!\n",
    "\n",
    "**When to Use Each:**\n",
    "- **Ordinary IS**: When you need unbiased estimates, have lots of data\n",
    "- **Weighted IS**: When variance is a problem, limited data (most practical cases)\n",
    "\n",
    "Let's implement weighted importance sampling and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def mc_prediction_off_policy_weighted(env, target_policy_func, behavior_policy_func,\n",
    "                                      target_policy_probs, behavior_policy_probs,\n",
    "                                      num_episodes=5000, gamma=0.9):\n",
    "    \"\"\"Off-policy Monte Carlo prediction with weighted importance sampling.\n",
    "    \n",
    "    Uses weighted average to reduce variance compared to ordinary IS.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        target_policy_func: Function that selects actions for target policy\n",
    "        behavior_policy_func: Function that selects actions for behavior policy\n",
    "        target_policy_probs: Function that returns P(a|s) for target policy\n",
    "        behavior_policy_probs: Function that returns P(a|s) for behavior policy\n",
    "        num_episodes: Number of episodes to generate\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        V: Estimated value function for target policy\n",
    "        stats: Learning statistics\n",
    "    \"\"\"\n",
    "    V = defaultdict(float)\n",
    "    # For weighted IS, we need numerator and denominator separately\n",
    "    weighted_returns_sum = defaultdict(float)  # Sum of (ratio * return)\n",
    "    weights_sum = defaultdict(float)  # Sum of ratios\n",
    "    \n",
    "    episode_returns = []\n",
    "    importance_ratios = []\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode using behavior policy\n",
    "        episode = generate_episode(env, behavior_policy_func, max_steps=100)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Calculate importance sampling ratios\n",
    "        ratios = compute_importance_sampling_ratio(\n",
    "            episode, target_policy_probs, behavior_policy_probs\n",
    "        )\n",
    "        \n",
    "        # Track statistics\n",
    "        if returns:\n",
    "            episode_returns.append(returns[0])\n",
    "        if ratios:\n",
    "            importance_ratios.append(ratios[-1])\n",
    "        \n",
    "        # Update value estimates (first-visit)\n",
    "        visited_states = set()\n",
    "        \n",
    "        for t, (state, action, reward) in enumerate(episode):\n",
    "            if state not in visited_states and t < len(ratios):\n",
    "                visited_states.add(state)\n",
    "                \n",
    "                # Weighted importance sampling\n",
    "                ratio = ratios[t]\n",
    "                weighted_return = ratio * returns[t]\n",
    "                \n",
    "                # Update numerator and denominator\n",
    "                weighted_returns_sum[state] += weighted_return\n",
    "                weights_sum[state] += ratio\n",
    "                \n",
    "                # Update value estimate (weighted average)\n",
    "                if weights_sum[state] > 0:\n",
    "                    V[state] = weighted_returns_sum[state] / weights_sum[state]\n",
    "    \n",
    "    stats = {\n",
    "        'episode_returns': episode_returns,\n",
    "        'importance_ratios': importance_ratios,\n",
    "        'states_visited': len(V)\n",
    "    }\n",
    "    \n",
    "    return dict(V), stats\n",
    "\n",
    "\n",
    "print(\"Weighted Importance Sampling Implementation\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nImplemented:\")\n",
    "print(\"  ‚úì Weighted importance sampling\")\n",
    "print(\"  ‚úì Variance reduction through weighted averaging\")\n",
    "print(\"  ‚úì Separate tracking of numerator and denominator\")\n",
    "print(\"\\nReady to compare with ordinary importance sampling!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Ordinary vs Weighted Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare ordinary and weighted importance sampling\n",
    "print(\"Comparing Ordinary vs Weighted Importance Sampling\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run multiple trials to measure variance\n",
    "num_trials = 20\n",
    "num_episodes_per_trial = 2000\n",
    "\n",
    "ordinary_estimates = []\n",
    "weighted_estimates = []\n",
    "\n",
    "print(f\"\\nRunning {num_trials} trials with {num_episodes_per_trial} episodes each...\")\n",
    "print(\"(This may take a moment)\\n\")\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    # Ordinary importance sampling\n",
    "    V_ordinary, _ = mc_prediction_off_policy(\n",
    "        env, target_policy_action, behavior_policy_action,\n",
    "        target_policy_prob, behavior_policy_prob,\n",
    "        num_episodes=num_episodes_per_trial, gamma=0.9\n",
    "    )\n",
    "    ordinary_estimates.append(V_ordinary.get((0,0), 0))\n",
    "    \n",
    "    # Weighted importance sampling\n",
    "    V_weighted, _ = mc_prediction_off_policy_weighted(\n",
    "        env, target_policy_action, behavior_policy_action,\n",
    "        target_policy_prob, behavior_policy_prob,\n",
    "        num_episodes=num_episodes_per_trial, gamma=0.9\n",
    "    )\n",
    "    weighted_estimates.append(V_weighted.get((0,0), 0))\n",
    "    \n",
    "    if (trial + 1) % 5 == 0:\n",
    "        print(f\"  Completed {trial + 1}/{num_trials} trials\")\n",
    "\n",
    "# Calculate statistics\n",
    "ordinary_mean = np.mean(ordinary_estimates)\n",
    "ordinary_std = np.std(ordinary_estimates)\n",
    "weighted_mean = np.mean(weighted_estimates)\n",
    "weighted_std = np.std(weighted_estimates)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nResults for Start State (0,0):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\nOrdinary Importance Sampling:\")\n",
    "print(f\"  Mean estimate: {ordinary_mean:.3f}\")\n",
    "print(f\"  Std deviation: {ordinary_std:.3f}\")\n",
    "print(f\"  Min estimate: {np.min(ordinary_estimates):.3f}\")\n",
    "print(f\"  Max estimate: {np.max(ordinary_estimates):.3f}\")\n",
    "\n",
    "print(f\"\\nWeighted Importance Sampling:\")\n",
    "print(f\"  Mean estimate: {weighted_mean:.3f}\")\n",
    "print(f\"  Std deviation: {weighted_std:.3f}\")\n",
    "print(f\"  Min estimate: {np.min(weighted_estimates):.3f}\")\n",
    "print(f\"  Max estimate: {np.max(weighted_estimates):.3f}\")\n",
    "\n",
    "variance_reduction = ((ordinary_std - weighted_std) / ordinary_std) * 100\n",
    "print(f\"\\nüìä Variance Reduction: {variance_reduction:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Variance Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Distribution of estimates\n",
    "ax = axes[0]\n",
    "ax.hist(ordinary_estimates, bins=15, alpha=0.6, label='Ordinary IS', color='red', edgecolor='black')\n",
    "ax.hist(weighted_estimates, bins=15, alpha=0.6, label='Weighted IS', color='green', edgecolor='black')\n",
    "ax.axvline(ordinary_mean, color='red', linestyle='--', linewidth=2, label=f'Ordinary Mean: {ordinary_mean:.2f}')\n",
    "ax.axvline(weighted_mean, color='green', linestyle='--', linewidth=2, label=f'Weighted Mean: {weighted_mean:.2f}')\n",
    "ax.set_xlabel('Value Estimate for State (0,0)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Distribution of Value Estimates', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Box plot comparison\n",
    "ax = axes[1]\n",
    "data_to_plot = [ordinary_estimates, weighted_estimates]\n",
    "bp = ax.boxplot(data_to_plot, labels=['Ordinary IS', 'Weighted IS'],\n",
    "                patch_artist=True, widths=0.6)\n",
    "\n",
    "# Color the boxes\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('Value Estimate for State (0,0)', fontsize=12)\n",
    "ax.set_title('Variance Comparison', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add variance reduction annotation\n",
    "ax.text(1.5, ax.get_ylim()[1] * 0.95, \n",
    "        f'Variance Reduction:\\n{variance_reduction:.1f}%',\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "        fontsize=11, fontweight='bold', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization Insights:\")\n",
    "print(\"\\n1. Left Plot: Distribution of estimates across trials\")\n",
    "print(\"   - Ordinary IS: Wider spread (higher variance)\")\n",
    "print(\"   - Weighted IS: Tighter distribution (lower variance)\")\n",
    "print(\"   - Both centered around similar mean values\")\n",
    "\n",
    "print(\"\\n2. Right Plot: Box plot shows variance clearly\")\n",
    "print(\"   - Ordinary IS: Larger box and whiskers\")\n",
    "print(\"   - Weighted IS: Smaller box (more consistent estimates)\")\n",
    "print(\"   - Outliers are less extreme with weighted IS\")\n",
    "\n",
    "print(\"\\n‚úÖ Key Takeaways:\")\n",
    "print(\"   1. Weighted IS significantly reduces variance\")\n",
    "print(\"   2. More stable and reliable estimates\")\n",
    "print(\"   3. Faster convergence in practice\")\n",
    "print(\"   4. Preferred method for most off-policy learning\")\n",
    "print(\"\\n   Weighted importance sampling is the practical choice!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Methods: Limitations and Challenges\n",
    "\n",
    "**Understanding When MC Methods Fall Short**\n",
    "\n",
    "While Monte Carlo methods are powerful and model-free, they have significant limitations that restrict their applicability. Understanding these limitations motivates the development of more advanced methods like Temporal Difference learning.\n",
    "\n",
    "**1. The \"Wait Until the End\" Problem**\n",
    "\n",
    "**The Issue:**\n",
    "- MC methods must wait until an episode completes before updating values\n",
    "- No learning happens during the episode\n",
    "- All updates occur at the end\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Long Episodes**: If episodes take 1000 steps, you wait 1000 steps to learn anything\n",
    "- **Continuing Tasks**: Some tasks never end (e.g., process control, robot operation)\n",
    "- **Slow Feedback**: Can't adjust behavior mid-episode based on what's working\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Episode: S‚ÇÄ ‚Üí S‚ÇÅ ‚Üí S‚ÇÇ ‚Üí ... ‚Üí S‚Çâ‚Çâ‚Çâ ‚Üí S‚ÇÅ‚ÇÄ‚ÇÄ‚ÇÄ (terminal)\n",
    "         ‚Üë                                    ‚Üë\n",
    "    No learning                         All learning happens here!\n",
    "```\n",
    "\n",
    "**Impact:**\n",
    "- Inefficient use of experience\n",
    "- Slow learning, especially with long episodes\n",
    "- Cannot handle continuing (non-episodic) tasks\n",
    "\n",
    "**2. High Variance in Return Estimates**\n",
    "\n",
    "**The Issue:**\n",
    "- Returns depend on entire trajectory of rewards\n",
    "- Many random events accumulate\n",
    "- Different episodes from same state can have very different returns\n",
    "\n",
    "**Mathematical Perspective:**\n",
    "\n",
    "Return: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$\n",
    "\n",
    "Each $R_i$ is random, and we're summing many random variables:\n",
    "- More terms ‚Üí more variance\n",
    "- Longer episodes ‚Üí higher variance\n",
    "- Stochastic environments ‚Üí even more variance\n",
    "\n",
    "**Consequences:**\n",
    "- Need many episodes to get accurate estimates\n",
    "- Slow convergence\n",
    "- Unstable learning, especially early on\n",
    "- Off-policy methods (importance sampling) make this worse\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "From state S, three episodes:\n",
    "  Episode 1: G = 10  (got lucky)\n",
    "  Episode 2: G = -5  (got unlucky)\n",
    "  Episode 3: G = 3   (typical)\n",
    "  \n",
    "Average: 2.67 (but high variance!)\n",
    "Need many more episodes for stable estimate\n",
    "```\n",
    "\n",
    "**3. Inefficient Learning from Experience**\n",
    "\n",
    "**The Issue:**\n",
    "- Each episode provides one data point per state visited\n",
    "- Can't learn from partial episodes\n",
    "- Doesn't leverage structure of the problem\n",
    "\n",
    "**Comparison:**\n",
    "- **MC**: Uses complete return from state to end\n",
    "- **Better approach**: Could learn from each step along the way\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Episode: S‚ÇÄ ‚Üí S‚ÇÅ ‚Üí S‚ÇÇ ‚Üí S‚ÇÉ ‚Üí S‚ÇÑ (terminal, R=10)\n",
    "\n",
    "MC Learning:\n",
    "  - Updates V(S‚ÇÄ), V(S‚ÇÅ), V(S‚ÇÇ), V(S‚ÇÉ) once at end\n",
    "  - Uses full return for each\n",
    "  \n",
    "Potential Improvement:\n",
    "  - Could update after each step\n",
    "  - Could learn from partial information\n",
    "  - 5 learning opportunities instead of 1!\n",
    "```\n",
    "\n",
    "**4. Requires Episodic Tasks**\n",
    "\n",
    "**The Issue:**\n",
    "- MC methods fundamentally require episodes to terminate\n",
    "- Many real-world problems are continuing (no natural end)\n",
    "\n",
    "**Examples of Continuing Tasks:**\n",
    "- Process control (factory, power plant)\n",
    "- Robot operation (runs indefinitely)\n",
    "- Stock trading (market never closes permanently)\n",
    "- Recommendation systems (always serving users)\n",
    "\n",
    "**Workarounds (not ideal):**\n",
    "- Artificially terminate episodes\n",
    "- Use very long episodes (but then variance increases)\n",
    "- Neither solution is satisfactory\n",
    "\n",
    "**5. Slow Convergence**\n",
    "\n",
    "**The Issue:**\n",
    "- Due to high variance, need many episodes\n",
    "- Each episode only updates visited states\n",
    "- Learning is sample-inefficient\n",
    "\n",
    "**Factors Affecting Convergence:**\n",
    "- Episode length (longer ‚Üí slower)\n",
    "- Environment stochasticity (more random ‚Üí slower)\n",
    "- State space size (larger ‚Üí slower)\n",
    "- Exploration strategy (poor exploration ‚Üí slower)\n",
    "\n",
    "**Practical Impact:**\n",
    "- May need millions of episodes for complex problems\n",
    "- Expensive in terms of computation and time\n",
    "- Not practical for real-world systems with costly interactions\n",
    "\n",
    "**Summary of Limitations:**\n",
    "\n",
    "| Limitation | Impact | Severity |\n",
    "|------------|--------|----------|\n",
    "| Wait until end | Slow learning | High |\n",
    "| High variance | Need many samples | High |\n",
    "| Episodic only | Can't handle continuing tasks | Critical |\n",
    "| Sample inefficiency | Expensive learning | Medium |\n",
    "| Slow convergence | Long training times | Medium |\n",
    "\n",
    "**The Path Forward: Temporal Difference Learning**\n",
    "\n",
    "These limitations motivate **Temporal Difference (TD) learning**, which we'll explore next. TD methods:\n",
    "\n",
    "‚úì Learn from every step (not just at episode end)\n",
    "‚úì Work with continuing tasks\n",
    "‚úì Lower variance (bootstrap from estimates)\n",
    "‚úì More sample-efficient\n",
    "‚úì Faster convergence\n",
    "\n",
    "**When to Use Monte Carlo Despite Limitations:**\n",
    "\n",
    "MC methods are still valuable when:\n",
    "- Episodes are short\n",
    "- You need unbiased estimates\n",
    "- Environment is deterministic or low-noise\n",
    "- You have access to a simulator (cheap episodes)\n",
    "- You want simple, easy-to-understand algorithms\n",
    "\n",
    "**Key Insight:**\n",
    "\n",
    "Monte Carlo methods taught us that we can learn from experience without a model. But their limitations show us that we can do better by learning from partial episodes and bootstrapping from our own estimates. This insight leads directly to Temporal Difference learning, which combines the best of MC and Dynamic Programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing MC Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Demonstrate the high variance problem\n",
    "print(\"Demonstrating Monte Carlo Limitations\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run MC prediction multiple times to show variance\n",
    "num_runs = 50\n",
    "episodes_per_run = 1000\n",
    "\n",
    "start_state_estimates = []\n",
    "\n",
    "print(f\"\\nRunning MC prediction {num_runs} times...\")\n",
    "print(f\"Each run uses {episodes_per_run} episodes\\n\")\n",
    "\n",
    "for run in range(num_runs):\n",
    "    V, _ = mc_prediction_first_visit(\n",
    "        env, greedy_policy, num_episodes=episodes_per_run, gamma=0.9\n",
    "    )\n",
    "    start_state_estimates.append(V.get((0,0), 0))\n",
    "\n",
    "mean_estimate = np.mean(start_state_estimates)\n",
    "std_estimate = np.std(start_state_estimates)\n",
    "\n",
    "print(f\"Results for start state (0,0):\")\n",
    "print(f\"  Mean estimate: {mean_estimate:.3f}\")\n",
    "print(f\"  Std deviation: {std_estimate:.3f}\")\n",
    "print(f\"  Min: {np.min(start_state_estimates):.3f}\")\n",
    "print(f\"  Max: {np.max(start_state_estimates):.3f}\")\n",
    "print(f\"  Range: {np.max(start_state_estimates) - np.min(start_state_estimates):.3f}\")\n",
    "\n",
    "# Visualize variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Distribution of estimates\n",
    "ax = axes[0]\n",
    "ax.hist(start_state_estimates, bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax.axvline(mean_estimate, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {mean_estimate:.2f}')\n",
    "ax.axvline(mean_estimate - std_estimate, color='orange', linestyle=':', linewidth=2,\n",
    "           label=f'¬±1 Std: {std_estimate:.2f}')\n",
    "ax.axvline(mean_estimate + std_estimate, color='orange', linestyle=':', linewidth=2)\n",
    "ax.set_xlabel('Value Estimate', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('High Variance in MC Estimates', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Estimates over runs\n",
    "ax = axes[1]\n",
    "ax.plot(start_state_estimates, marker='o', linestyle='-', alpha=0.6, color='steelblue')\n",
    "ax.axhline(mean_estimate, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "ax.fill_between(range(num_runs), \n",
    "                mean_estimate - std_estimate, \n",
    "                mean_estimate + std_estimate,\n",
    "                alpha=0.2, color='orange', label='¬±1 Std')\n",
    "ax.set_xlabel('Run Number', fontsize=12)\n",
    "ax.set_ylabel('Value Estimate', fontsize=12)\n",
    "ax.set_title('Variability Across Runs', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è  Key Observations:\")\n",
    "print(\"\\n1. High Variance:\")\n",
    "print(f\"   - Estimates vary significantly across runs\")\n",
    "print(f\"   - Standard deviation is {(std_estimate/mean_estimate)*100:.1f}% of mean\")\n",
    "print(f\"   - Need many episodes for stable estimates\")\n",
    "\n",
    "print(\"\\n2. Sample Inefficiency:\")\n",
    "print(f\"   - Used {num_runs * episodes_per_run:,} total episodes\")\n",
    "print(f\"   - Still seeing significant variance\")\n",
    "print(f\"   - Each episode only updates visited states once\")\n",
    "\n",
    "print(\"\\n3. Episodic Requirement:\")\n",
    "print(f\"   - Must wait for episode to complete\")\n",
    "print(f\"   - No learning during episode\")\n",
    "print(f\"   - Cannot handle continuing tasks\")\n",
    "\n",
    "print(\"\\nüéØ Motivation for Temporal Difference Learning:\")\n",
    "print(\"   These limitations show we need methods that:\")\n",
    "print(\"   ‚Ä¢ Learn from every step, not just episode ends\")\n",
    "print(\"   ‚Ä¢ Have lower variance through bootstrapping\")\n",
    "print(\"   ‚Ä¢ Work with continuing tasks\")\n",
    "print(\"   ‚Ä¢ Are more sample-efficient\")\n",
    "print(\"\\n   ‚Üí This leads us to TD learning in the next section!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='td-learning'></a>\n",
    "### Temporal Difference Learning\n",
    "\n",
    "**Learning from Every Step**\n",
    "\n",
    "Temporal Difference (TD) learning represents a fundamental breakthrough in reinforcement learning. Unlike Monte Carlo methods that must wait until the end of an episode to update value estimates, TD methods learn from **every single step** of experience.\n",
    "\n",
    "**The Key Insight:**\n",
    "\n",
    "TD learning combines the best aspects of two approaches:\n",
    "\n",
    "1. **From Monte Carlo**: Learn directly from experience without a model\n",
    "2. **From Dynamic Programming**: Update estimates based on other estimates (bootstrapping)\n",
    "\n",
    "**Why \"Temporal Difference\"?**\n",
    "\n",
    "The name comes from the fact that TD methods learn from the **difference** between estimates at successive **time** steps. Instead of waiting for the actual return, TD methods use the difference between the current estimate and a better estimate based on the next state.\n",
    "\n",
    "**Advantages of TD Learning:**\n",
    "\n",
    "1. **Online Learning**: Update after every step, not just at episode end\n",
    "2. **Lower Variance**: Bootstrap from estimates rather than full returns\n",
    "3. **Works with Continuing Tasks**: No need for episodes to terminate\n",
    "4. **More Sample Efficient**: Learn more from each experience\n",
    "5. **Faster Convergence**: Updates propagate information more quickly\n",
    "\n",
    "**The Trade-off:**\n",
    "\n",
    "- **MC**: Unbiased but high variance (uses actual returns)\n",
    "- **TD**: Biased but lower variance (uses estimated returns)\n",
    "- In practice, TD's lower variance usually wins!\n",
    "\n",
    "Let's explore the simplest TD method: TD(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD(0) Prediction Algorithm\n",
    "\n",
    "**The Simplest Temporal Difference Method**\n",
    "\n",
    "TD(0) (pronounced \"TD-zero\") is the most fundamental TD algorithm. It updates the value estimate for a state immediately after transitioning to the next state.\n",
    "\n",
    "**The TD(0) Update Rule:**\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]\n",
    "$$\n",
    "\n",
    "**Breaking Down the Formula:**\n",
    "\n",
    "- $V(S_t)$: Current value estimate for state $S_t$\n",
    "- $\\alpha$: Learning rate (step size), typically 0.01 to 0.5\n",
    "- $R_{t+1}$: Immediate reward received after taking action\n",
    "- $\\gamma$: Discount factor (0 to 1)\n",
    "- $V(S_{t+1})$: Value estimate for next state\n",
    "- $R_{t+1} + \\gamma V(S_{t+1})$: **TD target** (estimate of true value)\n",
    "- $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$: **TD error** (how wrong we were)\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "1. We're in state $S_t$ with value estimate $V(S_t)$\n",
    "2. We take an action and receive reward $R_{t+1}$, landing in state $S_{t+1}$\n",
    "3. We form a **better estimate** of $V(S_t)$: $R_{t+1} + \\gamma V(S_{t+1})$\n",
    "4. We move our estimate toward this better estimate\n",
    "\n",
    "**Comparison with Monte Carlo:**\n",
    "\n",
    "Monte Carlo update:\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right]\n",
    "$$\n",
    "where $G_t$ is the **actual return** (sum of all future rewards)\n",
    "\n",
    "TD(0) update:\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]\n",
    "$$\n",
    "where $R_{t+1} + \\gamma V(S_{t+1})$ is an **estimated return**\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Aspect | Monte Carlo | TD(0) |\n",
    "|--------|-------------|-------|\n",
    "| Target | $G_t$ (actual return) | $R_{t+1} + \\gamma V(S_{t+1})$ (estimated) |\n",
    "| When to update | End of episode | After each step |\n",
    "| Bias | Unbiased | Biased (uses estimate) |\n",
    "| Variance | High | Lower |\n",
    "| Continuing tasks | No | Yes |\n",
    "\n",
    "**The Bootstrapping Concept:**\n",
    "\n",
    "TD methods \"bootstrap\" - they update estimates based on other estimates. This is like pulling yourself up by your bootstraps! Initially, all estimates might be wrong, but they gradually improve and help each other converge to the true values.\n",
    "\n",
    "Let's implement TD(0) prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def td_prediction(env, policy, num_episodes=1000, alpha=0.1, gamma=0.9):\n",
    "    \"\"\"\n",
    "    TD(0) prediction: Estimate state-value function for a given policy.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment with reset() and step() methods\n",
    "        policy: Function that takes state and returns action\n",
    "        num_episodes: Number of episodes to run\n",
    "        alpha: Learning rate (step size)\n",
    "        gamma: Discount factor\n",
    "    \n",
    "    Returns:\n",
    "        V: Dictionary mapping states to value estimates\n",
    "        episode_lengths: List of episode lengths for tracking\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = defaultdict(float)\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_length = 0\n",
    "        \n",
    "        while True:\n",
    "            # Select action according to policy\n",
    "            action = policy(state)\n",
    "            \n",
    "            # Take action and observe next state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_length += 1\n",
    "            \n",
    "            # TD(0) update rule\n",
    "            # V(S) ‚Üê V(S) + Œ±[R + Œ≥V(S') - V(S)]\n",
    "            td_target = reward + gamma * V[next_state]\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] = V[state] + alpha * td_error\n",
    "            \n",
    "            if done:\n",
    "                episode_lengths.append(episode_length)\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "    \n",
    "    return V, episode_lengths\n",
    "\n",
    "\n",
    "print(\"TD(0) Prediction Algorithm Implemented!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"  ‚Ä¢ Updates after every step (online learning)\")\n",
    "print(\"  ‚Ä¢ Uses bootstrapping (estimates from estimates)\")\n",
    "print(\"  ‚Ä¢ Lower variance than Monte Carlo\")\n",
    "print(\"  ‚Ä¢ Works with continuing tasks\")\n",
    "print(\"\\nUpdate Rule: V(S) ‚Üê V(S) + Œ±[R + Œ≥V(S') - V(S)]\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing TD(0) with Monte Carlo\n",
    "\n",
    "Now let's compare TD(0) prediction with Monte Carlo prediction on the same environment to see the differences in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Use the same grid world environment from before\n",
    "print(\"Comparing TD(0) vs Monte Carlo Prediction\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment\n",
    "env = SimpleGridWorld()\n",
    "\n",
    "# Use the same greedy policy toward goal\n",
    "def greedy_policy(state):\n",
    "    \"\"\"Simple policy: move toward goal (3,3).\"\"\"\n",
    "    row, col = state\n",
    "    goal_row, goal_col = 3, 3\n",
    "    \n",
    "    # Move toward goal\n",
    "    if row < goal_row:\n",
    "        return 2  # Down\n",
    "    elif row > goal_row:\n",
    "        return 0  # Up\n",
    "    elif col < goal_col:\n",
    "        return 1  # Right\n",
    "    else:\n",
    "        return 3  # Left\n",
    "\n",
    "# Run both algorithms with same parameters\n",
    "num_episodes = 500\n",
    "gamma = 0.9\n",
    "\n",
    "print(f\"\\nRunning both algorithms for {num_episodes} episodes...\\n\")\n",
    "\n",
    "# TD(0) prediction\n",
    "print(\"Running TD(0) prediction...\")\n",
    "V_td, lengths_td = td_prediction(env, greedy_policy, \n",
    "                                  num_episodes=num_episodes, \n",
    "                                  alpha=0.1, gamma=gamma)\n",
    "\n",
    "# Monte Carlo prediction (first-visit)\n",
    "print(\"Running Monte Carlo prediction...\")\n",
    "V_mc, lengths_mc = mc_prediction_first_visit(env, greedy_policy, \n",
    "                                              num_episodes=num_episodes, \n",
    "                                              gamma=gamma)\n",
    "\n",
    "print(\"\\nDone!\\n\")\n",
    "\n",
    "# Compare value estimates for key states\n",
    "print(\"Value Estimates Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'State':<12} {'TD(0)':<12} {'MC':<12} {'Difference':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Compare some key states\n",
    "key_states = [(0,0), (0,3), (1,1), (2,2), (3,0), (3,3)]\n",
    "for state in key_states:\n",
    "    v_td = V_td.get(state, 0)\n",
    "    v_mc = V_mc.get(state, 0)\n",
    "    diff = abs(v_td - v_mc)\n",
    "    print(f\"{str(state):<12} {v_td:<12.3f} {v_mc:<12.3f} {diff:<12.3f}\")\n",
    "\n",
    "# Calculate statistics\n",
    "all_states = set(list(V_td.keys()) + list(V_mc.keys()))\n",
    "differences = [abs(V_td.get(s, 0) - V_mc.get(s, 0)) for s in all_states]\n",
    "mean_diff = np.mean(differences)\n",
    "max_diff = np.max(differences)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Mean absolute difference: {mean_diff:.4f}\")\n",
    "print(f\"  Max absolute difference:  {max_diff:.4f}\")\n",
    "print(f\"  Number of states visited: {len(all_states)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Faster Convergence of TD(0)\n",
    "\n",
    "One of the key advantages of TD learning is faster convergence. Let's visualize this by tracking how the value estimates evolve over episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def td_prediction_with_tracking(env, policy, num_episodes=500, alpha=0.1, gamma=0.9, track_state=(0,0)):\n",
    "    \"\"\"\n",
    "    TD(0) prediction with tracking of value estimates over time.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        policy: Policy function\n",
    "        num_episodes: Number of episodes\n",
    "        alpha: Learning rate\n",
    "        gamma: Discount factor\n",
    "        track_state: State to track value estimates for\n",
    "    \n",
    "    Returns:\n",
    "        V: Final value function\n",
    "        value_history: List of value estimates for tracked state\n",
    "    \"\"\"\n",
    "    V = defaultdict(float)\n",
    "    value_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # TD(0) update\n",
    "            td_target = reward + gamma * V[next_state]\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] = V[state] + alpha * td_error\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Track value estimate after each episode\n",
    "        value_history.append(V[track_state])\n",
    "    \n",
    "    return V, value_history\n",
    "\n",
    "\n",
    "def mc_prediction_with_tracking(env, policy, num_episodes=500, gamma=0.9, track_state=(0,0)):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction with tracking of value estimates over time.\n",
    "    \"\"\"\n",
    "    V = defaultdict(float)\n",
    "    returns = defaultdict(list)\n",
    "    value_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Generate episode\n",
    "        episode_data = []\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_data.append((state, reward))\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # Calculate returns and update values (first-visit)\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        \n",
    "        for state, reward in reversed(episode_data):\n",
    "            G = reward + gamma * G\n",
    "            \n",
    "            if state not in visited:\n",
    "                visited.add(state)\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "        \n",
    "        # Track value estimate after each episode\n",
    "        value_history.append(V[track_state])\n",
    "    \n",
    "    return V, value_history\n",
    "\n",
    "\n",
    "# Run both algorithms with tracking\n",
    "print(\"Tracking Convergence: TD(0) vs Monte Carlo\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = SimpleGridWorld()\n",
    "track_state = (0, 0)  # Track the start state\n",
    "num_episodes = 500\n",
    "\n",
    "print(f\"\\nTracking value estimates for state {track_state}...\\n\")\n",
    "\n",
    "# Run multiple times to get average behavior\n",
    "num_runs = 20\n",
    "td_histories = []\n",
    "mc_histories = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    # TD(0)\n",
    "    _, td_hist = td_prediction_with_tracking(env, greedy_policy, \n",
    "                                             num_episodes=num_episodes,\n",
    "                                             alpha=0.1, gamma=0.9,\n",
    "                                             track_state=track_state)\n",
    "    td_histories.append(td_hist)\n",
    "    \n",
    "    # Monte Carlo\n",
    "    _, mc_hist = mc_prediction_with_tracking(env, greedy_policy,\n",
    "                                             num_episodes=num_episodes,\n",
    "                                             gamma=0.9,\n",
    "                                             track_state=track_state)\n",
    "    mc_histories.append(mc_hist)\n",
    "\n",
    "# Average across runs\n",
    "td_avg = np.mean(td_histories, axis=0)\n",
    "mc_avg = np.mean(mc_histories, axis=0)\n",
    "\n",
    "# Calculate standard deviation for confidence bands\n",
    "td_std = np.std(td_histories, axis=0)\n",
    "mc_std = np.std(mc_histories, axis=0)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Convergence comparison\n",
    "ax = axes[0]\n",
    "episodes = np.arange(num_episodes)\n",
    "\n",
    "# TD(0) line\n",
    "ax.plot(episodes, td_avg, linewidth=2, color='blue', label='TD(0)', alpha=0.8)\n",
    "ax.fill_between(episodes, td_avg - td_std, td_avg + td_std, \n",
    "                alpha=0.2, color='blue')\n",
    "\n",
    "# Monte Carlo line\n",
    "ax.plot(episodes, mc_avg, linewidth=2, color='red', label='Monte Carlo', alpha=0.8)\n",
    "ax.fill_between(episodes, mc_avg - mc_std, mc_avg + mc_std, \n",
    "                alpha=0.2, color='red')\n",
    "\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel(f'Value Estimate for State {track_state}', fontsize=12)\n",
    "ax.set_title('Convergence Speed: TD(0) vs Monte Carlo', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Variance comparison\n",
    "ax = axes[1]\n",
    "\n",
    "# Calculate rolling standard deviation (variance proxy)\n",
    "window = 50\n",
    "td_rolling_std = pd.Series(td_avg).rolling(window=window, min_periods=1).std()\n",
    "mc_rolling_std = pd.Series(mc_avg).rolling(window=window, min_periods=1).std()\n",
    "\n",
    "ax.plot(episodes, td_rolling_std, linewidth=2, color='blue', \n",
    "        label='TD(0)', alpha=0.8)\n",
    "ax.plot(episodes, mc_rolling_std, linewidth=2, color='red', \n",
    "        label='Monte Carlo', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel(f'Rolling Std Dev (window={window})', fontsize=12)\n",
    "ax.set_title('Variance Comparison', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüìä Convergence Analysis:\\n\")\n",
    "\n",
    "# Find when each method gets close to final value\n",
    "td_final = td_avg[-1]\n",
    "mc_final = mc_avg[-1]\n",
    "threshold = 0.1  # Within 10% of final value\n",
    "\n",
    "td_converge = np.where(np.abs(td_avg - td_final) < threshold * abs(td_final))[0]\n",
    "mc_converge = np.where(np.abs(mc_avg - mc_final) < threshold * abs(mc_final))[0]\n",
    "\n",
    "td_converge_ep = td_converge[0] if len(td_converge) > 0 else num_episodes\n",
    "mc_converge_ep = mc_converge[0] if len(mc_converge) > 0 else num_episodes\n",
    "\n",
    "print(f\"Final Value Estimates:\")\n",
    "print(f\"  TD(0):        {td_final:.4f}\")\n",
    "print(f\"  Monte Carlo:  {mc_final:.4f}\")\n",
    "print(f\"  Difference:   {abs(td_final - mc_final):.4f}\")\n",
    "\n",
    "print(f\"\\nConvergence Speed (episodes to reach 90% of final value):\")\n",
    "print(f\"  TD(0):        {td_converge_ep} episodes\")\n",
    "print(f\"  Monte Carlo:  {mc_converge_ep} episodes\")\n",
    "if td_converge_ep < mc_converge_ep:\n",
    "    speedup = mc_converge_ep / max(td_converge_ep, 1)\n",
    "    print(f\"  ‚Üí TD(0) is {speedup:.1f}x faster!\")\n",
    "\n",
    "print(f\"\\nVariance (average std dev across runs):\")\n",
    "print(f\"  TD(0):        {np.mean(td_std):.4f}\")\n",
    "print(f\"  Monte Carlo:  {np.mean(mc_std):.4f}\")\n",
    "variance_reduction = (1 - np.mean(td_std) / np.mean(mc_std)) * 100\n",
    "print(f\"  ‚Üí TD(0) has {variance_reduction:.1f}% lower variance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚úÖ Key Observations:\\n\")\n",
    "print(\"1. Faster Convergence:\")\n",
    "print(\"   ‚Ä¢ TD(0) typically converges faster than MC\")\n",
    "print(\"   ‚Ä¢ Updates after every step vs waiting for episode end\")\n",
    "print(\"   ‚Ä¢ Information propagates more quickly through states\")\n",
    "\n",
    "print(\"\\n2. Lower Variance:\")\n",
    "print(\"   ‚Ä¢ TD(0) has smoother learning curves\")\n",
    "print(\"   ‚Ä¢ Bootstrapping reduces variance\")\n",
    "print(\"   ‚Ä¢ More stable estimates with fewer episodes\")\n",
    "\n",
    "print(\"\\n3. Sample Efficiency:\")\n",
    "print(\"   ‚Ä¢ TD(0) learns more from each episode\")\n",
    "print(\"   ‚Ä¢ Every transition provides a learning opportunity\")\n",
    "print(\"   ‚Ä¢ Better use of experience\")\n",
    "\n",
    "print(\"\\nüéØ Conclusion:\")\n",
    "print(\"   TD learning combines the best of both worlds:\")\n",
    "print(\"   ‚Ä¢ Model-free like Monte Carlo\")\n",
    "print(\"   ‚Ä¢ Bootstrapping like Dynamic Programming\")\n",
    "print(\"   ‚Ä¢ Result: Faster, more efficient learning!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: TD(0) Prediction\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **TD Learning Fundamentals**:\n",
    "   - Learn from every step, not just episode ends\n",
    "   - Bootstrap from current estimates\n",
    "   - Combine MC's model-free approach with DP's bootstrapping\n",
    "\n",
    "2. **TD(0) Update Rule**:\n",
    "   $$V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]$$\n",
    "   - TD target: $R_{t+1} + \\gamma V(S_{t+1})$\n",
    "   - TD error: $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$\n",
    "\n",
    "3. **Advantages Over Monte Carlo**:\n",
    "   - Faster convergence\n",
    "   - Lower variance\n",
    "   - Online learning\n",
    "   - Works with continuing tasks\n",
    "   - More sample efficient\n",
    "\n",
    "4. **Trade-offs**:\n",
    "   - TD is biased (uses estimates)\n",
    "   - MC is unbiased (uses actual returns)\n",
    "   - In practice, TD's lower variance usually wins\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "TD(0) is just for prediction (evaluating a policy). In the next sections, we'll explore:\n",
    "- **SARSA**: On-policy TD control (learning optimal policies)\n",
    "- **Q-Learning**: Off-policy TD control\n",
    "- **Deep RL**: Combining TD learning with neural networks\n",
    "\n",
    "These methods build on the TD(0) foundation to create powerful learning algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA: On-Policy TD Control\n",
    "\n",
    "**From Prediction to Control**\n",
    "\n",
    "TD(0) taught us how to evaluate a policy (prediction). Now we'll learn how to find optimal policies using **SARSA** (State-Action-Reward-State-Action), an on-policy TD control algorithm.\n",
    "\n",
    "**What is SARSA?**\n",
    "\n",
    "SARSA is a TD method that learns action-value functions Q(s,a) instead of state-value functions V(s). By learning Q-values, the agent can directly select actions without needing a model of the environment.\n",
    "\n",
    "**Why \"SARSA\"?**\n",
    "\n",
    "The name comes from the tuple of information used in each update:\n",
    "- **S**: Current state\n",
    "- **A**: Action taken\n",
    "- **R**: Reward received\n",
    "- **S'**: Next state\n",
    "- **A'**: Next action (chosen by the current policy)\n",
    "\n",
    "**On-Policy Learning:**\n",
    "\n",
    "SARSA is an **on-policy** algorithm, meaning:\n",
    "- It learns about the policy it's currently following\n",
    "- The next action A' used in the update is chosen by the same policy being learned\n",
    "- This makes SARSA more conservative and safer in practice\n",
    "\n",
    "**The SARSA Update Rule:**\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q(S_t, A_t)$: Current Q-value estimate\n",
    "- $\\alpha$: Learning rate (step size)\n",
    "- $R_{t+1}$: Immediate reward\n",
    "- $\\gamma$: Discount factor\n",
    "- $Q(S_{t+1}, A_{t+1})$: Q-value of next state-action pair\n",
    "- $A_{t+1}$: Action actually taken in next state (following current policy)\n",
    "\n",
    "**SARSA TD Target:**\n",
    "\n",
    "$$\n",
    "\\text{TD Target} = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})\n",
    "$$\n",
    "\n",
    "**SARSA TD Error:**\n",
    "\n",
    "$$\n",
    "\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\n",
    "$$\n",
    "\n",
    "**Key Differences from TD(0):**\n",
    "\n",
    "| Aspect | TD(0) | SARSA |\n",
    "|--------|-------|-------|\n",
    "| Learns | State values V(s) | Action values Q(s,a) |\n",
    "| Purpose | Policy evaluation | Policy improvement |\n",
    "| Update uses | Next state value | Next state-action value |\n",
    "| Output | Value function | Optimal policy |\n",
    "\n",
    "**SARSA Algorithm:**\n",
    "\n",
    "1. Initialize Q(s,a) arbitrarily for all state-action pairs\n",
    "2. For each episode:\n",
    "   - Initialize state S\n",
    "   - Choose action A from S using policy derived from Q (e.g., Œµ-greedy)\n",
    "   - For each step of episode:\n",
    "     - Take action A, observe R and S'\n",
    "     - Choose A' from S' using policy derived from Q\n",
    "     - Update: Q(S,A) ‚Üê Q(S,A) + Œ±[R + Œ≥Q(S',A') - Q(S,A)]\n",
    "     - S ‚Üê S', A ‚Üê A'\n",
    "   - Until S is terminal\n",
    "\n",
    "Let's implement SARSA and apply it to the Taxi-v3 environment from OpenAI Gym!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing SARSA for Taxi-v3\n",
    "\n",
    "**The Taxi Problem:**\n",
    "\n",
    "The Taxi-v3 environment is a classic RL problem where:\n",
    "- A taxi must pick up a passenger at one location and drop them off at another\n",
    "- The taxi can move in 4 directions (North, South, East, West)\n",
    "- The taxi can pick up and drop off passengers\n",
    "- Rewards: +20 for successful dropoff, -1 per step, -10 for illegal pick-up/drop-off\n",
    "\n",
    "This is a perfect environment to demonstrate SARSA because:\n",
    "- Discrete state and action spaces (good for tabular methods)\n",
    "- Clear goal and reward structure\n",
    "- Requires learning a multi-step strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    \"\"\"\n",
    "    SARSA (On-Policy TD Control) Agent.\n",
    "    \n",
    "    Learns optimal policy through on-policy temporal difference learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initialize SARSA agent.\n",
    "        \n",
    "        Args:\n",
    "            n_states: Number of states in the environment\n",
    "            n_actions: Number of actions available\n",
    "            alpha: Learning rate (step size)\n",
    "            gamma: Discount factor\n",
    "            epsilon: Exploration rate for Œµ-greedy policy\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize Q-table with zeros\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select action using Œµ-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            \n",
    "        Returns:\n",
    "            action: Selected action\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: choose random action\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            # Exploit: choose best action\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_action):\n",
    "        \"\"\"\n",
    "        Update Q-value using SARSA update rule.\n",
    "        \n",
    "        Q(S,A) ‚Üê Q(S,A) + Œ±[R + Œ≥Q(S',A') - Q(S,A)]\n",
    "        \n",
    "        Args:\n",
    "            state: Current state S\n",
    "            action: Action taken A\n",
    "            reward: Reward received R\n",
    "            next_state: Next state S'\n",
    "            next_action: Next action A' (chosen by policy)\n",
    "        \"\"\"\n",
    "        # SARSA TD target: R + Œ≥Q(S',A')\n",
    "        td_target = reward + self.gamma * self.Q[next_state, next_action]\n",
    "        \n",
    "        # TD error: TD target - current estimate\n",
    "        td_error = td_target - self.Q[state, action]\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.Q[state, action] += self.alpha * td_error\n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        \"\"\"\n",
    "        Get the greedy action (best action) for a state.\n",
    "        Used for evaluation without exploration.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            \n",
    "        Returns:\n",
    "            action: Best action according to Q-table\n",
    "        \"\"\"\n",
    "        return np.argmax(self.Q[state])\n",
    "\n",
    "\n",
    "print(\"SARSA Agent Implemented!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"  ‚Ä¢ On-policy TD control algorithm\")\n",
    "print(\"  ‚Ä¢ Learns Q(s,a) action-value function\")\n",
    "print(\"  ‚Ä¢ Uses Œµ-greedy policy for exploration\")\n",
    "print(\"  ‚Ä¢ Updates based on action actually taken\")\n",
    "print(\"  ‚Ä¢ Suitable for episodic tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training SARSA on Taxi-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(env, agent, num_episodes=5000, max_steps=200):\n",
    "    \"\"\"\n",
    "    Train SARSA agent on an environment.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI Gym environment\n",
    "        agent: SARSA agent\n",
    "        num_episodes: Number of training episodes\n",
    "        max_steps: Maximum steps per episode\n",
    "        \n",
    "    Returns:\n",
    "        episode_rewards: List of total rewards per episode\n",
    "        episode_lengths: List of episode lengths\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize episode\n",
    "        state = env.reset()\n",
    "        action = agent.select_action(state)  # Choose initial action\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Take action, observe result\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if not done:\n",
    "                # Choose next action using current policy\n",
    "                next_action = agent.select_action(next_state)\n",
    "                \n",
    "                # SARSA update\n",
    "                agent.update(state, action, reward, next_state, next_action)\n",
    "                \n",
    "                # Move to next state-action pair\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            else:\n",
    "                # Terminal state: Q(S',A') = 0\n",
    "                agent.update(state, action, reward, next_state, 0)\n",
    "                break\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_length = np.mean(episode_lengths[-100:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | \"\n",
    "                  f\"Avg Reward (last 100): {avg_reward:.2f} | \"\n",
    "                  f\"Avg Length: {avg_length:.1f}\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "\n",
    "# Create Taxi-v3 environment\n",
    "print(\"Training SARSA Agent on Taxi-v3 Environment\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "print(f\"\\nEnvironment Details:\")\n",
    "print(f\"  State space size: {env.observation_space.n}\")\n",
    "print(f\"  Action space size: {env.action_space.n}\")\n",
    "print(f\"  Actions: 0=South, 1=North, 2=East, 3=West, 4=Pickup, 5=Dropoff\")\n",
    "\n",
    "# Create SARSA agent\n",
    "agent = SARSAAgent(\n",
    "    n_states=env.observation_space.n,\n",
    "    n_actions=env.action_space.n,\n",
    "    alpha=0.1,      # Learning rate\n",
    "    gamma=0.99,     # Discount factor\n",
    "    epsilon=0.1     # Exploration rate\n",
    ")\n",
    "\n",
    "print(f\"\\nAgent Parameters:\")\n",
    "print(f\"  Learning rate (Œ±): {agent.alpha}\")\n",
    "print(f\"  Discount factor (Œ≥): {agent.gamma}\")\n",
    "print(f\"  Exploration rate (Œµ): {agent.epsilon}\")\n",
    "\n",
    "print(f\"\\nStarting training...\\n\")\n",
    "\n",
    "# Train the agent\n",
    "episode_rewards, episode_lengths = train_sarsa(env, agent, num_episodes=5000)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate final performance\n",
    "final_avg_reward = np.mean(episode_rewards[-100:])\n",
    "final_avg_length = np.mean(episode_lengths[-100:])\n",
    "\n",
    "print(f\"\\nFinal Performance (last 100 episodes):\")\n",
    "print(f\"  Average Reward: {final_avg_reward:.2f}\")\n",
    "print(f\"  Average Episode Length: {final_avg_length:.1f} steps\")\n",
    "print(f\"  Success Rate: {(np.array(episode_rewards[-100:]) > 0).mean() * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing SARSA Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of learning progress\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Calculate moving averages for smoother curves\n",
    "window = 100\n",
    "rewards_smooth = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "lengths_smooth = np.convolve(episode_lengths, np.ones(window)/window, mode='valid')\n",
    "\n",
    "episodes = np.arange(len(episode_rewards))\n",
    "episodes_smooth = np.arange(len(rewards_smooth))\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "ax1.plot(episodes, episode_rewards, alpha=0.3, color='blue', linewidth=0.5, label='Raw Rewards')\n",
    "ax1.plot(episodes_smooth, rewards_smooth, color='darkblue', linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5, linewidth=1, label='Break-even')\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\n",
    "ax1.set_title('SARSA Learning Curve: Episode Rewards Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode Lengths\n",
    "ax2.plot(episodes, episode_lengths, alpha=0.3, color='green', linewidth=0.5, label='Raw Lengths')\n",
    "ax2.plot(episodes_smooth, lengths_smooth, color='darkgreen', linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Episode Length (steps)', fontsize=12)\n",
    "ax2.set_title('SARSA Learning Curve: Episode Length Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Learning Curve Analysis:\\n\")\n",
    "print(\"1. Episode Rewards:\")\n",
    "print(\"   ‚Ä¢ Initially negative (agent is learning)\")\n",
    "print(\"   ‚Ä¢ Gradually improves as Q-values converge\")\n",
    "print(\"   ‚Ä¢ Stabilizes at positive rewards (successful deliveries)\")\n",
    "\n",
    "print(\"\\n2. Episode Length:\")\n",
    "print(\"   ‚Ä¢ Initially high (random exploration)\")\n",
    "print(\"   ‚Ä¢ Decreases as agent learns efficient paths\")\n",
    "print(\"   ‚Ä¢ Stabilizes at optimal trajectory length\")\n",
    "\n",
    "print(\"\\n3. Learning Progress:\")\n",
    "initial_avg = np.mean(episode_rewards[:100])\n",
    "final_avg = np.mean(episode_rewards[-100:])\n",
    "improvement = final_avg - initial_avg\n",
    "print(f\"   ‚Ä¢ Initial performance (first 100 episodes): {initial_avg:.2f}\")\n",
    "print(f\"   ‚Ä¢ Final performance (last 100 episodes): {final_avg:.2f}\")\n",
    "print(f\"   ‚Ä¢ Total improvement: {improvement:.2f} ({improvement/abs(initial_avg)*100:.1f}% better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, agent, num_episodes=100, render=False):\n",
    "    \"\"\"\n",
    "    Evaluate the learned policy without exploration.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI Gym environment\n",
    "        agent: Trained SARSA agent\n",
    "        num_episodes: Number of evaluation episodes\n",
    "        render: Whether to render the environment\n",
    "        \n",
    "    Returns:\n",
    "        avg_reward: Average reward over episodes\n",
    "        avg_length: Average episode length\n",
    "        success_rate: Percentage of successful episodes\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    episode_lengths = []\n",
    "    successes = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            if render and episode == 0:  # Render first episode only\n",
    "                env.render()\n",
    "            \n",
    "            # Use greedy policy (no exploration)\n",
    "            action = agent.get_greedy_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        if total_reward > 0:  # Successful delivery\n",
    "            successes += 1\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_length = np.mean(episode_lengths)\n",
    "    success_rate = (successes / num_episodes) * 100\n",
    "    \n",
    "    return avg_reward, avg_length, success_rate\n",
    "\n",
    "\n",
    "print(\"Evaluating Learned Policy\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "avg_reward, avg_length, success_rate = evaluate_policy(env, agent, num_episodes=100)\n",
    "\n",
    "print(f\"\\nEvaluation Results (100 episodes, greedy policy):\")\n",
    "print(f\"  Average Reward: {avg_reward:.2f}\")\n",
    "print(f\"  Average Episode Length: {avg_length:.1f} steps\")\n",
    "print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚úÖ SARSA Successfully Learned an Optimal Policy!\")\n",
    "print(\"\\nKey Achievements:\")\n",
    "print(\"  ‚Ä¢ Agent learned to navigate the taxi environment\")\n",
    "print(\"  ‚Ä¢ Discovered optimal pickup and dropoff strategies\")\n",
    "print(\"  ‚Ä¢ Achieved high success rate with efficient paths\")\n",
    "print(\"  ‚Ä¢ Learned entirely from trial and error!\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: SARSA Algorithm\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **SARSA Fundamentals**:\n",
    "   - On-policy TD control algorithm\n",
    "   - Learns Q(s,a) action-value function\n",
    "   - Updates based on actions actually taken by the policy\n",
    "   - Name from: State-Action-Reward-State-Action\n",
    "\n",
    "2. **SARSA Update Rule**:\n",
    "   $$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]$$\n",
    "   - Uses the action A' actually chosen by the policy\n",
    "   - On-policy: learns about the policy being followed\n",
    "   - Conservative: accounts for exploration in learning\n",
    "\n",
    "3. **Practical Implementation**:\n",
    "   - Successfully trained agent on Taxi-v3 environment\n",
    "   - Achieved high success rate and efficient navigation\n",
    "   - Demonstrated clear learning progress over episodes\n",
    "   - Learned complex multi-step strategies\n",
    "\n",
    "4. **Key Advantages**:\n",
    "   - Model-free: no need to know environment dynamics\n",
    "   - Online learning: updates after every step\n",
    "   - Guaranteed convergence (under certain conditions)\n",
    "   - Safe exploration: learns about actual policy behavior\n",
    "\n",
    "5. **On-Policy vs Off-Policy**:\n",
    "   - SARSA (on-policy): learns about the policy it follows\n",
    "   - More conservative, safer in practice\n",
    "   - Next: Q-Learning (off-policy) for comparison\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Now that we understand SARSA (on-policy TD control), we'll explore:\n",
    "- **Q-Learning**: Off-policy TD control that learns optimal policy directly\n",
    "- **Deep Q-Networks (DQN)**: Scaling TD learning to large state spaces\n",
    "- **Policy Gradient Methods**: Direct policy optimization\n",
    "\n",
    "These methods build on the TD learning foundation to tackle increasingly complex problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q-learning'></a>\\n",
    "### Q-Learning: Off-Policy TD Control\\n",
    "\\n",
    "**From On-Policy to Off-Policy Learning**\\n",
    "\\n",
    "We've seen how SARSA learns about the policy it's currently following (on-policy). Now we'll explore **Q-Learning**, one of the most important breakthroughs in reinforcement learning - an off-policy TD control algorithm that learns the optimal policy directly!\\n",
    "\\n",
    "**What is Q-Learning?**\\n",
    "\\n",
    "Q-Learning is a model-free, off-policy TD control algorithm that learns the optimal action-value function Q*(s,a) regardless of the policy being followed. This makes it more flexible and often more sample-efficient than on-policy methods.\\n",
    "\\n",
    "**Key Characteristics:**\\n",
    "\\n",
    "1. **Off-Policy**: Learns about the optimal policy while following a different (exploratory) policy\\n",
    "2. **Model-Free**: Doesn't require knowledge of environment dynamics (transition probabilities or rewards)\\n",
    "3. **Value-Based**: Learns Q-values, from which the optimal policy can be derived\\n",
    "4. **Bootstrapping**: Updates estimates based on other estimates (like all TD methods)\\n",
    "\\n",
    "**Why is Q-Learning Model-Free?**\\n",
    "\\n",
    "Q-Learning is considered model-free because:\\n",
    "\\n",
    "- **No Environment Model Required**: The agent doesn't need to know P(s'|s,a) (transition probabilities) or R(s,a) (reward function)\\n",
    "- **Learns from Experience**: Updates Q-values directly from observed transitions (s, a, r, s')\\n",
    "- **No Planning**: Doesn't simulate future trajectories using a model\\n",
    "- **Direct Learning**: Learns the value function without first learning how the environment works\\n",
    "\\n",
    "This is in contrast to model-based methods (like Dynamic Programming) that require complete knowledge of the environment's dynamics.\\n",
    "\\n",
    "**The Q-Learning Update Rule:**\\n",
    "\\n",
    "$$\\n",
    "Q(S_t, A_t) \\\\leftarrow Q(S_t, A_t) + \\\\alpha \\\\left[ R_{t+1} + \\\\gamma \\\\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\\\right]\\n",
    "$$\\n",
    "\\n",
    "Where:\\n",
    "- $S_t$: Current state\\n",
    "- $A_t$: Action taken\\n",
    "- $R_{t+1}$: Reward received\\n",
    "- $S_{t+1}$: Next state\\n",
    "- $\\\\alpha$: Learning rate\\n",
    "- $\\\\gamma$: Discount factor\\n",
    "- $\\\\max_{a} Q(S_{t+1}, a)$: Maximum Q-value over all actions in next state\\n",
    "\\n",
    "**Q-Learning TD Target:**\\n",
    "\\n",
    "$$\\n",
    "\\\\text{TD Target} = R_{t+1} + \\\\gamma \\\\max_{a} Q(S_{t+1}, a)\\n",
    "$$\\n",
    "\\n",
    "**Q-Learning TD Error:**\\n",
    "\\n",
    "$$\\n",
    "\\\\delta_t = R_{t+1} + \\\\gamma \\\\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)\\n",
    "$$\\n",
    "\\n",
    "**The Key Difference: max vs actual action**\\n",
    "\\n",
    "| Aspect | SARSA (On-Policy) | Q-Learning (Off-Policy) |\\n",
    "|--------|-------------------|-------------------------|\\n",
    "| Update uses | $Q(S', A')$ - action actually taken | $\\\\max_a Q(S', a)$ - best possible action |\\n",
    "| Learns about | Policy being followed | Optimal policy |\\n",
    "| Behavior | Conservative, accounts for exploration | Optimistic, assumes optimal behavior |\\n",
    "| Convergence | To policy being followed | To optimal policy Q* |\\n",
    "\\n",
    "**Why the max operator matters:**\\n",
    "\\n",
    "- SARSA: \\\"What will I actually do next?\\\" ‚Üí Uses A' from current policy\\n",
    "- Q-Learning: \\\"What's the best I could do next?\\\" ‚Üí Uses max over all actions\\n",
    "\\n",
    "This makes Q-Learning learn the optimal policy even while exploring randomly!\\n",
    "\\n",
    "**Q-Learning Algorithm:**\\n",
    "\\n",
    "1. Initialize Q(s,a) arbitrarily for all state-action pairs\\n",
    "2. For each episode:\\n",
    "   - Initialize state S\\n",
    "   - For each step of episode:\\n",
    "     - Choose action A from S using policy derived from Q (e.g., Œµ-greedy)\\n",
    "     - Take action A, observe R and S'\\n",
    "     - Update: $Q(S,A) \\\\leftarrow Q(S,A) + \\\\alpha[R + \\\\gamma \\\\max_a Q(S',a) - Q(S,A)]$\\n",
    "     - S ‚Üê S'\\n",
    "   - Until S is terminal\\n",
    "\\n",
    "Let's implement Q-Learning and apply it to a grid-world problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Q-Learning for Grid World\\n",
    "\\n",
    "**The Grid World Problem:**\\n",
    "\\n",
    "We'll create a simple grid world environment where:\\n",
    "- The agent starts at a specific position\\n",
    "- The goal is to reach a target position\\n",
    "- The agent can move in 4 directions: up, down, left, right\\n",
    "- Rewards: +10 for reaching goal, -1 for each step, -10 for hitting walls\\n",
    "\\n",
    "This is perfect for demonstrating Q-Learning because:\\n",
    "- Simple, discrete state and action spaces\\n",
    "- Clear optimal policy exists\\n",
    "- Easy to visualize Q-values and learned policy\\n",
    "- Can compare with SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\\n",
    "    \\\"\\\"\\\"A simple grid world environment for Q-Learning.\\\"\\\"\\\"\\n",
    "    \\n",
    "    def __init__(self, size=5, start=(0, 0), goal=(4, 4), obstacles=None):\\n",
    "        self.size = size\\n",
    "        self.start = start\\n",
    "        self.goal = goal\\n",
    "        self.obstacles = obstacles if obstacles else []\\n",
    "        self.state = start\\n",
    "        \\n",
    "        # Actions: 0=up, 1=down, 2=left, 3=right\\n",
    "        self.actions = [0, 1, 2, 3]\\n",
    "        self.action_names = ['UP', 'DOWN', 'LEFT', 'RIGHT']\\n",
    "    \\n",
    "    def reset(self):\\n",
    "        \\\"\\\"\\\"Reset environment to start state.\\\"\\\"\\\"\\n",
    "        self.state = self.start\\n",
    "        return self.state\\n",
    "    \\n",
    "    def step(self, action):\\n",
    "        \\\"\\\"\\\"Execute action and return (next_state, reward, done).\\\"\\\"\\\"\\n",
    "        row, col = self.state\\n",
    "        \\n",
    "        # Calculate new position\\n",
    "        if action == 0:  # UP\\n",
    "            new_state = (max(0, row - 1), col)\\n",
    "        elif action == 1:  # DOWN\\n",
    "            new_state = (min(self.size - 1, row + 1), col)\\n",
    "        elif action == 2:  # LEFT\\n",
    "            new_state = (row, max(0, col - 1))\\n",
    "        else:  # RIGHT\\n",
    "            new_state = (row, min(self.size - 1, col + 1))\\n",
    "        \\n",
    "        # Check if hit obstacle\\n",
    "        if new_state in self.obstacles:\\n",
    "            new_state = self.state  # Stay in place\\n",
    "            reward = -10\\n",
    "        # Check if reached goal\\n",
    "        elif new_state == self.goal:\\n",
    "            reward = 10\\n",
    "        # Normal step\\n",
    "        else:\\n",
    "            reward = -1\\n",
    "        \\n",
    "        self.state = new_state\\n",
    "        done = (new_state == self.goal)\\n",
    "        \\n",
    "        return new_state, reward, done\\n",
    "\\n",
    "print(\\\"Grid World Environment Created!\\\")\\n",
    "print(\\\"=\\\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\\n",
    "    \\\"\\\"\\\"Q-Learning (Off-Policy TD Control) Agent.\\\"\\\"\\\"\\n",
    "    \\n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\\n",
    "        \\\"\\\"\\\"\\n",
    "        Initialize Q-Learning agent.\\n",
    "        \\n",
    "        Args:\\n",
    "            n_states: Number of states (for grid: size * size)\\n",
    "            n_actions: Number of actions (4 for grid world)\\n",
    "            alpha: Learning rate\\n",
    "            gamma: Discount factor\\n",
    "            epsilon: Exploration rate for Œµ-greedy\\n",
    "        \\\"\\\"\\\"\\n",
    "        self.n_states = n_states\\n",
    "        self.n_actions = n_actions\\n",
    "        self.alpha = alpha\\n",
    "        self.gamma = gamma\\n",
    "        self.epsilon = epsilon\\n",
    "        \\n",
    "        # Initialize Q-table: Q[state][action]\\n",
    "        # For grid world, state is (row, col) tuple\\n",
    "        self.Q = {}\\n",
    "    \\n",
    "    def get_q_value(self, state, action):\\n",
    "        \\\"\\\"\\\"Get Q-value for state-action pair.\\\"\\\"\\\"\\n",
    "        if state not in self.Q:\\n",
    "            self.Q[state] = np.zeros(self.n_actions)\\n",
    "        return self.Q[state][action]\\n",
    "    \\n",
    "    def select_action(self, state):\\n",
    "        \\\"\\\"\\\"Select action using Œµ-greedy policy.\\\"\\\"\\\"\\n",
    "        if np.random.random() < self.epsilon:\\n",
    "            # Explore: random action\\n",
    "            return np.random.randint(self.n_actions)\\n",
    "        else:\\n",
    "            # Exploit: best action\\n",
    "            return self.get_greedy_action(state)\\n",
    "    \\n",
    "    def get_greedy_action(self, state):\\n",
    "        \\\"\\\"\\\"Get best action for state (greedy).\\\"\\\"\\\"\\n",
    "        if state not in self.Q:\\n",
    "            self.Q[state] = np.zeros(self.n_actions)\\n",
    "        return np.argmax(self.Q[state])\\n",
    "    \\n",
    "    def update(self, state, action, reward, next_state, done):\\n",
    "        \\\"\\\"\\\"\\n",
    "        Update Q-value using Q-Learning update rule.\\n",
    "        \\n",
    "        Q(S,A) ‚Üê Q(S,A) + Œ±[R + Œ≥ max_a Q(S',a) - Q(S,A)]\\n",
    "        \\n",
    "        Args:\\n",
    "            state: Current state S\\n",
    "            action: Action taken A\\n",
    "            reward: Reward received R\\n",
    "            next_state: Next state S'\\n",
    "            done: Whether episode ended\\n",
    "        \\\"\\\"\\\"\\n",
    "        # Ensure states exist in Q-table\\n",
    "        if state not in self.Q:\\n",
    "            self.Q[state] = np.zeros(self.n_actions)\\n",
    "        if next_state not in self.Q:\\n",
    "            self.Q[next_state] = np.zeros(self.n_actions)\\n",
    "        \\n",
    "        # Q-Learning TD target: R + Œ≥ max_a Q(S',a)\\n",
    "        # Key difference from SARSA: uses MAX instead of actual next action\\n",
    "        if done:\\n",
    "            td_target = reward  # No future rewards if episode ended\\n",
    "        else:\\n",
    "            td_target = reward + self.gamma * np.max(self.Q[next_state])\\n",
    "        \\n",
    "        # TD error\\n",
    "        td_error = td_target - self.Q[state][action]\\n",
    "        \\n",
    "        # Update Q-value\\n",
    "        self.Q[state][action] += self.alpha * td_error\\n",
    "\\n",
    "print(\\\"Q-Learning Agent Implemented!\\\")\\n",
    "print(\\\"=\\\" * 60)\\n",
    "print(\\\"\\\\nKey Features:\\\")\\n",
    "print(\\\"  ‚Ä¢ Off-policy learning with Œµ-greedy exploration\\\")\\n",
    "print(\\\"  ‚Ä¢ Uses max Q-value for next state (not actual action)\\\")\\n",
    "print(\\\"  ‚Ä¢ Learns optimal policy Q* directly\\\")\\n",
    "print(\\\"  ‚Ä¢ Model-free: no environment dynamics needed\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qlearning(env, agent, num_episodes=1000, max_steps=100):\\n",
    "    \\\"\\\"\\\"\\n",
    "    Train Q-Learning agent on environment.\\n",
    "    \\n",
    "    Args:\\n",
    "        env: Grid world environment\\n",
    "        agent: Q-Learning agent\\n",
    "        num_episodes: Number of training episodes\\n",
    "        max_steps: Maximum steps per episode\\n",
    "    \\n",
    "    Returns:\\n",
    "        episode_rewards: List of total rewards per episode\\n",
    "        episode_lengths: List of episode lengths\\n",
    "    \\\"\\\"\\\"\\n",
    "    episode_rewards = []\\n",
    "    episode_lengths = []\\n",
    "    \\n",
    "    for episode in range(num_episodes):\\n",
    "        state = env.reset()\\n",
    "        total_reward = 0\\n",
    "        steps = 0\\n",
    "        \\n",
    "        for step in range(max_steps):\\n",
    "            # Select action using Œµ-greedy\\n",
    "            action = agent.select_action(state)\\n",
    "            \\n",
    "            # Take action\\n",
    "            next_state, reward, done = env.step(action)\\n",
    "            \\n",
    "            # Q-Learning update (off-policy)\\n",
    "            agent.update(state, action, reward, next_state, done)\\n",
    "            \\n",
    "            total_reward += reward\\n",
    "            steps += 1\\n",
    "            state = next_state\\n",
    "            \\n",
    "            if done:\\n",
    "                break\\n",
    "        \\n",
    "        episode_rewards.append(total_reward)\\n",
    "        episode_lengths.append(steps)\\n",
    "        \\n",
    "        # Print progress\\n",
    "        if (episode + 1) % 100 == 0:\\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\\n",
    "            avg_length = np.mean(episode_lengths[-100:])\\n",
    "            print(f\\\"Episode {episode + 1}/{num_episodes} | \\\"\\n",
    "                  f\\\"Avg Reward: {avg_reward:.2f} | Avg Length: {avg_length:.1f}\\\")\\n",
    "    \\n",
    "    return episode_rewards, episode_lengths\\n",
    "\\n",
    "print(\\\"Training function ready!\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and agent\\n",
    "print(\\\"Training Q-Learning Agent on Grid World\\\\n\\\")\\n",
    "print(\\\"=\\\" * 60)\\n",
    "\\n",
    "# Create 5x5 grid world with some obstacles\\n",
    "obstacles = [(1, 1), (2, 2), (3, 1)]\\n",
    "env = GridWorld(size=5, start=(0, 0), goal=(4, 4), obstacles=obstacles)\\n",
    "\\n",
    "print(f\\\"Environment:\\\")\\n",
    "print(f\\\"  Grid size: {env.size}x{env.size}\\\")\\n",
    "print(f\\\"  Start: {env.start}\\\")\\n",
    "print(f\\\"  Goal: {env.goal}\\\")\\n",
    "print(f\\\"  Obstacles: {obstacles}\\\")\\n",
    "print(f\\\"  Actions: {env.action_names}\\\")\\n",
    "\\n",
    "# Create Q-Learning agent\\n",
    "agent = QLearningAgent(\\n",
    "    n_states=env.size * env.size,\\n",
    "    n_actions=len(env.actions),\\n",
    "    alpha=0.1,\\n",
    "    gamma=0.95,\\n",
    "    epsilon=0.1\\n",
    ")\\n",
    "\\n",
    "print(f\\\"\\\\nAgent Parameters:\\\")\\n",
    "print(f\\\"  Learning rate (Œ±): {agent.alpha}\\\")\\n",
    "print(f\\\"  Discount factor (Œ≥): {agent.gamma}\\\")\\n",
    "print(f\\\"  Exploration rate (Œµ): {agent.epsilon}\\\")\\n",
    "\\n",
    "print(f\\\"\\\\nStarting training...\\\\n\\\")\\n",
    "\\n",
    "# Train the agent\\n",
    "episode_rewards, episode_lengths = train_qlearning(env, agent, num_episodes=1000)\\n",
    "\\n",
    "print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\\n",
    "print(\\\"Training complete!\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Q-Learning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\\n",
    "\\n",
    "# Smooth the curves\\n",
    "window = 50\\n",
    "rewards_smooth = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\\n",
    "lengths_smooth = np.convolve(episode_lengths, np.ones(window)/window, mode='valid')\\n",
    "\\n",
    "# Plot 1: Rewards\\n",
    "ax1.plot(episode_rewards, alpha=0.3, color='blue', label='Raw')\\n",
    "ax1.plot(range(window-1, len(episode_rewards)), rewards_smooth, \\n",
    "         linewidth=2, color='blue', label=f'Smoothed ({window}-episode avg)')\\n",
    "ax1.set_xlabel('Episode', fontsize=12)\\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\\n",
    "ax1.set_title('Q-Learning: Episode Rewards Over Time', fontsize=14, fontweight='bold')\\n",
    "ax1.legend(fontsize=10)\\n",
    "ax1.grid(True, alpha=0.3)\\n",
    "\\n",
    "# Plot 2: Episode lengths\\n",
    "ax2.plot(episode_lengths, alpha=0.3, color='green', label='Raw')\\n",
    "ax2.plot(range(window-1, len(episode_lengths)), lengths_smooth, \\n",
    "         linewidth=2, color='green', label=f'Smoothed ({window}-episode avg)')\\n",
    "ax2.set_xlabel('Episode', fontsize=12)\\n",
    "ax2.set_ylabel('Episode Length (steps)', fontsize=12)\\n",
    "ax2.set_title('Q-Learning: Episode Length Over Time', fontsize=14, fontweight='bold')\\n",
    "ax2.legend(fontsize=10)\\n",
    "ax2.grid(True, alpha=0.3)\\n",
    "\\n",
    "plt.tight_layout()\\n",
    "plt.show()\\n",
    "\\n",
    "print(\\\"\\\\nüìä Interpretation:\\\")\\n",
    "print(\\\"   - Rewards increase as agent learns optimal policy\\\")\\n",
    "print(\\\"   - Episode length decreases as agent finds shorter paths\\\")\\n",
    "print(\\\"   - Convergence indicates successful learning!\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Learned Q-Values and Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_values_and_policy(agent, env):\\n",
    "    \\\"\\\"\\\"Visualize Q-values and learned policy on grid.\\\"\\\"\\\"\\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\\n",
    "    \\n",
    "    # Create grids for visualization\\n",
    "    max_q_grid = np.zeros((env.size, env.size))\\n",
    "    policy_grid = np.zeros((env.size, env.size), dtype=int)\\n",
    "    \\n",
    "    # Fill grids\\n",
    "    for row in range(env.size):\\n",
    "        for col in range(env.size):\\n",
    "            state = (row, col)\\n",
    "            if state in agent.Q:\\n",
    "                max_q_grid[row, col] = np.max(agent.Q[state])\\n",
    "                policy_grid[row, col] = np.argmax(agent.Q[state])\\n",
    "    \\n",
    "    # Plot 1: Q-values heatmap\\n",
    "    im1 = ax1.imshow(max_q_grid, cmap='RdYlGn', interpolation='nearest')\\n",
    "    ax1.set_title('Maximum Q-Values per State', fontsize=14, fontweight='bold')\\n",
    "    ax1.set_xlabel('Column', fontsize=12)\\n",
    "    ax1.set_ylabel('Row', fontsize=12)\\n",
    "    \\n",
    "    # Add Q-values as text\\n",
    "    for row in range(env.size):\\n",
    "        for col in range(env.size):\\n",
    "            state = (row, col)\\n",
    "            if state == env.goal:\\n",
    "                text = 'GOAL'\\n",
    "                color = 'white'\\n",
    "            elif state in env.obstacles:\\n",
    "                text = 'X'\\n",
    "                color = 'red'\\n",
    "            else:\\n",
    "                text = f'{max_q_grid[row, col]:.1f}'\\n",
    "                color = 'black' if max_q_grid[row, col] < 5 else 'white'\\n",
    "            ax1.text(col, row, text, ha='center', va='center', \\n",
    "                    color=color, fontsize=10, fontweight='bold')\\n",
    "    \\n",
    "    plt.colorbar(im1, ax=ax1, label='Max Q-Value')\\n",
    "    \\n",
    "    # Plot 2: Policy arrows\\n",
    "    ax2.imshow(np.ones((env.size, env.size)) * 0.5, cmap='gray', alpha=0.3)\\n",
    "    ax2.set_title('Learned Policy (Greedy)', fontsize=14, fontweight='bold')\\n",
    "    ax2.set_xlabel('Column', fontsize=12)\\n",
    "    ax2.set_ylabel('Row', fontsize=12)\\n",
    "    \\n",
    "    # Arrow directions\\n",
    "    arrows = {0: '‚Üë', 1: '‚Üì', 2: '‚Üê', 3: '‚Üí'}\\n",
    "    \\n",
    "    for row in range(env.size):\\n",
    "        for col in range(env.size):\\n",
    "            state = (row, col)\\n",
    "            if state == env.goal:\\n",
    "                ax2.add_patch(plt.Rectangle((col-0.4, row-0.4), 0.8, 0.8, \\n",
    "                                            fill=True, color='gold', alpha=0.7))\\n",
    "                ax2.text(col, row, '‚òÖ', ha='center', va='center', \\n",
    "                        fontsize=24, color='darkgreen')\\n",
    "            elif state in env.obstacles:\\n",
    "                ax2.add_patch(plt.Rectangle((col-0.4, row-0.4), 0.8, 0.8, \\n",
    "                                            fill=True, color='red', alpha=0.5))\\n",
    "                ax2.text(col, row, 'X', ha='center', va='center', \\n",
    "                        fontsize=20, color='darkred', fontweight='bold')\\n",
    "            else:\\n",
    "                action = policy_grid[row, col]\\n",
    "                ax2.text(col, row, arrows[action], ha='center', va='center', \\n",
    "                        fontsize=24, color='blue', fontweight='bold')\\n",
    "    \\n",
    "    ax2.set_xticks(range(env.size))\\n",
    "    ax2.set_yticks(range(env.size))\\n",
    "    ax2.grid(True, alpha=0.3)\\n",
    "    \\n",
    "    plt.tight_layout()\\n",
    "    plt.show()\\n",
    "\\n",
    "# Visualize\\n",
    "visualize_q_values_and_policy(agent, env)\\n",
    "\\n",
    "print(\\\"\\\\nüìä Visualization Explanation:\\\")\\n",
    "print(\\\"\\\\nLeft Plot (Q-Values):\\\")\\n",
    "print(\\\"  ‚Ä¢ Shows maximum Q-value for each state\\\")\\n",
    "print(\\\"  ‚Ä¢ Higher values (green) indicate states closer to goal\\\")\\n",
    "print(\\\"  ‚Ä¢ Lower values (red) indicate less desirable states\\\")\\n",
    "print(\\\"\\\\nRight Plot (Policy):\\\")\\n",
    "print(\\\"  ‚Ä¢ Arrows show the best action in each state\\\")\\n",
    "print(\\\"  ‚Ä¢ Policy learned to navigate around obstacles\\\")\\n",
    "print(\\\"  ‚Ä¢ All arrows point toward the goal (‚òÖ)\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Q-Learning Algorithm\\n",
    "\\n",
    "**What We Learned:**\\n",
    "\\n",
    "1. **Q-Learning Fundamentals**:\\n",
    "   - Off-policy TD control algorithm\\n",
    "   - Learns optimal Q*(s,a) directly\\n",
    "   - Uses max operator for next state value\\n",
    "   - Model-free: no environment dynamics needed\\n",
    "\\n",
    "2. **Q-Learning Update Rule**:\\n",
    "   $Q(S_t, A_t) \\\\leftarrow Q(S_t, A_t) + \\\\alpha \\\\left[ R_{t+1} + \\\\gamma \\\\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\\\right]$\\n",
    "   - Uses max Q-value for next state (not actual action)\\n",
    "   - Off-policy: learns optimal policy while exploring\\n",
    "   - Optimistic: assumes best possible future actions\\n",
    "\\n",
    "3. **Practical Implementation**:\\n",
    "   - Successfully trained agent on grid world\\n",
    "   - Learned to navigate around obstacles\\n",
    "   - Found optimal paths to goal\\n",
    "   - Visualized Q-values and policy\\n",
    "\\n",
    "4. **Key Advantages**:\\n",
    "   - Model-free: works without knowing environment dynamics\\n",
    "   - Off-policy: can learn from any exploratory policy\\n",
    "   - Converges to optimal policy Q*\\n",
    "   - Simple and effective for tabular problems\\n",
    "\\n",
    "5. **SARSA vs Q-Learning Comparison**:\\n",
    "\\n",
    "| Aspect | SARSA | Q-Learning |\\n",
    "|--------|-------|------------|\\n",
    "| Policy Type | On-policy | Off-policy |\\n",
    "| Update Target | $R + \\\\gamma Q(S', A')$ | $R + \\\\gamma \\\\max_a Q(S', a)$ |\\n",
    "| Learns | Policy being followed | Optimal policy |\\n",
    "| Behavior | Conservative | Optimistic |\\n",
    "| Safety | Safer (accounts for exploration) | Can be risky |\\n",
    "| Convergence | To followed policy | To optimal policy Q* |\\n",
    "\\n",
    "**Why Q-Learning is Model-Free:**\\n",
    "\\n",
    "Q-Learning doesn't require:\\n",
    "- Transition probabilities P(s'|s,a)\\n",
    "- Reward function R(s,a)\\n",
    "- Environment model for planning\\n",
    "\\n",
    "It learns directly from experience (s, a, r, s') tuples!\\n",
    "\\n",
    "**Next Steps:**\\n",
    "\\n",
    "Q-Learning works great for small, discrete state spaces. For larger problems, we need:\\n",
    "- **Deep Q-Networks (DQN)**: Combining Q-Learning with neural networks\\n",
    "- **Function Approximation**: Handling continuous and high-dimensional states\\n",
    "- **Advanced Techniques**: Experience replay, target networks, double Q-learning\\n",
    "\\n",
    "These extensions allow Q-Learning to scale to complex problems like Atari games and robotic control!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-Decreasing Exploration Strategy\n",
    "\n",
    "**The Problem with Fixed Epsilon**\n",
    "\n",
    "In our Q-Learning implementation above, we used a fixed epsilon value (Œµ = 0.1). This means the agent explores randomly 10% of the time throughout the entire training process. While this works, it's not optimal:\n",
    "\n",
    "- **Early in training**: We want MORE exploration to discover good actions\n",
    "- **Late in training**: We want LESS exploration to exploit what we've learned\n",
    "\n",
    "A fixed epsilon means we're either:\n",
    "- Under-exploring early (if Œµ is too small)\n",
    "- Over-exploring late (if Œµ is too large)\n",
    "\n",
    "**Exploration Schedules: The Solution**\n",
    "\n",
    "An **exploration schedule** (or **epsilon decay**) gradually reduces epsilon over time, allowing the agent to:\n",
    "1. Explore extensively at the beginning\n",
    "2. Gradually shift toward exploitation\n",
    "3. Eventually converge to a near-greedy policy\n",
    "\n",
    "**Common Epsilon Decay Strategies:**\n",
    "\n",
    "1. **Linear Decay**: Decrease epsilon by a constant amount each episode\n",
    "   $$\\epsilon_t = \\epsilon_{start} - \\frac{t}{T}(\\epsilon_{start} - \\epsilon_{end})$$\n",
    "   where $t$ is the current episode and $T$ is the total episodes\n",
    "\n",
    "2. **Exponential Decay**: Multiply epsilon by a decay factor each episode\n",
    "   $$\\epsilon_t = \\epsilon_{start} \\cdot \\gamma^t$$\n",
    "   where $\\gamma$ is the decay rate (e.g., 0.995)\n",
    "\n",
    "3. **Step Decay**: Reduce epsilon by a factor at specific intervals\n",
    "   $$\\epsilon_t = \\epsilon_{start} \\cdot \\text{decay}^{\\lfloor t / \\text{step} \\rfloor}$$\n",
    "\n",
    "**Key Parameters:**\n",
    "- $\\epsilon_{start}$: Initial exploration rate (e.g., 1.0 for full exploration)\n",
    "- $\\epsilon_{end}$: Minimum exploration rate (e.g., 0.01 to maintain some exploration)\n",
    "- Decay rate: How quickly epsilon decreases\n",
    "\n",
    "Let's implement these strategies and see their effect on learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonSchedule:\n",
    "    \"\"\"Base class for epsilon decay schedules.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon_start=1.0, epsilon_end=0.01):\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.current_epsilon = epsilon_start\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Get epsilon value for current episode.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset epsilon to starting value.\"\"\"\n",
    "        self.current_epsilon = self.epsilon_start\n",
    "\n",
    "\n",
    "class LinearDecay(EpsilonSchedule):\n",
    "    \"\"\"Linear epsilon decay schedule.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, decay_episodes=1000):\n",
    "        super().__init__(epsilon_start, epsilon_end)\n",
    "        self.decay_episodes = decay_episodes\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Linear decay: Œµ_t = Œµ_start - (t/T)(Œµ_start - Œµ_end)\"\"\"\n",
    "        if episode >= self.decay_episodes:\n",
    "            self.current_epsilon = self.epsilon_end\n",
    "        else:\n",
    "            decay_amount = (self.epsilon_start - self.epsilon_end) * (episode / self.decay_episodes)\n",
    "            self.current_epsilon = self.epsilon_start - decay_amount\n",
    "        \n",
    "        return self.current_epsilon\n",
    "\n",
    "\n",
    "class ExponentialDecay(EpsilonSchedule):\n",
    "    \"\"\"Exponential epsilon decay schedule.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, decay_rate=0.995):\n",
    "        super().__init__(epsilon_start, epsilon_end)\n",
    "        self.decay_rate = decay_rate\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Exponential decay: Œµ_t = Œµ_start * Œ≥^t\"\"\"\n",
    "        self.current_epsilon = max(\n",
    "            self.epsilon_end,\n",
    "            self.epsilon_start * (self.decay_rate ** episode)\n",
    "        )\n",
    "        return self.current_epsilon\n",
    "\n",
    "\n",
    "class StepDecay(EpsilonSchedule):\n",
    "    \"\"\"Step-based epsilon decay schedule.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, decay_factor=0.5, step_size=200):\n",
    "        super().__init__(epsilon_start, epsilon_end)\n",
    "        self.decay_factor = decay_factor\n",
    "        self.step_size = step_size\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Step decay: Œµ_t = Œµ_start * decay^‚åät/step‚åã\"\"\"\n",
    "        num_steps = episode // self.step_size\n",
    "        self.current_epsilon = max(\n",
    "            self.epsilon_end,\n",
    "            self.epsilon_start * (self.decay_factor ** num_steps)\n",
    "        )\n",
    "        return self.current_epsilon\n",
    "\n",
    "\n",
    "print(\"Epsilon Decay Schedules Implemented!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAvailable schedules:\")\n",
    "print(\"  1. LinearDecay: Constant decrease per episode\")\n",
    "print(\"  2. ExponentialDecay: Multiplicative decrease per episode\")\n",
    "print(\"  3. StepDecay: Decrease at fixed intervals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Epsilon Decay Over Time\n",
    "\n",
    "Let's visualize how different decay strategies behave over the course of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different decay schedules\n",
    "num_episodes = 1000\n",
    "\n",
    "schedules = {\n",
    "    'Linear Decay': LinearDecay(epsilon_start=1.0, epsilon_end=0.01, decay_episodes=800),\n",
    "    'Exponential Decay': ExponentialDecay(epsilon_start=1.0, epsilon_end=0.01, decay_rate=0.995),\n",
    "    'Step Decay': StepDecay(epsilon_start=1.0, epsilon_end=0.01, decay_factor=0.5, step_size=200),\n",
    "    'Fixed Epsilon': None  # For comparison\n",
    "}\n",
    "\n",
    "# Track epsilon values over episodes\n",
    "epsilon_values = {name: [] for name in schedules.keys()}\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    for name, schedule in schedules.items():\n",
    "        if schedule is None:\n",
    "            epsilon_values[name].append(0.1)  # Fixed epsilon\n",
    "        else:\n",
    "            epsilon_values[name].append(schedule.get_epsilon(episode))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'gray']\n",
    "linestyles = ['-', '-', '-', '--']\n",
    "\n",
    "for (name, values), color, linestyle in zip(epsilon_values.items(), colors, linestyles):\n",
    "    plt.plot(values, label=name, color=color, linestyle=linestyle, linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Episode', fontsize=12)\n",
    "plt.ylabel('Epsilon (Œµ)', fontsize=12)\n",
    "plt.title('Comparison of Epsilon Decay Strategies', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([-0.05, 1.05])\n",
    "\n",
    "# Add annotations\n",
    "plt.axhline(y=0.01, color='black', linestyle=':', alpha=0.5, linewidth=1)\n",
    "plt.text(num_episodes * 0.95, 0.05, 'Œµ_min = 0.01', fontsize=10, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Decay Strategy Characteristics:\\n\")\n",
    "print(\"Linear Decay:\")\n",
    "print(\"  ‚Ä¢ Constant rate of decrease\")\n",
    "print(\"  ‚Ä¢ Predictable and easy to tune\")\n",
    "print(\"  ‚Ä¢ Good for problems with known training duration\\n\")\n",
    "\n",
    "print(\"Exponential Decay:\")\n",
    "print(\"  ‚Ä¢ Fast initial decrease, then slower\")\n",
    "print(\"  ‚Ä¢ Smooth transition from exploration to exploitation\")\n",
    "print(\"  ‚Ä¢ Most commonly used in practice\\n\")\n",
    "\n",
    "print(\"Step Decay:\")\n",
    "print(\"  ‚Ä¢ Sudden drops at intervals\")\n",
    "print(\"  ‚Ä¢ Allows extended exploration at each level\")\n",
    "print(\"  ‚Ä¢ Useful for curriculum learning\\n\")\n",
    "\n",
    "print(\"Fixed Epsilon:\")\n",
    "print(\"  ‚Ä¢ No decay - constant exploration\")\n",
    "print(\"  ‚Ä¢ Simple but suboptimal\")\n",
    "print(\"  ‚Ä¢ Continues exploring even after convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified Q-Learning Agent with Epsilon Decay\n",
    "\n",
    "Now let's create an enhanced Q-Learning agent that uses epsilon decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgentWithDecay:\n",
    "    \"\"\"Q-Learning agent with epsilon decay schedule.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon_schedule=None):\n",
    "        \"\"\"\n",
    "        Initialize Q-Learning agent with epsilon decay.\n",
    "        \n",
    "        Args:\n",
    "            n_states: Number of states\n",
    "            n_actions: Number of actions\n",
    "            alpha: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon_schedule: EpsilonSchedule object (if None, uses fixed epsilon=0.1)\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_schedule = epsilon_schedule\n",
    "        self.epsilon = 0.1 if epsilon_schedule is None else epsilon_schedule.epsilon_start\n",
    "        \n",
    "        # Q-table: dictionary mapping states to action values\n",
    "        self.Q = {}\n",
    "    \n",
    "    def get_q_values(self, state):\n",
    "        \"\"\"Get Q-values for a state (initialize if not seen before).\"\"\"\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = np.zeros(self.n_actions)\n",
    "        return self.Q[state]\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using Œµ-greedy policy with current epsilon.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            q_values = self.get_q_values(state)\n",
    "            return np.argmax(q_values)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-value using Q-Learning rule.\"\"\"\n",
    "        q_values = self.get_q_values(state)\n",
    "        next_q_values = self.get_q_values(next_state)\n",
    "        \n",
    "        # Q-Learning TD target\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * np.max(next_q_values)\n",
    "        \n",
    "        # TD error\n",
    "        td_error = td_target - q_values[action]\n",
    "        \n",
    "        # Update Q-value\n",
    "        q_values[action] += self.alpha * td_error\n",
    "    \n",
    "    def update_epsilon(self, episode):\n",
    "        \"\"\"Update epsilon based on schedule.\"\"\"\n",
    "        if self.epsilon_schedule is not None:\n",
    "            self.epsilon = self.epsilon_schedule.get_epsilon(episode)\n",
    "\n",
    "\n",
    "print(\"Enhanced Q-Learning Agent with Epsilon Decay Implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Learning Performance with Different Decay Strategies\n",
    "\n",
    "Let's train multiple agents with different epsilon strategies and compare their learning performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_decay(env, agent, num_episodes=1000, max_steps=100):\n",
    "    \"\"\"Train Q-Learning agent with epsilon decay.\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilon_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Update epsilon for this episode\n",
    "        agent.update_epsilon(episode)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        # Run episode\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "    \n",
    "    return episode_rewards, episode_lengths, epsilon_history\n",
    "\n",
    "\n",
    "# Run experiments with different decay strategies\n",
    "print(\"Training Q-Learning Agents with Different Epsilon Strategies\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_episodes = 1000\n",
    "num_runs = 5  # Multiple runs for statistical significance\n",
    "\n",
    "# Define strategies to compare\n",
    "strategies = {\n",
    "    'Fixed Œµ=0.1': None,\n",
    "    'Linear Decay': LinearDecay(epsilon_start=1.0, epsilon_end=0.01, decay_episodes=800),\n",
    "    'Exponential Decay': ExponentialDecay(epsilon_start=1.0, epsilon_end=0.01, decay_rate=0.995),\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {name: {'rewards': [], 'lengths': []} for name in strategies.keys()}\n",
    "\n",
    "# Run experiments\n",
    "np.random.seed(42)\n",
    "for strategy_name, schedule in strategies.items():\n",
    "    print(f\"\\nTraining with {strategy_name}...\")\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        # Create fresh environment and agent\n",
    "        obstacles = [(1, 1), (2, 2), (3, 1)]\n",
    "        env = GridWorld(size=5, start=(0, 0), goal=(4, 4), obstacles=obstacles)\n",
    "        \n",
    "        # Reset schedule for each run\n",
    "        if schedule is not None:\n",
    "            schedule.reset()\n",
    "        \n",
    "        agent = QLearningAgentWithDecay(\n",
    "            n_states=env.size * env.size,\n",
    "            n_actions=len(env.actions),\n",
    "            alpha=0.1,\n",
    "            gamma=0.95,\n",
    "            epsilon_schedule=schedule\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        rewards, lengths, _ = train_with_decay(env, agent, num_episodes)\n",
    "        results[strategy_name]['rewards'].append(rewards)\n",
    "        results[strategy_name]['lengths'].append(lengths)\n",
    "    \n",
    "    print(f\"  Completed {num_runs} runs\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance across runs\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "colors = {'Fixed Œµ=0.1': 'gray', 'Linear Decay': 'blue', 'Exponential Decay': 'green'}\n",
    "window = 50  # Smoothing window\n",
    "\n",
    "# Plot 1: Average Rewards\n",
    "for strategy_name, data in results.items():\n",
    "    # Average across runs\n",
    "    avg_rewards = np.mean(data['rewards'], axis=0)\n",
    "    \n",
    "    # Smooth the curve\n",
    "    if len(avg_rewards) >= window:\n",
    "        smoothed = np.convolve(avg_rewards, np.ones(window)/window, mode='valid')\n",
    "        x = range(window-1, len(avg_rewards))\n",
    "    else:\n",
    "        smoothed = avg_rewards\n",
    "        x = range(len(avg_rewards))\n",
    "    \n",
    "    ax1.plot(x, smoothed, label=strategy_name, color=colors[strategy_name], \n",
    "             linewidth=2.5, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Average Reward', fontsize=12)\n",
    "ax1.set_title('Learning Performance: Rewards Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11, loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Average Episode Lengths\n",
    "for strategy_name, data in results.items():\n",
    "    # Average across runs\n",
    "    avg_lengths = np.mean(data['lengths'], axis=0)\n",
    "    \n",
    "    # Smooth the curve\n",
    "    if len(avg_lengths) >= window:\n",
    "        smoothed = np.convolve(avg_lengths, np.ones(window)/window, mode='valid')\n",
    "        x = range(window-1, len(avg_lengths))\n",
    "    else:\n",
    "        smoothed = avg_lengths\n",
    "        x = range(len(avg_lengths))\n",
    "    \n",
    "    ax2.plot(x, smoothed, label=strategy_name, color=colors[strategy_name], \n",
    "             linewidth=2.5, alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Average Episode Length (steps)', fontsize=12)\n",
    "ax2.set_title('Learning Performance: Episode Length Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11, loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nüìä Performance Summary (Final 100 Episodes):\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for strategy_name, data in results.items():\n",
    "    # Calculate statistics for last 100 episodes\n",
    "    final_rewards = [np.mean(run[-100:]) for run in data['rewards']]\n",
    "    final_lengths = [np.mean(run[-100:]) for run in data['lengths']]\n",
    "    \n",
    "    avg_reward = np.mean(final_rewards)\n",
    "    std_reward = np.std(final_rewards)\n",
    "    avg_length = np.mean(final_lengths)\n",
    "    std_length = np.std(final_lengths)\n",
    "    \n",
    "    print(f\"\\n{strategy_name}:\")\n",
    "    print(f\"  Average Reward: {avg_reward:6.2f} ¬± {std_reward:.2f}\")\n",
    "    print(f\"  Average Length: {avg_length:6.2f} ¬± {std_length:.2f} steps\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Epsilon decay strategies typically converge faster\")\n",
    "print(\"   ‚Ä¢ Final performance is often better with decay\")\n",
    "print(\"   ‚Ä¢ Exponential decay provides smooth transition\")\n",
    "print(\"   ‚Ä¢ Linear decay is more predictable and easier to tune\")\n",
    "print(\"   ‚Ä¢ Fixed epsilon continues exploring unnecessarily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Epsilon-Decreasing Exploration Strategy\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **The Problem with Fixed Epsilon**:\n",
    "   - Explores too much late in training (wastes time)\n",
    "   - Or explores too little early in training (misses good actions)\n",
    "   - Doesn't adapt to the learning progress\n",
    "\n",
    "2. **Epsilon Decay Strategies**:\n",
    "   - **Linear Decay**: Constant decrease rate, predictable\n",
    "   - **Exponential Decay**: Fast initial decrease, then slower (most common)\n",
    "   - **Step Decay**: Sudden drops at intervals\n",
    "\n",
    "3. **Benefits of Epsilon Decay**:\n",
    "   - Better exploration early in training\n",
    "   - Better exploitation late in training\n",
    "   - Faster convergence to optimal policy\n",
    "   - Higher final performance\n",
    "\n",
    "4. **Practical Considerations**:\n",
    "   - Start with high epsilon (0.9-1.0) for thorough exploration\n",
    "   - End with small epsilon (0.01-0.05) to maintain some exploration\n",
    "   - Tune decay rate based on problem complexity\n",
    "   - Exponential decay is a good default choice\n",
    "\n",
    "5. **When to Use Each Strategy**:\n",
    "   - **Linear**: When you know training duration and want predictable behavior\n",
    "   - **Exponential**: General-purpose, works well in most scenarios\n",
    "   - **Step**: For curriculum learning or staged training\n",
    "   - **Fixed**: Only for very simple problems or when exploration is critical\n",
    "\n",
    "**Implementation Tips:**\n",
    "\n",
    "```python\n",
    "# Good starting values for exponential decay\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "decay_rate = 0.995  # Reaches ~0.01 after ~900 episodes\n",
    "\n",
    "# For linear decay\n",
    "decay_episodes = 0.8 * total_episodes  # Decay over 80% of training\n",
    "```\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Epsilon decay is a fundamental technique used in:\n",
    "- Deep Q-Networks (DQN)\n",
    "- All epsilon-greedy based algorithms\n",
    "- Exploration strategies in general\n",
    "\n",
    "In the next sections, we'll see how this technique scales to deep reinforcement learning with neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dqn'></a>\n",
    "### Deep Q-Networks (DQN)\n",
    "\n",
    "**The Limitation of Tabular Q-Learning**\n",
    "\n",
    "So far, we've been using **tabular methods** - storing Q-values in a table (or dictionary) with one entry for each state-action pair. This works well for small problems like grid worlds, but it has severe limitations:\n",
    "\n",
    "**Problems with Tabular Methods:**\n",
    "\n",
    "1. **Memory Explosion**: \n",
    "   - A 100√ó100 grid with 4 actions needs 40,000 entries\n",
    "   - Atari games have ~10^9 possible screen states!\n",
    "   - Continuous state spaces (e.g., robot joint angles) have infinite states\n",
    "\n",
    "2. **No Generalization**:\n",
    "   - Each state is learned independently\n",
    "   - Similar states don't share knowledge\n",
    "   - Must visit every state many times to learn\n",
    "\n",
    "3. **Scalability**:\n",
    "   - Can't handle high-dimensional inputs (images, sensor data)\n",
    "   - Impractical for real-world problems\n",
    "\n",
    "**The Solution: Function Approximation**\n",
    "\n",
    "Instead of storing Q-values in a table, we use a **function approximator** to estimate them:\n",
    "\n",
    "$$Q(s, a) \\approx Q(s, a; \\theta)$$\n",
    "\n",
    "where $\\theta$ represents the parameters of our function approximator.\n",
    "\n",
    "**Why Neural Networks?**\n",
    "\n",
    "Neural networks are universal function approximators that can:\n",
    "\n",
    "1. **Handle High-Dimensional Inputs**: Process images, sensor data, etc.\n",
    "2. **Generalize**: Similar inputs produce similar outputs\n",
    "3. **Learn Features**: Automatically extract relevant features from raw data\n",
    "4. **Scale**: Work with millions of states using thousands of parameters\n",
    "\n",
    "**From Q-Table to Q-Network:**\n",
    "\n",
    "```\n",
    "Tabular Q-Learning:          Deep Q-Learning:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  State-Action   ‚îÇ          ‚îÇ     State       ‚îÇ\n",
    "‚îÇ     Table       ‚îÇ          ‚îÇ   (e.g., image) ‚îÇ\n",
    "‚îÇ                 ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îÇ  s‚ÇÅ,a‚ÇÅ ‚Üí 0.5    ‚îÇ                  ‚îÇ\n",
    "‚îÇ  s‚ÇÅ,a‚ÇÇ ‚Üí 0.3    ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  s‚ÇÇ,a‚ÇÅ ‚Üí 0.8    ‚îÇ          ‚îÇ Neural Network ‚îÇ\n",
    "‚îÇ  ...            ‚îÇ          ‚îÇ   Q(s; Œ∏)      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                     ‚îÇ\n",
    "                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                             ‚îÇ  Q-values for  ‚îÇ\n",
    "                             ‚îÇ  all actions   ‚îÇ\n",
    "                             ‚îÇ [Q(s,a‚ÇÅ), ...] ‚îÇ\n",
    "                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Key Insight:**\n",
    "\n",
    "Instead of looking up Q(s,a) in a table, we:\n",
    "1. Feed the state into a neural network\n",
    "2. The network outputs Q-values for all actions\n",
    "3. Select the action with the highest Q-value\n",
    "\n",
    "This allows us to:\n",
    "- Handle complex, high-dimensional states\n",
    "- Generalize to unseen states\n",
    "- Learn from raw sensory inputs (pixels, audio, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN Architecture\n",
    "\n",
    "**What is a Deep Q-Network?**\n",
    "\n",
    "A Deep Q-Network (DQN) is a neural network that approximates the Q-function. It was introduced by DeepMind in 2015 and achieved human-level performance on Atari games by learning directly from pixel inputs.\n",
    "\n",
    "**Network Architecture:**\n",
    "\n",
    "The basic DQN architecture consists of:\n",
    "\n",
    "1. **Input Layer**: Receives the state representation\n",
    "   - For images: Raw pixels or preprocessed frames\n",
    "   - For vectors: State features (position, velocity, etc.)\n",
    "\n",
    "2. **Hidden Layers**: Extract features and learn representations\n",
    "   - Fully connected layers for vector inputs\n",
    "   - Convolutional layers for image inputs\n",
    "   - Activation functions (ReLU is common)\n",
    "\n",
    "3. **Output Layer**: Produces Q-values for each action\n",
    "   - One output neuron per action\n",
    "   - No activation (linear output)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Given a state $s$, the Q-network computes:\n",
    "\n",
    "$$Q(s, a; \\theta) = f_{\\theta}(s)_a$$\n",
    "\n",
    "where:\n",
    "- $f_{\\theta}$ is the neural network with parameters $\\theta$\n",
    "- $f_{\\theta}(s)$ outputs a vector of Q-values, one for each action\n",
    "- $f_{\\theta}(s)_a$ is the Q-value for action $a$\n",
    "\n",
    "**Training Objective:**\n",
    "\n",
    "We train the network to minimize the **temporal difference error**:\n",
    "\n",
    "$$L(\\theta) = \\mathbb{E}\\left[(r + \\gamma \\max_{a'} Q(s', a'; \\theta) - Q(s, a; \\theta))^2\\right]$$\n",
    "\n",
    "This is the same TD error from Q-learning, but now we're updating network parameters $\\theta$ instead of table entries!\n",
    "\n",
    "**Architecture Variants:**\n",
    "\n",
    "1. **Simple DQN** (for low-dimensional states):\n",
    "   ```\n",
    "   State ‚Üí FC(64) ‚Üí ReLU ‚Üí FC(64) ‚Üí ReLU ‚Üí FC(n_actions) ‚Üí Q-values\n",
    "   ```\n",
    "\n",
    "2. **Convolutional DQN** (for image inputs):\n",
    "   ```\n",
    "   Image ‚Üí Conv(32,8√ó8,s=4) ‚Üí ReLU ‚Üí Conv(64,4√ó4,s=2) ‚Üí ReLU ‚Üí \n",
    "         ‚Üí Conv(64,3√ó3,s=1) ‚Üí ReLU ‚Üí FC(512) ‚Üí ReLU ‚Üí FC(n_actions) ‚Üí Q-values\n",
    "   ```\n",
    "\n",
    "3. **Dueling DQN** (separates value and advantage):\n",
    "   ```\n",
    "   State ‚Üí Shared Layers ‚Üí ‚î¨‚Üí Value Stream ‚Üí V(s)\n",
    "                           ‚îî‚Üí Advantage Stream ‚Üí A(s,a)\n",
    "   Q(s,a) = V(s) + (A(s,a) - mean(A(s,:)))\n",
    "   ```\n",
    "\n",
    "**Key Design Choices:**\n",
    "\n",
    "1. **Network Size**: \n",
    "   - Larger networks can represent more complex functions\n",
    "   - But require more data and computation\n",
    "   - Start small and increase if needed\n",
    "\n",
    "2. **Activation Functions**:\n",
    "   - ReLU is standard for hidden layers\n",
    "   - No activation on output (Q-values can be any real number)\n",
    "\n",
    "3. **Output Structure**:\n",
    "   - Output Q-values for ALL actions simultaneously\n",
    "   - More efficient than separate networks per action\n",
    "   - Allows easy action selection: argmax over outputs\n",
    "\n",
    "Let's implement a simple Q-network in PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Q-Network in PyTorch\n",
    "\n",
    "We'll create a flexible Q-network class that can handle different state dimensions and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network for approximating Q-values.\n",
    "    \n",
    "    This network takes a state as input and outputs Q-values for all actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[64, 64]):\n",
    "        \"\"\"\n",
    "        Initialize the Q-Network.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of the state space (input size)\n",
    "            action_dim: Number of possible actions (output size)\n",
    "            hidden_dims: List of hidden layer sizes\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Build the network layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (no activation - Q-values can be any real number)\n",
    "        layers.append(nn.Linear(prev_dim, action_dim))\n",
    "        \n",
    "        # Combine all layers into a sequential model\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights for better training.\"\"\"\n",
    "        for module in self.network:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0.0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            state: State tensor of shape (batch_size, state_dim) or (state_dim,)\n",
    "        \n",
    "        Returns:\n",
    "            Q-values for all actions, shape (batch_size, action_dim) or (action_dim,)\n",
    "        \"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (numpy array or tensor)\n",
    "            epsilon: Exploration rate (0 = greedy, 1 = random)\n",
    "        \n",
    "        Returns:\n",
    "            Selected action (integer)\n",
    "        \"\"\"\n",
    "        # Exploration: random action\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        # Exploitation: greedy action\n",
    "        with torch.no_grad():\n",
    "            # Convert state to tensor if needed\n",
    "            if isinstance(state, np.ndarray):\n",
    "                state = torch.FloatTensor(state)\n",
    "            \n",
    "            # Get Q-values and select best action\n",
    "            q_values = self.forward(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def get_q_values(self, state):\n",
    "        \"\"\"\n",
    "        Get Q-values for a state.\n",
    "        \n",
    "        Args:\n",
    "            state: State tensor or numpy array\n",
    "        \n",
    "        Returns:\n",
    "            Q-values as numpy array\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if isinstance(state, np.ndarray):\n",
    "                state = torch.FloatTensor(state)\n",
    "            q_values = self.forward(state)\n",
    "            return q_values.numpy()\n",
    "\n",
    "\n",
    "print(\"Q-Network Implementation Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"  ‚Ä¢ Flexible architecture with configurable hidden layers\")\n",
    "print(\"  ‚Ä¢ Xavier weight initialization for stable training\")\n",
    "print(\"  ‚Ä¢ Epsilon-greedy action selection built-in\")\n",
    "print(\"  ‚Ä¢ Handles both single states and batches\")\n",
    "print(\"  ‚Ä¢ PyTorch implementation for GPU acceleration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrating the Q-Network with Sample States\n",
    "\n",
    "Let's create a Q-network and see how it processes states and produces Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple Q-Network for CartPole-like environment\n",
    "print(\"Example 1: Q-Network for CartPole Environment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CartPole has 4-dimensional state and 2 actions\n",
    "state_dim = 4  # [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "action_dim = 2  # [push left, push right]\n",
    "\n",
    "# Create Q-network\n",
    "q_net = QNetwork(state_dim=state_dim, action_dim=action_dim, hidden_dims=[64, 64])\n",
    "\n",
    "print(f\"\\nNetwork Architecture:\")\n",
    "print(q_net)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in q_net.parameters())\n",
    "trainable_params = sum(p.numel() for p in q_net.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Create a sample state\n",
    "sample_state = np.array([0.02, 0.5, -0.1, 0.3])  # Example CartPole state\n",
    "print(f\"\\nSample State: {sample_state}\")\n",
    "\n",
    "# Forward pass: get Q-values\n",
    "q_values = q_net.get_q_values(sample_state)\n",
    "print(f\"\\nQ-values:\")\n",
    "print(f\"  Q(s, left):  {q_values[0]:.4f}\")\n",
    "print(f\"  Q(s, right): {q_values[1]:.4f}\")\n",
    "\n",
    "# Select action (greedy)\n",
    "action = q_net.get_action(sample_state, epsilon=0.0)\n",
    "action_name = \"LEFT\" if action == 0 else \"RIGHT\"\n",
    "print(f\"\\nGreedy Action: {action} ({action_name})\")\n",
    "\n",
    "# Select action with exploration\n",
    "print(f\"\\nWith Œµ=0.1 exploration:\")\n",
    "actions = [q_net.get_action(sample_state, epsilon=0.1) for _ in range(10)]\n",
    "print(f\"  10 action samples: {actions}\")\n",
    "print(f\"  Greedy action selected: {actions.count(action)}/10 times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Batch Processing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 2: Batch Processing Multiple States\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a batch of states\n",
    "batch_size = 5\n",
    "batch_states = np.random.randn(batch_size, state_dim)\n",
    "\n",
    "print(f\"\\nBatch of {batch_size} states:\")\n",
    "print(batch_states)\n",
    "\n",
    "# Forward pass with batch\n",
    "batch_states_tensor = torch.FloatTensor(batch_states)\n",
    "batch_q_values = q_net(batch_states_tensor)\n",
    "\n",
    "print(f\"\\nBatch Q-values (shape: {batch_q_values.shape}):\")\n",
    "print(batch_q_values.detach().numpy())\n",
    "\n",
    "# Select best action for each state in batch\n",
    "best_actions = torch.argmax(batch_q_values, dim=1)\n",
    "print(f\"\\nBest actions for each state: {best_actions.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Different Network Architectures\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 3: Comparing Different Network Architectures\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "architectures = {\n",
    "    'Small': [32],\n",
    "    'Medium': [64, 64],\n",
    "    'Large': [128, 128, 64],\n",
    "    'Deep': [64, 64, 64, 64]\n",
    "}\n",
    "\n",
    "print(f\"\\nState dim: {state_dim}, Action dim: {action_dim}\\n\")\n",
    "\n",
    "for name, hidden_dims in architectures.items():\n",
    "    net = QNetwork(state_dim=state_dim, action_dim=action_dim, hidden_dims=hidden_dims)\n",
    "    params = sum(p.numel() for p in net.parameters())\n",
    "    \n",
    "    print(f\"{name:10s} {str(hidden_dims):20s} ‚Üí {params:,} parameters\")\n",
    "\n",
    "print(\"\\nüí° Architecture Selection Tips:\")\n",
    "print(\"   ‚Ä¢ Start with medium-sized networks (64-128 units)\")\n",
    "print(\"   ‚Ä¢ Increase size if underfitting (poor performance)\")\n",
    "print(\"   ‚Ä¢ Decrease size if overfitting or slow training\")\n",
    "print(\"   ‚Ä¢ Deeper networks can learn more complex patterns\")\n",
    "print(\"   ‚Ä¢ But require more data and careful tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Visualizing Q-values for Different States\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 4: Visualizing Q-values Across State Space\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple 2D state space for visualization\n",
    "simple_q_net = QNetwork(state_dim=2, action_dim=4, hidden_dims=[32, 32])\n",
    "\n",
    "# Generate a grid of states\n",
    "x = np.linspace(-2, 2, 20)\n",
    "y = np.linspace(-2, 2, 20)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Compute Q-values for each state in the grid\n",
    "q_values_grid = np.zeros((20, 20, 4))\n",
    "\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        state = np.array([X[i, j], Y[i, j]])\n",
    "        q_values_grid[i, j] = simple_q_net.get_q_values(state)\n",
    "\n",
    "# Plot Q-values for each action\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "action_names = ['Action 0', 'Action 1', 'Action 2', 'Action 3']\n",
    "\n",
    "for idx, (ax, action_name) in enumerate(zip(axes.flat, action_names)):\n",
    "    im = ax.contourf(X, Y, q_values_grid[:, :, idx], levels=20, cmap='RdYlGn')\n",
    "    ax.set_xlabel('State Dimension 1', fontsize=11)\n",
    "    ax.set_ylabel('State Dimension 2', fontsize=11)\n",
    "    ax.set_title(f'Q-values for {action_name}', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, label='Q-value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Q-Network Output Across 2D State Space\\n(Untrained Network - Random Initialization)', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"   ‚Ä¢ Each subplot shows Q-values for one action\")\n",
    "print(\"   ‚Ä¢ Colors indicate Q-value magnitude (green=high, red=low)\")\n",
    "print(\"   ‚Ä¢ This is an UNTRAINED network (random weights)\")\n",
    "print(\"   ‚Ä¢ After training, Q-values would reflect learned policy\")\n",
    "print(\"   ‚Ä¢ The network can generalize to unseen states!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Neural Network Q-Function\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **Function Approximation**:\n",
    "   - Replaces Q-tables with neural networks\n",
    "   - Enables handling of large/continuous state spaces\n",
    "   - Provides generalization to unseen states\n",
    "\n",
    "2. **Q-Network Architecture**:\n",
    "   - Input: State representation\n",
    "   - Hidden layers: Feature extraction with ReLU activations\n",
    "   - Output: Q-values for all actions (no activation)\n",
    "\n",
    "3. **Key Design Decisions**:\n",
    "   - Network size: Balance capacity vs. sample efficiency\n",
    "   - Depth: Deeper networks for complex patterns\n",
    "   - Initialization: Xavier/He initialization for stable training\n",
    "\n",
    "4. **Advantages Over Tabular Methods**:\n",
    "   - **Scalability**: Handle millions of states with thousands of parameters\n",
    "   - **Generalization**: Similar states produce similar Q-values\n",
    "   - **Flexibility**: Can process raw sensory inputs (images, audio)\n",
    "   - **Efficiency**: Share knowledge across similar states\n",
    "\n",
    "5. **Implementation Details**:\n",
    "   - PyTorch provides automatic differentiation for training\n",
    "   - Batch processing for efficient computation\n",
    "   - Epsilon-greedy action selection integrated\n",
    "   - GPU acceleration available\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "We now have a Q-network that can approximate Q-values, but we haven't trained it yet! In the next sections, we'll learn about:\n",
    "\n",
    "- **Experience Replay**: Storing and reusing past experiences for stable training\n",
    "- **Target Networks**: Preventing moving target problems during training\n",
    "- **DQN Training**: Putting it all together to learn from experience\n",
    "- **Double DQN**: Reducing overestimation bias\n",
    "\n",
    "These techniques are crucial for making deep Q-learning work in practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay\n",
    "\n",
    "**The Problem with Online Learning**\n",
    "\n",
    "When training a Q-network, a naive approach would be to update the network immediately after each interaction with the environment:\n",
    "\n",
    "1. Observe state $s_t$\n",
    "2. Take action $a_t$\n",
    "3. Receive reward $r_t$ and next state $s_{t+1}$\n",
    "4. Immediately update the network using this single transition\n",
    "5. Discard the transition and move on\n",
    "\n",
    "**Why This Fails:**\n",
    "\n",
    "This online learning approach has several critical problems:\n",
    "\n",
    "1. **Correlation Between Consecutive Samples**:\n",
    "   - Sequential experiences are highly correlated\n",
    "   - The agent visits similar states in succession\n",
    "   - Neural networks assume i.i.d. (independent and identically distributed) data\n",
    "   - Correlated samples lead to poor convergence and overfitting\n",
    "\n",
    "2. **Sample Inefficiency**:\n",
    "   - Each experience is used only once for learning\n",
    "   - Gathering experiences can be expensive (especially in real-world scenarios)\n",
    "   - We're throwing away valuable data!\n",
    "\n",
    "3. **Catastrophic Forgetting**:\n",
    "   - The network quickly forgets what it learned about earlier states\n",
    "   - As the agent explores new regions, it \"overwrites\" knowledge about previous regions\n",
    "   - This leads to unstable and oscillating behavior\n",
    "\n",
    "**The Solution: Experience Replay**\n",
    "\n",
    "Experience replay, introduced in the original DQN paper (Mnih et al., 2015), solves these problems elegantly:\n",
    "\n",
    "**Key Idea**: Store past experiences in a **replay buffer** (memory) and randomly sample mini-batches for training.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "1. **Store**: Save each transition $(s_t, a_t, r_t, s_{t+1}, done_t)$ in a replay buffer\n",
    "2. **Sample**: Randomly sample a mini-batch of transitions from the buffer\n",
    "3. **Learn**: Update the network using the sampled batch\n",
    "4. **Repeat**: Continue storing new experiences and sampling for training\n",
    "\n",
    "**Why Experience Replay Works:**\n",
    "\n",
    "1. **Breaks Correlation**:\n",
    "   - Random sampling creates i.i.d. training batches\n",
    "   - Transitions from different episodes and time steps are mixed\n",
    "   - Network sees diverse experiences in each update\n",
    "\n",
    "2. **Improves Sample Efficiency**:\n",
    "   - Each experience can be used multiple times\n",
    "   - Rare or important experiences aren't immediately forgotten\n",
    "   - Better utilization of collected data\n",
    "\n",
    "3. **Stabilizes Learning**:\n",
    "   - Smooths out the learning process\n",
    "   - Reduces variance in updates\n",
    "   - Prevents catastrophic forgetting\n",
    "\n",
    "4. **Enables Off-Policy Learning**:\n",
    "   - Can learn from experiences generated by old policies\n",
    "   - Decouples data collection from learning\n",
    "   - More flexible training strategies\n",
    "\n",
    "**Mathematical Perspective:**\n",
    "\n",
    "The Q-learning update with experience replay:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\text{Sample mini-batch: } & \\{(s_i, a_i, r_i, s'_i, done_i)\\}_{i=1}^{N} \\sim \\mathcal{D} \\\\\n",
    "\\text{Target: } & y_i = r_i + \\gamma (1 - done_i) \\max_{a'} Q(s'_i, a'; \\theta^-) \\\\\n",
    "\\text{Loss: } & \\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i - Q(s_i, a_i; \\theta)\\right)^2\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{D}$ is the replay buffer\n",
    "- $N$ is the mini-batch size\n",
    "- $\\theta$ are the current network parameters\n",
    "- $\\theta^-$ are the target network parameters (we'll cover this next)\n",
    "\n",
    "**Practical Considerations:**\n",
    "\n",
    "- **Buffer Size**: Typically 10,000 to 1,000,000 transitions\n",
    "  - Larger buffers provide more diversity but use more memory\n",
    "  - Should be large enough to store experiences from many episodes\n",
    "\n",
    "- **Batch Size**: Usually 32 to 256 transitions\n",
    "  - Larger batches provide more stable gradients\n",
    "  - Smaller batches train faster but with more variance\n",
    "\n",
    "- **Sampling Strategy**: Uniform random sampling is most common\n",
    "  - Advanced: Prioritized Experience Replay samples important transitions more often\n",
    "\n",
    "- **When to Start Training**: Wait until buffer has enough samples\n",
    "  - Typically start training after 1,000-10,000 initial experiences\n",
    "\n",
    "Let's implement a replay buffer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer for storing and sampling transitions.\n",
    "    \n",
    "    The replay buffer stores transitions (s, a, r, s', done) and provides\n",
    "    random sampling for training. This breaks correlation between consecutive\n",
    "    samples and improves sample efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        \"\"\"Initialize the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            capacity: Maximum number of transitions to store\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)  # Automatically removes old experiences\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition in the buffer.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state\n",
    "            done: Whether episode ended\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random batch of transitions.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of transitions to sample\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, rewards, next_states, dones)\n",
    "            Each element is a numpy array or list\n",
    "        \"\"\"\n",
    "        # Randomly sample batch_size transitions\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Unzip the batch into separate arrays\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert to numpy arrays for easier processing\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of the buffer.\"\"\"\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def is_ready(self, batch_size):\n",
    "        \"\"\"Check if buffer has enough samples for training.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Required batch size\n",
    "            \n",
    "        Returns:\n",
    "            True if buffer has at least batch_size samples\n",
    "        \"\"\"\n",
    "        return len(self.buffer) >= batch_size\n",
    "\n",
    "\n",
    "print(\"ReplayBuffer class implemented successfully!\")\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"  ‚Ä¢ Stores transitions (s, a, r, s', done)\")\n",
    "print(\"  ‚Ä¢ Automatic capacity management with deque\")\n",
    "print(\"  ‚Ä¢ Random sampling for breaking correlations\")\n",
    "print(\"  ‚Ä¢ Efficient batch preparation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrating the Replay Buffer\n",
    "\n",
    "Let's see how the replay buffer works in practice with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic Usage\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Basic Replay Buffer Operations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a replay buffer with small capacity for demonstration\n",
    "replay_buffer = ReplayBuffer(capacity=5)\n",
    "\n",
    "print(f\"\\nInitial buffer size: {len(replay_buffer)}\")\n",
    "print(f\"Buffer capacity: {replay_buffer.capacity}\")\n",
    "\n",
    "# Add some transitions\n",
    "print(\"\\nAdding transitions to buffer...\")\n",
    "for i in range(7):\n",
    "    state = np.array([i, i*2])\n",
    "    action = i % 4\n",
    "    reward = i * 0.1\n",
    "    next_state = np.array([i+1, (i+1)*2])\n",
    "    done = (i == 6)\n",
    "    \n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    print(f\"  Step {i}: Added transition | Buffer size: {len(replay_buffer)}\")\n",
    "\n",
    "print(f\"\\nüí° Notice: Buffer size capped at {replay_buffer.capacity}\")\n",
    "print(\"   Oldest transitions are automatically removed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Sampling from the Buffer\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 2: Sampling Mini-Batches\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a larger buffer and fill it\n",
    "replay_buffer = ReplayBuffer(capacity=100)\n",
    "\n",
    "# Simulate collecting experiences\n",
    "print(\"\\nCollecting 50 experiences...\")\n",
    "for i in range(50):\n",
    "    state = np.random.randn(4)  # Random 4D state\n",
    "    action = np.random.randint(0, 3)  # Random action from {0, 1, 2}\n",
    "    reward = np.random.randn()  # Random reward\n",
    "    next_state = np.random.randn(4)\n",
    "    done = (i % 10 == 9)  # Episode ends every 10 steps\n",
    "    \n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"Buffer size: {len(replay_buffer)}\")\n",
    "\n",
    "# Sample a mini-batch\n",
    "batch_size = 8\n",
    "print(f\"\\nSampling mini-batch of size {batch_size}...\")\n",
    "\n",
    "states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "print(f\"\\nBatch contents:\")\n",
    "print(f\"  States shape: {states.shape}\")\n",
    "print(f\"  Actions shape: {actions.shape}\")\n",
    "print(f\"  Rewards shape: {rewards.shape}\")\n",
    "print(f\"  Next states shape: {next_states.shape}\")\n",
    "print(f\"  Dones shape: {dones.shape}\")\n",
    "\n",
    "print(f\"\\nFirst 3 transitions in batch:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n  Transition {i}:\")\n",
    "    print(f\"    State: {states[i]}\")\n",
    "    print(f\"    Action: {actions[i]}\")\n",
    "    print(f\"    Reward: {rewards[i]:.3f}\")\n",
    "    print(f\"    Done: {bool(dones[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Demonstrating Correlation Breaking\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 3: Breaking Temporal Correlation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create buffer and add sequential experiences\n",
    "replay_buffer = ReplayBuffer(capacity=100)\n",
    "\n",
    "# Simulate an agent moving through a 1D environment\n",
    "print(\"\\nSimulating sequential experiences (agent moving right):\")\n",
    "for position in range(50):\n",
    "    state = np.array([position])\n",
    "    action = 1  # Always move right\n",
    "    reward = 0.1\n",
    "    next_state = np.array([position + 1])\n",
    "    done = False\n",
    "    \n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"Added {len(replay_buffer)} sequential transitions (positions 0-49)\")\n",
    "\n",
    "# Sample and show that samples are NOT sequential\n",
    "print(\"\\nSampling 10 transitions:\")\n",
    "states, actions, rewards, next_states, dones = replay_buffer.sample(10)\n",
    "\n",
    "positions = states.flatten()\n",
    "print(f\"\\nSampled positions: {positions.astype(int)}\")\n",
    "print(f\"\\n‚úì Notice: Positions are NOT consecutive!\")\n",
    "print(f\"  This breaks the temporal correlation.\")\n",
    "print(f\"  The network sees diverse experiences in each batch.\")\n",
    "\n",
    "# Show correlation in sequential vs sampled\n",
    "sequential_positions = np.arange(10)\n",
    "sampled_positions = np.sort(positions[:10].astype(int))\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Sequential (online learning): {sequential_positions}\")\n",
    "print(f\"  Sampled (replay buffer):      {sampled_positions}\")\n",
    "print(f\"\\n  Sequential correlation: HIGH (consecutive states)\")\n",
    "print(f\"  Sampled correlation:    LOW (random states)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Sample Efficiency Demonstration\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 4: Sample Efficiency with Experience Replay\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate training with and without replay\n",
    "num_experiences = 1000\n",
    "batch_size = 32\n",
    "training_steps = 100\n",
    "\n",
    "print(f\"\\nScenario: Collected {num_experiences} experiences\")\n",
    "print(f\"Training for {training_steps} steps with batch size {batch_size}\")\n",
    "\n",
    "# Without replay: each experience used once\n",
    "experiences_used_no_replay = num_experiences\n",
    "\n",
    "# With replay: each training step uses batch_size samples\n",
    "experiences_used_with_replay = training_steps * batch_size\n",
    "\n",
    "print(f\"\\nüìä Sample Usage:\")\n",
    "print(f\"\\n  WITHOUT Experience Replay:\")\n",
    "print(f\"    ‚Ä¢ Each experience used: 1 time\")\n",
    "print(f\"    ‚Ä¢ Total experience usage: {experiences_used_no_replay}\")\n",
    "print(f\"    ‚Ä¢ Training updates: {num_experiences}\")\n",
    "\n",
    "print(f\"\\n  WITH Experience Replay:\")\n",
    "print(f\"    ‚Ä¢ Each experience used: ~{experiences_used_with_replay / num_experiences:.1f} times (on average)\")\n",
    "print(f\"    ‚Ä¢ Total experience usage: {experiences_used_with_replay}\")\n",
    "print(f\"    ‚Ä¢ Training updates: {training_steps}\")\n",
    "\n",
    "efficiency_gain = experiences_used_with_replay / experiences_used_no_replay\n",
    "print(f\"\\n  ‚úì Sample Efficiency Gain: {efficiency_gain:.1f}x\")\n",
    "print(f\"    We get {efficiency_gain:.1f}x more learning from the same data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Visualizing Buffer Dynamics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 5: Replay Buffer Dynamics Over Time\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate buffer filling and sampling over time\n",
    "buffer_capacity = 100\n",
    "replay_buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "buffer_sizes = []\n",
    "experiences_collected = []\n",
    "\n",
    "# Collect experiences over time\n",
    "for step in range(200):\n",
    "    # Add new experience\n",
    "    state = np.random.randn(4)\n",
    "    action = np.random.randint(0, 3)\n",
    "    reward = np.random.randn()\n",
    "    next_state = np.random.randn(4)\n",
    "    done = False\n",
    "    \n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    buffer_sizes.append(len(replay_buffer))\n",
    "    experiences_collected.append(step + 1)\n",
    "\n",
    "# Plot buffer size over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(experiences_collected, buffer_sizes, linewidth=2, color='blue', label='Buffer Size')\n",
    "plt.axhline(y=buffer_capacity, color='red', linestyle='--', linewidth=2, label=f'Capacity ({buffer_capacity})')\n",
    "plt.fill_between(experiences_collected, 0, buffer_sizes, alpha=0.3, color='blue')\n",
    "\n",
    "plt.xlabel('Experiences Collected', fontsize=12)\n",
    "plt.ylabel('Buffer Size', fontsize=12)\n",
    "plt.title('Replay Buffer Size Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"   ‚Ä¢ Buffer grows linearly until reaching capacity\")\n",
    "print(\"   ‚Ä¢ After capacity is reached, oldest experiences are removed\")\n",
    "print(\"   ‚Ä¢ This maintains a 'sliding window' of recent experiences\")\n",
    "print(\"   ‚Ä¢ Ensures buffer contains relevant, up-to-date experiences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Experience Replay\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **The Problem**:\n",
    "   - Online learning suffers from correlated samples\n",
    "   - Sample inefficiency (each experience used once)\n",
    "   - Catastrophic forgetting of earlier experiences\n",
    "\n",
    "2. **The Solution**:\n",
    "   - Store transitions in a replay buffer\n",
    "   - Randomly sample mini-batches for training\n",
    "   - Reuse experiences multiple times\n",
    "\n",
    "3. **Key Benefits**:\n",
    "   - **Breaks Correlation**: Random sampling creates i.i.d. batches\n",
    "   - **Sample Efficiency**: Each experience used multiple times\n",
    "   - **Stability**: Smooths learning, reduces variance\n",
    "   - **Off-Policy**: Can learn from old experiences\n",
    "\n",
    "4. **Implementation Details**:\n",
    "   - Use `deque` with `maxlen` for automatic capacity management\n",
    "   - Store complete transitions: $(s, a, r, s', done)$\n",
    "   - Random sampling with `random.sample()`\n",
    "   - Batch preparation for efficient training\n",
    "\n",
    "5. **Practical Guidelines**:\n",
    "   - Buffer size: 10,000 - 1,000,000 transitions\n",
    "   - Batch size: 32 - 256 transitions\n",
    "   - Start training after buffer has enough samples\n",
    "   - Larger buffers = more diversity, more memory\n",
    "\n",
    "**Impact on Deep RL:**\n",
    "\n",
    "Experience replay was a crucial innovation that made DQN work. Without it:\n",
    "- Training is unstable and often diverges\n",
    "- Sample efficiency is poor\n",
    "- Performance is significantly worse\n",
    "\n",
    "With experience replay:\n",
    "- DQN can learn from pixels to play Atari games\n",
    "- Training is stable and reliable\n",
    "- Sample efficiency is greatly improved\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Now that we have both a Q-network and experience replay, we need one more ingredient for stable DQN training: **target networks**. We'll cover this next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Networks for Stable Training\n",
    "\n",
    "**The Moving Target Problem**\n",
    "\n",
    "When training a Q-network, we face a fundamental instability issue. Recall the Q-learning update:\n",
    "\n",
    "$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$\n",
    "\n",
    "The problem: We're using the **same network** to:\n",
    "1. Estimate the current Q-value: $Q(s, a)$\n",
    "2. Estimate the target Q-value: $r + \\gamma \\max_{a'} Q(s', a')$\n",
    "\n",
    "This creates a **moving target problem**:\n",
    "- Every time we update the network, we change both the prediction AND the target\n",
    "- It's like trying to hit a target that moves every time you adjust your aim\n",
    "- This leads to oscillations, divergence, and unstable training\n",
    "\n",
    "**The Target Network Solution**\n",
    "\n",
    "The solution is to use **two separate networks**:\n",
    "\n",
    "1. **Online Network** (parameters $\\theta$): Updated every step, used to select actions\n",
    "2. **Target Network** (parameters $\\theta^-$): Updated infrequently, used to compute targets\n",
    "\n",
    "The modified update becomes:\n",
    "\n",
    "$\n",
    "\\text{Loss} = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\left[ \\left( r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a') - Q_\\theta(s, a) \\right)^2 \\right]\n",
    "$\n",
    "\n",
    "where:\n",
    "- $Q_\\theta$ is the online network (being trained)\n",
    "- $Q_{\\theta^-}$ is the target network (held fixed)\n",
    "- $\\mathcal{D}$ is the replay buffer\n",
    "\n",
    "**How Target Networks Work:**\n",
    "\n",
    "1. Initialize both networks with the same weights: $\\theta^- = \\theta$\n",
    "2. For many steps:\n",
    "   - Use online network to select actions and compute current Q-values\n",
    "   - Use target network to compute target Q-values\n",
    "   - Update only the online network\n",
    "3. Periodically (e.g., every 1000 steps): $\\theta^- \\leftarrow \\theta$\n",
    "\n",
    "**Why This Works:**\n",
    "\n",
    "- The target network provides **stable targets** for many updates\n",
    "- The online network can learn without chasing a moving target\n",
    "- Periodic updates ensure the target network eventually catches up\n",
    "- This dramatically improves training stability\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "\n",
    "- **Update Frequency**: How often to copy weights (e.g., every 1000-10000 steps)\n",
    "- **Soft Updates** (alternative): $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau) \\theta^-$ with small $\\tau$ (e.g., 0.001)\n",
    "\n",
    "Let's implement a complete DQN agent with target networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Network agent with experience replay and target network.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, \n",
    "                 lr=1e-3, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, \n",
    "                 epsilon_decay=0.995, buffer_size=10000, batch_size=64,\n",
    "                 target_update_freq=1000):\n",
    "        \"\"\"\n",
    "        Initialize DQN agent.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Number of possible actions\n",
    "            hidden_dim: Size of hidden layers\n",
    "            lr: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon_start: Initial exploration rate\n",
    "            epsilon_end: Minimum exploration rate\n",
    "            epsilon_decay: Decay rate for epsilon\n",
    "            buffer_size: Size of replay buffer\n",
    "            batch_size: Mini-batch size for training\n",
    "            target_update_freq: Steps between target network updates\n",
    "        \"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Create online and target networks\n",
    "        self.online_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        \n",
    "        # Initialize target network with same weights as online network\n",
    "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "        self.target_network.eval()  # Target network is always in eval mode\n",
    "        \n",
    "        # Optimizer for online network\n",
    "        self.optimizer = optim.Adam(self.online_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            training: If True, use epsilon-greedy; if False, use greedy\n",
    "            \n",
    "        Returns:\n",
    "            action: Selected action\n",
    "        \"\"\"\n",
    "        # Exploration: random action\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        # Exploitation: best action according to online network\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.online_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition in the replay buffer.\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Perform one update step using a mini-batch from replay buffer.\n",
    "        \n",
    "        Returns:\n",
    "            loss: TD loss value (or None if buffer too small)\n",
    "        \"\"\"\n",
    "        # Don't update if buffer doesn't have enough samples\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample mini-batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Compute current Q-values using online network\n",
    "        current_q_values = self.online_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values using target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize the online network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_network.parameters(), max_norm=10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from online network to target network.\"\"\"\n",
    "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "        print(f\"Target network updated at step {self.steps}\")\n",
    "\n",
    "\n",
    "print(\"DQN Agent with Target Network implemented!\")\n",
    "print(\"\\nKey components:\")\n",
    "print(\"  ‚úì Online network for action selection and training\")\n",
    "print(\"  ‚úì Target network for stable Q-value targets\")\n",
    "print(\"  ‚úì Experience replay for breaking correlations\")\n",
    "print(\"  ‚úì Epsilon-greedy exploration\")\n",
    "print(\"  ‚úì Periodic target network updates\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training DQN on CartPole\n",
    "\n",
    "Now let's train our DQN agent on the CartPole environment! CartPole is a classic control problem where the goal is to balance a pole on a moving cart.\n",
    "\n",
    "**CartPole Environment:**\n",
    "- **State**: 4 continuous values (cart position, cart velocity, pole angle, pole angular velocity)\n",
    "- **Actions**: 2 discrete actions (push left or push right)\n",
    "- **Reward**: +1 for every timestep the pole stays upright\n",
    "- **Episode ends**: When pole falls too far or cart moves off screen\n",
    "- **Success**: Average reward of 195+ over 100 episodes\n",
    "\n",
    "Let's implement the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_dqn(env_name='CartPole-v1', num_episodes=500, max_steps=500, \n",
    "              print_every=50, render=False):\n",
    "    \"\"\"\n",
    "    Train a DQN agent on a Gym environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Name of Gym environment\n",
    "        num_episodes: Number of episodes to train\n",
    "        max_steps: Maximum steps per episode\n",
    "        print_every: Print progress every N episodes\n",
    "        render: Whether to render the environment\n",
    "        \n",
    "    Returns:\n",
    "        agent: Trained DQN agent\n",
    "        episode_rewards: List of total rewards per episode\n",
    "        episode_lengths: List of episode lengths\n",
    "        losses: List of training losses\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    print(f\"Training DQN on {env_name}\")\n",
    "    print(f\"State dimension: {state_dim}\")\n",
    "    print(f\"Action dimension: {action_dim}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create agent\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=128,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        buffer_size=10000,\n",
    "        batch_size=64,\n",
    "        target_update_freq=100\n",
    "    )\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    recent_rewards = deque(maxlen=100)\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            # Select and perform action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update agent\n",
    "            loss = agent.update()\n",
    "            if loss is not None:\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        recent_rewards.append(episode_reward)\n",
    "        if episode_loss:\n",
    "            losses.append(np.mean(episode_loss))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(recent_rewards)\n",
    "            avg_length = np.mean(list(recent_rewards))\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "            print(f\"  Avg Reward (last 100): {avg_reward:.2f}\")\n",
    "            print(f\"  Epsilon: {agent.epsilon:.3f}\")\n",
    "            print(f\"  Buffer Size: {len(agent.replay_buffer)}\")\n",
    "            if losses:\n",
    "                print(f\"  Avg Loss: {np.mean(losses[-100:]):.4f}\")\n",
    "            \n",
    "            # Check if solved\n",
    "            if avg_reward >= 195.0:\n",
    "                print(f\"\\nüéâ Environment solved in {episode + 1} episodes!\")\n",
    "                print(f\"   Average reward: {avg_reward:.2f}\")\n",
    "                break\n",
    "    \n",
    "    env.close()\n",
    "    return agent, episode_rewards, episode_lengths, losses\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "print(\"Starting DQN training...\\n\")\n",
    "agent, rewards, lengths, losses = train_dqn(\n",
    "    env_name='CartPole-v1',\n",
    "    num_episodes=500,\n",
    "    max_steps=500,\n",
    "    print_every=50\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Total episodes: {len(rewards)}\")\n",
    "print(f\"Final average reward (last 100): {np.mean(rewards[-100:]):.2f}\")\n",
    "print(f\"Best episode reward: {max(rewards):.2f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Training Progress\n",
    "\n",
    "Let's visualize how the agent's performance improved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comprehensive training visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "# Moving average\n",
    "window = 50\n",
    "if len(rewards) >= window:\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(rewards)), moving_avg, color='red', \n",
    "             linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "ax1.axhline(y=195, color='green', linestyle='--', linewidth=2, \n",
    "            label='Solved Threshold (195)', alpha=0.7)\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\n",
    "ax1.set_title('DQN Training Progress: Episode Rewards', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode Lengths\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(lengths, alpha=0.3, color='purple', label='Episode Length')\n",
    "if len(lengths) >= window:\n",
    "    moving_avg_length = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(range(window-1, len(lengths)), moving_avg_length, color='orange', \n",
    "             linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Episode Length (steps)', fontsize=12)\n",
    "ax2.set_title('Episode Lengths Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training Loss\n",
    "ax3 = axes[1, 0]\n",
    "if losses:\n",
    "    ax3.plot(losses, alpha=0.5, color='red', label='TD Loss')\n",
    "    if len(losses) >= window:\n",
    "        moving_avg_loss = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "        ax3.plot(range(window-1, len(losses)), moving_avg_loss, color='darkred', \n",
    "                 linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "    ax3.set_xlabel('Episode', fontsize=12)\n",
    "    ax3.set_ylabel('Loss', fontsize=12)\n",
    "    ax3.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')  # Log scale for better visualization\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No loss data available', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Reward Distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(rewards, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "ax4.axvline(x=np.mean(rewards), color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Mean: {np.mean(rewards):.2f}')\n",
    "ax4.axvline(x=195, color='blue', linestyle='--', \n",
    "            linewidth=2, label='Solved: 195')\n",
    "ax4.set_xlabel('Total Reward', fontsize=12)\n",
    "ax4.set_ylabel('Frequency', fontsize=12)\n",
    "ax4.set_title('Distribution of Episode Rewards', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nüìä Training Summary Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Episodes: {len(rewards)}\")\n",
    "print(f\"\\nReward Statistics:\")\n",
    "print(f\"  Mean: {np.mean(rewards):.2f}\")\n",
    "print(f\"  Std: {np.std(rewards):.2f}\")\n",
    "print(f\"  Min: {np.min(rewards):.2f}\")\n",
    "print(f\"  Max: {np.max(rewards):.2f}\")\n",
    "print(f\"  Last 100 episodes mean: {np.mean(rewards[-100:]):.2f}\")\n",
    "print(f\"\\nEpisode Length Statistics:\")\n",
    "print(f\"  Mean: {np.mean(lengths):.2f}\")\n",
    "print(f\"  Max: {np.max(lengths)}\")\n",
    "print(f\"\\nExploration:\")\n",
    "print(f\"  Final epsilon: {agent.epsilon:.4f}\")\n",
    "print(f\"  Replay buffer size: {len(agent.replay_buffer)}\")\n",
    "\n",
    "if np.mean(rewards[-100:]) >= 195:\n",
    "    print(\"\\n‚úÖ Environment SOLVED! Average reward ‚â• 195 over last 100 episodes\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Not quite solved yet. Need {195 - np.mean(rewards[-100:]):.2f} more reward on average.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Results\n",
    "\n",
    "**What We Observe:**\n",
    "\n",
    "1. **Learning Curve**: The agent starts with poor performance (random actions) and gradually improves as it learns\n",
    "\n",
    "2. **Exploration vs Exploitation**: \n",
    "   - Early episodes: High epsilon ‚Üí more exploration ‚Üí variable performance\n",
    "   - Later episodes: Low epsilon ‚Üí more exploitation ‚Üí stable high performance\n",
    "\n",
    "3. **Target Network Impact**:\n",
    "   - Training is stable without wild oscillations\n",
    "   - Loss decreases smoothly over time\n",
    "   - The agent converges to a good policy\n",
    "\n",
    "4. **Experience Replay Benefits**:\n",
    "   - Efficient use of past experiences\n",
    "   - Breaks temporal correlations\n",
    "   - Enables mini-batch training\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "- **Target networks** prevent the moving target problem and stabilize training\n",
    "- **Experience replay** breaks correlations and improves sample efficiency\n",
    "- **Epsilon-greedy** exploration ensures the agent discovers good strategies\n",
    "- The combination of these techniques makes DQN work!\n",
    "\n",
    "**Hyperparameter Sensitivity:**\n",
    "\n",
    "- **Learning rate**: Too high ‚Üí instability; too low ‚Üí slow learning\n",
    "- **Target update frequency**: Too frequent ‚Üí moving target; too rare ‚Üí slow adaptation\n",
    "- **Batch size**: Larger ‚Üí more stable gradients; smaller ‚Üí more updates\n",
    "- **Buffer size**: Larger ‚Üí more diversity; smaller ‚Üí more recent experiences\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "DQN with target networks is a powerful algorithm, but it still has limitations. In the next section, we'll explore **Double DQN**, which addresses the overestimation bias in standard DQN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double DQN: Addressing Overestimation Bias",
    "",
    "**The Problem with Standard DQN**",
    "",
    "Standard DQN has a subtle but important flaw: it tends to **overestimate** Q-values. This happens because of how the max operator is used in the Q-learning update:",
    "",
    "$",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s,a) \\right]",
    "$",
    "",
    "**Why Overestimation Occurs:**",
    "",
    "The same network is used for both:",
    "1. **Selecting** the best action: $\\arg\\max_{a'} Q(s', a')$",
    "2. **Evaluating** that action: $Q(s', a')$",
    "",
    "This creates a **maximization bias**: if the Q-values have any estimation errors (which they always do), the max operation will tend to select actions with positive errors, leading to systematic overestimation.",
    "",
    "**Example of the Problem:**",
    "",
    "Imagine you're estimating the value of 3 actions, and your estimates have random errors:",
    "- True values: [1.0, 1.0, 1.0] (all equal)",
    "- Noisy estimates: [0.9, 1.2, 0.8] (with random errors)",
    "- Standard DQN picks: max([0.9, 1.2, 0.8]) = 1.2",
    "- This overestimates the true value of 1.0!",
    "",
    "Over many updates, these overestimations accumulate and can hurt performance.",
    "",
    "**The Double DQN Solution**",
    "",
    "Double DQN (DDQN) addresses this by **decoupling action selection from action evaluation**:",
    "",
    "1. Use the **online network** to select the best action",
    "2. Use the **target network** to evaluate that action",
    "",
    "**Double DQN Update Rule:**",
    "",
    "$",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma Q_{\\theta^-}\\left(s', \\arg\\max_{a'} Q_\\theta(s', a')\\right) - Q(s,a) \\right]",
    "$",
    "",
    "where:",
    "- $Q_\\theta$ is the online network (selects action)",
    "- $Q_{\\theta^-}$ is the target network (evaluates action)",
    "",
    "**Key Insight:**",
    "",
    "By using different networks for selection and evaluation, we reduce the correlation between the errors, which reduces overestimation bias.",
    "",
    "**Benefits of Double DQN:**",
    "",
    "- More accurate Q-value estimates",
    "- Better performance on many tasks",
    "- More stable learning",
    "- Minimal computational overhead (we already have both networks!)",
    "",
    "Let's implement Double DQN and compare it with standard DQN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DQNAgent):",
    "    \"\"\"Double DQN agent that reduces overestimation bias.",
    "    ",
    "    Inherits from DQNAgent and only modifies the update method to use",
    "    Double Q-learning: online network selects actions, target network evaluates them.",
    "    \"\"\"",
    "    ",
    "    def update(self):",
    "        \"\"\"Update the agent using Double Q-learning.",
    "        ",
    "        Key difference from standard DQN:",
    "        - Online network selects the best action",
    "        - Target network evaluates that action",
    "        \"\"\"",
    "        # Need enough samples in buffer",
    "        if len(self.replay_buffer) < self.batch_size:",
    "            return None",
    "        ",
    "        # Sample mini-batch from replay buffer",
    "        batch = random.sample(self.replay_buffer, self.batch_size)",
    "        states, actions, rewards, next_states, dones = zip(*batch)",
    "        ",
    "        # Convert to tensors",
    "        states = torch.FloatTensor(np.array(states))",
    "        actions = torch.LongTensor(actions)",
    "        rewards = torch.FloatTensor(rewards)",
    "        next_states = torch.FloatTensor(np.array(next_states))",
    "        dones = torch.FloatTensor(dones)",
    "        ",
    "        # Compute current Q-values",
    "        current_q_values = self.online_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)",
    "        ",
    "        # Compute target Q-values using Double Q-learning",
    "        with torch.no_grad():",
    "            # DOUBLE DQN: Use online network to SELECT actions",
    "            next_actions = self.online_network(next_states).argmax(1)",
    "            ",
    "            # DOUBLE DQN: Use target network to EVALUATE those actions",
    "            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)",
    "            ",
    "            # Compute targets",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values",
    "        ",
    "        # Compute loss",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)",
    "        ",
    "        # Optimize the online network",
    "        self.optimizer.zero_grad()",
    "        loss.backward()",
    "        self.optimizer.step()",
    "        ",
    "        # Update target network periodically",
    "        self.steps += 1",
    "        if self.steps % self.target_update_freq == 0:",
    "            self.update_target_network()",
    "        ",
    "        # Decay epsilon",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)",
    "        ",
    "        return loss.item()",
    "",
    "",
    "print(\"Double DQN Agent implemented!\")",
    "print(\"\\nKey difference from standard DQN:\")",
    "print(\"  ‚úì Online network SELECTS the best action\")",
    "print(\"  ‚úì Target network EVALUATES that action\")",
    "print(\"  ‚úì This decoupling reduces overestimation bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Standard DQN vs Double DQN",
    "",
    "Now let's train both algorithms on the same environment and compare their performance. We'll look at:",
    "",
    "1. **Learning curves**: How quickly do they learn?",
    "2. **Final performance**: Which achieves better results?",
    "3. **Stability**: Which is more consistent?",
    "4. **Q-value estimates**: Do we see evidence of overestimation?",
    "",
    "Let's run the comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_comparison(agent_class, agent_name, env_name='CartPole-v1', ",
    "                            num_episodes=300, max_steps=500, seed=42):",
    "    \"\"\"",
    "    Train an agent and return metrics for comparison.",
    "    ",
    "    Args:",
    "        agent_class: DQNAgent or DoubleDQNAgent class",
    "        agent_name: Name for logging",
    "        env_name: Gym environment name",
    "        num_episodes: Number of training episodes",
    "        max_steps: Max steps per episode",
    "        seed: Random seed for reproducibility",
    "        ",
    "    Returns:",
    "        Dictionary with training metrics",
    "    \"\"\"",
    "    # Set seeds for reproducibility",
    "    np.random.seed(seed)",
    "    random.seed(seed)",
    "    torch.manual_seed(seed)",
    "    ",
    "    # Create environment",
    "    env = gym.make(env_name)",
    "    env.seed(seed)",
    "    state_dim = env.observation_space.shape[0]",
    "    action_dim = env.action_space.n",
    "    ",
    "    print(f\"\\nTraining {agent_name} on {env_name}\")",
    "    print(\"=\" * 60)",
    "    ",
    "    # Create agent",
    "    agent = agent_class(",
    "        state_dim=state_dim,",
    "        action_dim=action_dim,",
    "        hidden_dim=128,",
    "        lr=1e-3,",
    "        gamma=0.99,",
    "        epsilon_start=1.0,",
    "        epsilon_end=0.01,",
    "        epsilon_decay=0.995,",
    "        buffer_size=10000,",
    "        batch_size=64,",
    "        target_update_freq=100",
    "    )",
    "    ",
    "    # Training metrics",
    "    episode_rewards = []",
    "    episode_lengths = []",
    "    losses = []",
    "    q_values = []  # Track Q-values to detect overestimation",
    "    ",
    "    # Training loop",
    "    for episode in range(num_episodes):",
    "        state = env.reset()",
    "        episode_reward = 0",
    "        episode_loss = []",
    "        episode_q = []",
    "        ",
    "        for step in range(max_steps):",
    "            # Select and perform action",
    "            action = agent.select_action(state, training=True)",
    "            ",
    "            # Track Q-values",
    "            with torch.no_grad():",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)",
    "                q_vals = agent.online_network(state_tensor).max().item()",
    "                episode_q.append(q_vals)",
    "            ",
    "            next_state, reward, done, _ = env.step(action)",
    "            ",
    "            # Store transition",
    "            agent.store_transition(state, action, reward, next_state, done)",
    "            ",
    "            # Update agent",
    "            loss = agent.update()",
    "            if loss is not None:",
    "                episode_loss.append(loss)",
    "            ",
    "            episode_reward += reward",
    "            state = next_state",
    "            ",
    "            if done:",
    "                break",
    "        ",
    "        # Record metrics",
    "        episode_rewards.append(episode_reward)",
    "        episode_lengths.append(step + 1)",
    "        if episode_loss:",
    "            losses.append(np.mean(episode_loss))",
    "        if episode_q:",
    "            q_values.append(np.mean(episode_q))",
    "        ",
    "        # Print progress",
    "        if (episode + 1) % 50 == 0:",
    "            avg_reward = np.mean(episode_rewards[-50:])",
    "            print(f\"Episode {episode + 1}/{num_episodes} | \"",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")",
    "    ",
    "    env.close()",
    "    ",
    "    return {",
    "        'name': agent_name,",
    "        'rewards': episode_rewards,",
    "        'lengths': episode_lengths,",
    "        'losses': losses,",
    "        'q_values': q_values,",
    "        'agent': agent",
    "    }",
    "",
    "",
    "# Train both agents with the same seed for fair comparison",
    "print(\"Starting comparison experiment...\")",
    "print(\"This will train both DQN and Double DQN on CartPole-v1\")",
    "print(\"=\" * 60)",
    "",
    "# Train standard DQN",
    "dqn_results = train_agent_comparison(",
    "    DQNAgent, ",
    "    \"Standard DQN\",",
    "    num_episodes=300,",
    "    seed=42",
    ")",
    "",
    "# Train Double DQN",
    "ddqn_results = train_agent_comparison(",
    "    DoubleDQNAgent,",
    "    \"Double DQN\", ",
    "    num_episodes=300,",
    "    seed=42",
    ")",
    "",
    "print(\"\\n\" + \"=\" * 60)",
    "print(\"Training completed for both agents!\")",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Comparison",
    "",
    "Let's create comprehensive visualizations to compare the two algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualization",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))",
    "",
    "# Prepare data",
    "window = 20  # Moving average window",
    "",
    "# Plot 1: Episode Rewards Comparison",
    "ax1 = axes[0, 0]",
    "ax1.plot(dqn_results['rewards'], alpha=0.2, color='blue', linewidth=0.5)",
    "ax1.plot(ddqn_results['rewards'], alpha=0.2, color='red', linewidth=0.5)",
    "",
    "# Moving averages",
    "if len(dqn_results['rewards']) >= window:",
    "    dqn_ma = np.convolve(dqn_results['rewards'], np.ones(window)/window, mode='valid')",
    "    ax1.plot(range(window-1, len(dqn_results['rewards'])), dqn_ma, ",
    "             color='blue', linewidth=2.5, label='Standard DQN')",
    "",
    "if len(ddqn_results['rewards']) >= window:",
    "    ddqn_ma = np.convolve(ddqn_results['rewards'], np.ones(window)/window, mode='valid')",
    "    ax1.plot(range(window-1, len(ddqn_results['rewards'])), ddqn_ma, ",
    "             color='red', linewidth=2.5, label='Double DQN')",
    "",
    "ax1.axhline(y=195, color='green', linestyle='--', linewidth=2, ",
    "            label='Solved Threshold', alpha=0.7)",
    "ax1.set_xlabel('Episode', fontsize=12)",
    "ax1.set_ylabel('Total Reward', fontsize=12)",
    "ax1.set_title('Learning Curves: DQN vs Double DQN', fontsize=14, fontweight='bold')",
    "ax1.legend(fontsize=11)",
    "ax1.grid(True, alpha=0.3)",
    "",
    "# Plot 2: Q-Value Estimates Over Time",
    "ax2 = axes[0, 1]",
    "if dqn_results['q_values'] and ddqn_results['q_values']:",
    "    ax2.plot(dqn_results['q_values'], alpha=0.3, color='blue', linewidth=0.5)",
    "    ax2.plot(ddqn_results['q_values'], alpha=0.3, color='red', linewidth=0.5)",
    "    ",
    "    # Moving averages",
    "    if len(dqn_results['q_values']) >= window:",
    "        dqn_q_ma = np.convolve(dqn_results['q_values'], np.ones(window)/window, mode='valid')",
    "        ax2.plot(range(window-1, len(dqn_results['q_values'])), dqn_q_ma, ",
    "                 color='blue', linewidth=2.5, label='Standard DQN')",
    "    ",
    "    if len(ddqn_results['q_values']) >= window:",
    "        ddqn_q_ma = np.convolve(ddqn_results['q_values'], np.ones(window)/window, mode='valid')",
    "        ax2.plot(range(window-1, len(ddqn_results['q_values'])), ddqn_q_ma, ",
    "                 color='red', linewidth=2.5, label='Double DQN')",
    "    ",
    "    ax2.set_xlabel('Episode', fontsize=12)",
    "    ax2.set_ylabel('Average Max Q-Value', fontsize=12)",
    "    ax2.set_title('Q-Value Estimates: Evidence of Overestimation?', fontsize=14, fontweight='bold')",
    "    ax2.legend(fontsize=11)",
    "    ax2.grid(True, alpha=0.3)",
    "",
    "# Plot 3: Training Loss Comparison",
    "ax3 = axes[1, 0]",
    "if dqn_results['losses'] and ddqn_results['losses']:",
    "    ax3.plot(dqn_results['losses'], alpha=0.3, color='blue', linewidth=0.5)",
    "    ax3.plot(ddqn_results['losses'], alpha=0.3, color='red', linewidth=0.5)",
    "    ",
    "    # Moving averages",
    "    if len(dqn_results['losses']) >= window:",
    "        dqn_loss_ma = np.convolve(dqn_results['losses'], np.ones(window)/window, mode='valid')",
    "        ax3.plot(range(window-1, len(dqn_results['losses'])), dqn_loss_ma, ",
    "                 color='blue', linewidth=2.5, label='Standard DQN')",
    "    ",
    "    if len(ddqn_results['losses']) >= window:",
    "        ddqn_loss_ma = np.convolve(ddqn_results['losses'], np.ones(window)/window, mode='valid')",
    "        ax3.plot(range(window-1, len(ddqn_results['losses'])), ddqn_loss_ma, ",
    "                 color='red', linewidth=2.5, label='Double DQN')",
    "    ",
    "    ax3.set_xlabel('Episode', fontsize=12)",
    "    ax3.set_ylabel('Loss', fontsize=12)",
    "    ax3.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')",
    "    ax3.legend(fontsize=11)",
    "    ax3.grid(True, alpha=0.3)",
    "    ax3.set_yscale('log')",
    "",
    "# Plot 4: Performance Distribution",
    "ax4 = axes[1, 1]",
    "ax4.hist(dqn_results['rewards'], bins=30, alpha=0.5, color='blue', ",
    "         label='Standard DQN', edgecolor='black')",
    "ax4.hist(ddqn_results['rewards'], bins=30, alpha=0.5, color='red', ",
    "         label='Double DQN', edgecolor='black')",
    "ax4.axvline(x=np.mean(dqn_results['rewards']), color='blue', ",
    "            linestyle='--', linewidth=2, alpha=0.7)",
    "ax4.axvline(x=np.mean(ddqn_results['rewards']), color='red', ",
    "            linestyle='--', linewidth=2, alpha=0.7)",
    "ax4.set_xlabel('Total Reward', fontsize=12)",
    "ax4.set_ylabel('Frequency', fontsize=12)",
    "ax4.set_title('Reward Distribution Comparison', fontsize=14, fontweight='bold')",
    "ax4.legend(fontsize=11)",
    "ax4.grid(True, alpha=0.3, axis='y')",
    "",
    "plt.tight_layout()",
    "plt.show()",
    "",
    "# Print detailed comparison statistics",
    "print(\"\\n\" + \"=\" * 70)",
    "print(\"DETAILED COMPARISON: Standard DQN vs Double DQN\")",
    "print(\"=\" * 70)",
    "",
    "print(\"\\nüìä REWARD STATISTICS:\")",
    "print(\"-\" * 70)",
    "print(f\"{'Metric':<30} {'Standard DQN':>18} {'Double DQN':>18}\")",
    "print(\"-\" * 70)",
    "print(f\"{'Mean Reward':<30} {np.mean(dqn_results['rewards']):>18.2f} \"",
    "      f\"{np.mean(ddqn_results['rewards']):>18.2f}\")",
    "print(f\"{'Std Reward':<30} {np.std(dqn_results['rewards']):>18.2f} \"",
    "      f\"{np.std(ddqn_results['rewards']):>18.2f}\")",
    "print(f\"{'Max Reward':<30} {np.max(dqn_results['rewards']):>18.2f} \"",
    "      f\"{np.max(ddqn_results['rewards']):>18.2f}\")",
    "print(f\"{'Last 50 Episodes Mean':<30} {np.mean(dqn_results['rewards'][-50:]):>18.2f} \"",
    "      f\"{np.mean(ddqn_results['rewards'][-50:]):>18.2f}\")",
    "",
    "print(\"\\nüìà Q-VALUE STATISTICS (Overestimation Check):\")",
    "print(\"-\" * 70)",
    "if dqn_results['q_values'] and ddqn_results['q_values']:",
    "    print(f\"{'Mean Q-Value':<30} {np.mean(dqn_results['q_values']):>18.2f} \"",
    "          f\"{np.mean(ddqn_results['q_values']):>18.2f}\")",
    "    print(f\"{'Max Q-Value':<30} {np.max(dqn_results['q_values']):>18.2f} \"",
    "          f\"{np.max(ddqn_results['q_values']):>18.2f}\")",
    "    print(f\"{'Final 50 Episodes Mean Q':<30} {np.mean(dqn_results['q_values'][-50:]):>18.2f} \"",
    "          f\"{np.mean(ddqn_results['q_values'][-50:]):>18.2f}\")",
    "",
    "print(\"\\nüéØ CONVERGENCE:\")",
    "print(\"-\" * 70)",
    "# Find first episode where moving average exceeds 195",
    "dqn_solved = -1",
    "ddqn_solved = -1",
    "threshold = 195",
    "window_size = 100",
    "",
    "for i in range(window_size, len(dqn_results['rewards'])):",
    "    if np.mean(dqn_results['rewards'][i-window_size:i]) >= threshold:",
    "        dqn_solved = i",
    "        break",
    "",
    "for i in range(window_size, len(ddqn_results['rewards'])):",
    "    if np.mean(ddqn_results['rewards'][i-window_size:i]) >= threshold:",
    "        ddqn_solved = i",
    "        break",
    "",
    "if dqn_solved > 0:",
    "    print(f\"{'Standard DQN solved at':<30} Episode {dqn_solved}\")",
    "else:",
    "    print(f\"{'Standard DQN':<30} Not solved\")",
    "",
    "if ddqn_solved > 0:",
    "    print(f\"{'Double DQN solved at':<30} Episode {ddqn_solved}\")",
    "else:",
    "    print(f\"{'Double DQN':<30} Not solved\")",
    "",
    "print(\"\\n\" + \"=\" * 70)",
    "",
    "# Determine winner",
    "dqn_final = np.mean(dqn_results['rewards'][-50:])",
    "ddqn_final = np.mean(ddqn_results['rewards'][-50:])",
    "",
    "if ddqn_final > dqn_final:",
    "    improvement = ((ddqn_final - dqn_final) / dqn_final) * 100",
    "    print(f\"\\nüèÜ WINNER: Double DQN\")",
    "    print(f\"   Improvement: {improvement:.1f}% better final performance\")",
    "elif dqn_final > ddqn_final:",
    "    improvement = ((dqn_final - ddqn_final) / ddqn_final) * 100",
    "    print(f\"\\nüèÜ WINNER: Standard DQN\")",
    "    print(f\"   Improvement: {improvement:.1f}% better final performance\")",
    "else:",
    "    print(f\"\\nü§ù TIE: Both algorithms performed similarly\")",
    "",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: Why Double DQN Works Better",
    "",
    "**Key Observations from the Comparison:**",
    "",
    "1. **Q-Value Estimates**:",
    "   - Standard DQN typically shows higher Q-values, indicating overestimation",
    "   - Double DQN produces more conservative, accurate Q-value estimates",
    "   - This is evidence of the overestimation bias being reduced",
    "",
    "2. **Learning Stability**:",
    "   - Double DQN often shows smoother learning curves",
    "   - Less variance in performance across episodes",
    "   - More consistent convergence to good policies",
    "",
    "3. **Final Performance**:",
    "   - Double DQN frequently achieves better or equal final performance",
    "   - The improvement is more pronounced in complex environments",
    "   - CartPole is relatively simple, so differences may be subtle",
    "",
    "4. **Computational Cost**:",
    "   - Double DQN has virtually no additional computational cost",
    "   - Same network architecture and training time",
    "   - Only the update rule changes slightly",
    "",
    "**When Does Double DQN Help Most?**",
    "",
    "Double DQN provides the biggest benefits when:",
    "- The environment has stochastic rewards or transitions",
    "- The action space is large",
    "- Q-value estimation is noisy",
    "- Long-term planning is important",
    "",
    "**Practical Recommendations:**",
    "",
    "‚úÖ **Use Double DQN** as your default choice - it's strictly better than standard DQN with no downsides",
    "",
    "‚úÖ **Combine with other improvements** like:",
    "   - Prioritized Experience Replay",
    "   - Dueling Networks",
    "   - Noisy Networks for exploration",
    "",
    "‚úÖ **Monitor Q-values** during training to detect overestimation issues",
    "",
    "**Mathematical Insight:**",
    "",
    "The key insight is that by decoupling action selection from evaluation, we reduce the positive bias:",
    "",
    "$",
    "\\mathbb{E}[\\max_a Q(s,a)] \\geq \\max_a \\mathbb{E}[Q(s,a)]",
    "$",
    "",
    "This inequality (Jensen's inequality for the max function) shows that taking the max of noisy estimates gives a biased result. Double DQN mitigates this by using independent estimates.",
    "",
    "**Next Steps:**",
    "",
    "Double DQN is a foundational improvement to DQN. Modern deep RL often combines it with other techniques like:",
    "- **Dueling DQN**: Separate value and advantage streams",
    "- **Prioritized Replay**: Sample important transitions more frequently  ",
    "- **Rainbow DQN**: Combines multiple improvements into one algorithm",
    "",
    "In the next sections, we'll explore policy gradient methods, which take a fundamentally different approach to RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## Section 3: Advanced Topics\n",
    "\n",
    "*Content will be added in subsequent tasks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## Section 4: Code Implementations\n",
    "\n",
    "*Content will be added in subsequent tasks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## Section 5: Real-World Applications\n",
    "\n",
    "*Content will be added in subsequent tasks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## Section 6: Advanced Research & Deployment\n",
    "\n",
    "*Content will be added in subsequent tasks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## Conclusion and Next Steps\n",
    "\n",
    "*Content will be added in subsequent tasks*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}