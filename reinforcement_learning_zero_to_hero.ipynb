{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Zero to Hero\n",
    "\n",
    "## A Comprehensive Journey from Foundational Concepts to Advanced Applications\n",
    "\n",
    "Welcome to this comprehensive Jupyter notebook on Reinforcement Learning! This notebook will guide you through a complete learning journey, starting from the basics and progressing to advanced research topics and real-world applications.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Foundational Concepts**: Understanding the core principles of reinforcement learning, including MDPs, value functions, and policies\n",
    "- **Core Algorithms**: Implementing and understanding key algorithms like Q-Learning, SARSA, DQN, and policy gradient methods\n",
    "- **Advanced Topics**: Exploring cutting-edge techniques in reward engineering, scaling, and specialized RL methods\n",
    "- **Real-World Applications**: Seeing how RL is applied in robotics, game playing, finance, healthcare, and more\n",
    "- **Research & Deployment**: Understanding current research trends and how to deploy RL systems in production\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "1. Execute cells sequentially from top to bottom\n",
    "2. Read the explanations carefully before running code\n",
    "3. Experiment with the code examples\n",
    "4. Modify parameters to see how they affect results\n",
    "5. Complete the exercises to reinforce your understanding\n",
    "\n",
    "Let's begin your journey into the exciting world of Reinforcement Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Dependencies](#setup)\n",
    "2. [Section 1: Foundational Concepts](#section1)\n",
    "   - [Introduction to Reinforcement Learning](#intro-rl)\n",
    "   - [Multi-Armed Bandit Problem](#bandits)\n",
    "   - [Core Terminology and MDP Framework](#mdp)\n",
    "   - [Policies and Value Functions](#policies)\n",
    "   - [Dynamic Programming](#dynamic-programming)\n",
    "   - [Learning Paradigms](#learning-paradigms)\n",
    "3. [Section 2: Core Algorithms](#section2)\n",
    "   - [Monte Carlo Methods](#monte-carlo)\n",
    "   - [Temporal Difference Learning](#td-learning)\n",
    "   - [Q-Learning](#q-learning)\n",
    "   - [Deep Q-Networks (DQN)](#dqn)\n",
    "   - [Policy Optimization Methods](#policy-optimization)\n",
    "4. [Section 3: Advanced Topics](#section3)\n",
    "   - [Reward Engineering](#reward-engineering)\n",
    "   - [Scaling and Generalization](#scaling)\n",
    "   - [Advanced Policy Methods](#advanced-policy)\n",
    "   - [Specialized RL Techniques](#specialized)\n",
    "5. [Section 4: Code Implementations](#section4)\n",
    "   - [Bandit Algorithms](#bandit-implementations)\n",
    "   - [MDP and Dynamic Programming](#mdp-implementations)\n",
    "   - [Monte Carlo Methods](#mc-implementations)\n",
    "   - [Temporal Difference Methods](#td-implementations)\n",
    "   - [Deep RL Implementations](#deep-rl-implementations)\n",
    "6. [Section 5: Real-World Applications](#section5)\n",
    "   - [Traffic Signal Control](#traffic)\n",
    "   - [Robotics](#robotics)\n",
    "   - [Autonomous Trading](#trading)\n",
    "   - [Recommendation Systems](#recommendations)\n",
    "   - [Healthcare](#healthcare)\n",
    "   - [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "   - [Game Playing](#game-playing)\n",
    "   - [Energy Management](#energy)\n",
    "   - [Chess Environment](#chess)\n",
    "7. [Section 6: Advanced Research & Deployment](#section6)\n",
    "   - [Current Research Trends](#research-trends)\n",
    "   - [Ethical and Safety Considerations](#ethics)\n",
    "   - [Deployment Challenges](#deployment)\n",
    "   - [End-to-End Pipeline](#pipeline)\n",
    "   - [Recent Research](#recent-research)\n",
    "8. [Conclusion and Next Steps](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Setup & Dependencies\n",
    "\n",
    "Before we begin, we need to install the required Python packages. This notebook uses several popular libraries for numerical computation, visualization, and reinforcement learning environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: You may need to restart the kernel after installation\n",
    "\n",
    "!pip install numpy>=1.21.0\n",
    "!pip install matplotlib>=3.4.0\n",
    "!pip install seaborn>=0.11.0\n",
    "!pip install gym>=0.21.0\n",
    "!pip install torch>=1.10.0\n",
    "!pip install pandas>=1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "\n",
    "Now let's import all the libraries we'll be using throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core numerical and scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Reinforcement Learning environments\n",
    "import gym\n",
    "\n",
    "# Deep Learning framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Installation\n",
    "\n",
    "Let's verify that all packages are installed correctly and check their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell - check all installations\n",
    "import sys\n",
    "\n",
    "def check_package(package_name, import_name=None):\n",
    "    \"\"\"Check if a package is installed and print its version.\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    \n",
    "    try:\n",
    "        module = __import__(import_name)\n",
    "        version = getattr(module, '__version__', 'Unknown')\n",
    "        print(f\"\u2713 {package_name:20s} version: {version}\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"\u2717 {package_name:20s} NOT INSTALLED\")\n",
    "        return False\n",
    "\n",
    "print(\"Checking package installations...\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "packages = [\n",
    "    ('numpy', 'numpy'),\n",
    "    ('matplotlib', 'matplotlib'),\n",
    "    ('seaborn', 'seaborn'),\n",
    "    ('gym', 'gym'),\n",
    "    ('torch', 'torch'),\n",
    "    ('pandas', 'pandas')\n",
    "]\n",
    "\n",
    "all_installed = True\n",
    "for package_name, import_name in packages:\n",
    "    if not check_package(package_name, import_name):\n",
    "        all_installed = False\n",
    "\n",
    "print(\"\" + \"=\"*50)\n",
    "if all_installed:\n",
    "    print(\"\u2713 All packages installed successfully!\")\n",
    "    print(\"You're ready to start learning Reinforcement Learning!\")\n",
    "else:\n",
    "    print(\"\u2717 Some packages are missing. Please install them using:\")\n",
    "    print(\"  pip install numpy matplotlib seaborn gym torch pandas\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Section 1: Foundational Concepts\n",
    "\n",
    "In this section, we'll build a solid foundation in reinforcement learning by exploring core concepts, starting with the simplest problems and gradually increasing complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro-rl'></a>\n",
    "### Introduction to Reinforcement Learning\n",
    "\n",
    "**What is Reinforcement Learning?**\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. Unlike other machine learning paradigms, RL focuses on learning through trial and error, receiving feedback in the form of rewards or penalties.\n",
    "\n",
    "**How RL Differs from Other Machine Learning Paradigms:**\n",
    "\n",
    "1. **Supervised Learning**: \n",
    "   - Learns from labeled examples (input-output pairs)\n",
    "   - The correct answer is provided for each training example\n",
    "   - Example: Image classification, where each image has a label\n",
    "\n",
    "2. **Unsupervised Learning**: \n",
    "   - Learns patterns from unlabeled data\n",
    "   - No explicit feedback or correct answers\n",
    "   - Example: Clustering customers based on purchasing behavior\n",
    "\n",
    "3. **Reinforcement Learning**: \n",
    "   - Learns from interaction with an environment\n",
    "   - Receives delayed rewards/penalties as feedback\n",
    "   - Must discover which actions yield the most reward through exploration\n",
    "   - Example: Teaching a robot to walk, playing chess, or optimizing ad placement\n",
    "\n",
    "**Key Characteristics of RL:**\n",
    "\n",
    "- **Sequential Decision Making**: Actions affect future states and rewards\n",
    "- **Trial and Error**: The agent must explore to discover good strategies\n",
    "- **Delayed Consequences**: Rewards may come long after the actions that caused them\n",
    "- **Trade-offs**: Must balance exploration (trying new things) vs exploitation (using known good strategies)\n",
    "\n",
    "**The RL Loop:**\n",
    "\n",
    "The fundamental interaction pattern in RL is:\n",
    "\n",
    "1. Agent observes the current **state** of the environment\n",
    "2. Agent selects and performs an **action**\n",
    "3. Environment transitions to a new **state**\n",
    "4. Agent receives a **reward** signal\n",
    "5. Repeat\n",
    "\n",
    "Let's see this in action with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple demonstration of the RL loop\n",
    "# We'll create a basic environment and agent to illustrate the interaction\n",
    "\n",
    "class SimpleEnvironment:\n",
    "    \"\"\"A simple environment where the agent tries to reach a goal.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.position = 0  # Agent starts at position 0\n",
    "        self.goal = 5      # Goal is at position 5\n",
    "        self.max_steps = 10\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state.\"\"\"\n",
    "        self.position = 0\n",
    "        self.current_step = 0\n",
    "        return self.position\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute an action and return (next_state, reward, done).\n",
    "        \n",
    "        Args:\n",
    "            action: 0 = move left, 1 = move right\n",
    "        \n",
    "        Returns:\n",
    "            next_state: The new position\n",
    "            reward: Reward for this transition\n",
    "            done: Whether the episode is finished\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Update position based on action\n",
    "        if action == 0:  # Move left\n",
    "            self.position = max(0, self.position - 1)\n",
    "        else:  # Move right\n",
    "            self.position = min(10, self.position + 1)\n",
    "        \n",
    "        # Calculate reward\n",
    "        if self.position == self.goal:\n",
    "            reward = 10  # Large reward for reaching goal\n",
    "            done = True\n",
    "        elif self.current_step >= self.max_steps:\n",
    "            reward = -5  # Penalty for taking too long\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1  # Small penalty for each step (encourages efficiency)\n",
    "            done = False\n",
    "        \n",
    "        return self.position, reward, done\n",
    "\n",
    "\n",
    "class SimpleAgent:\n",
    "    \"\"\"A simple agent that takes random actions.\"\"\"\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action (randomly for now).\"\"\"\n",
    "        return np.random.choice([0, 1])  # 0 = left, 1 = right\n",
    "\n",
    "\n",
    "# Demonstrate the RL loop\n",
    "print(\"Demonstrating the Reinforcement Learning Loop\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = SimpleEnvironment()\n",
    "agent = SimpleAgent()\n",
    "\n",
    "# Run one episode\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "step = 0\n",
    "\n",
    "print(f\"Initial State: Position = {state}, Goal = {env.goal}\")\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # Agent observes state and selects action\n",
    "    action = agent.select_action(state)\n",
    "    action_name = \"LEFT\" if action == 0 else \"RIGHT\"\n",
    "    \n",
    "    # Environment responds to action\n",
    "    next_state, reward, done = env.step(action)\n",
    "    \n",
    "    # Track cumulative reward\n",
    "    total_reward += reward\n",
    "    step += 1\n",
    "    \n",
    "    # Display the interaction\n",
    "    print(f\"Step {step}:\")\n",
    "    print(f\"  State: {state} \u2192 Action: {action_name} \u2192 Next State: {next_state}\")\n",
    "    print(f\"  Reward: {reward:+.0f} | Total Reward: {total_reward:+.0f}\")\n",
    "    \n",
    "    if done:\n",
    "        if next_state == env.goal:\n",
    "            print(f\"\u2713 Goal reached in {step} steps!\")\n",
    "        else:\n",
    "            print(f\"\u2717 Failed to reach goal within {env.max_steps} steps.\")\n",
    "    print()\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Final Total Reward: {total_reward:+.0f}\")\n",
    "print(\"This demonstrates the core RL loop:\")\n",
    "print(\"  1. Agent observes STATE\")\n",
    "print(\"  2. Agent takes ACTION\")\n",
    "print(\"  3. Environment provides REWARD and new STATE\")\n",
    "print(\"  4. Repeat until episode ends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "\n",
    "- The agent doesn't know the optimal strategy initially\n",
    "- It must learn through experience which actions lead to higher rewards\n",
    "- The random agent above is inefficient - a learning agent would improve over time\n",
    "- This simple example captures the essence of RL: learning to make good decisions through interaction\n",
    "\n",
    "In the following sections, we'll explore how agents can learn optimal strategies, starting with one of the simplest RL problems: the Multi-Armed Bandit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bandits'></a>\n",
    "### Multi-Armed Bandit Problem\n",
    "\n",
    "**What is the Multi-Armed Bandit Problem?**\n",
    "\n",
    "Imagine you're in a casino facing a row of slot machines (also called \"one-armed bandits\"). Each machine has a different, unknown probability of paying out. You have a limited budget and want to maximize your total winnings. Which machines should you play?\n",
    "\n",
    "This is the **K-Armed Bandit Problem**, one of the simplest yet most fundamental problems in reinforcement learning. It's called \"K-armed\" because there are K different slot machines (or \"arms\") to choose from.\n",
    "\n",
    "**The Exploration-Exploitation Dilemma**\n",
    "\n",
    "The bandit problem perfectly illustrates the core challenge in RL:\n",
    "\n",
    "- **Exploitation**: Play the machine that has given you the best results so far (use your current knowledge)\n",
    "- **Exploration**: Try other machines to see if they might be better (gather more information)\n",
    "\n",
    "If you only exploit, you might miss out on better options you haven't tried enough. If you only explore, you waste time on machines you already know are bad. The key is finding the right balance.\n",
    "\n",
    "**Formal Definition:**\n",
    "\n",
    "- You have K actions (arms) to choose from\n",
    "- Each action has an unknown expected reward (the \"true value\")\n",
    "- When you select an action, you receive a reward drawn from that action's probability distribution\n",
    "- Your goal: maximize the total reward over many time steps\n",
    "\n",
    "Let's implement a simple bandit environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    \"\"\"A K-armed bandit environment.\n",
    "    \n",
    "    Each arm has a true mean reward, and pulling an arm gives a reward\n",
    "    sampled from a normal distribution around that mean.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=10, mean_range=(0, 1), std=1.0):\n",
    "        \"\"\"Initialize the bandit.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of arms\n",
    "            mean_range: Range for true mean rewards\n",
    "            std: Standard deviation of reward distributions\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.std = std\n",
    "        \n",
    "        # True mean reward for each arm (unknown to the agent)\n",
    "        self.true_means = np.random.uniform(mean_range[0], mean_range[1], k)\n",
    "        \n",
    "        # Track which arm is actually best\n",
    "        self.best_arm = np.argmax(self.true_means)\n",
    "        \n",
    "    def pull(self, arm):\n",
    "        \"\"\"Pull an arm and receive a reward.\n",
    "        \n",
    "        Args:\n",
    "            arm: Index of the arm to pull (0 to k-1)\n",
    "            \n",
    "        Returns:\n",
    "            reward: Sampled reward from this arm's distribution\n",
    "        \"\"\"\n",
    "        # Sample reward from normal distribution around true mean\n",
    "        reward = np.random.normal(self.true_means[arm], self.std)\n",
    "        return reward\n",
    "    \n",
    "    def get_optimal_reward(self):\n",
    "        \"\"\"Return the expected reward of the best arm.\"\"\"\n",
    "        return self.true_means[self.best_arm]\n",
    "\n",
    "\n",
    "# Create a 10-armed bandit\n",
    "np.random.seed(42)\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "\n",
    "print(\"Multi-Armed Bandit Environment Created\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of arms: {bandit.k}\")\n",
    "print(f\"True mean rewards for each arm:\")\n",
    "for i, mean in enumerate(bandit.true_means):\n",
    "    marker = \" \u2190 BEST\" if i == bandit.best_arm else \"\"\n",
    "    print(f\"  Arm {i}: {mean:.3f}{marker}\")\n",
    "print(f\"Optimal arm: {bandit.best_arm} (mean reward: {bandit.get_optimal_reward():.3f})\")\n",
    "print(\"Note: The agent doesn't know these true values!\")\n",
    "print(\"      It must learn them through experience.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Greedy Strategy and Its Fatal Flaw\n",
    "\n",
    "**What is the Greedy Strategy?**\n",
    "\n",
    "The simplest approach to the bandit problem is the **greedy strategy**: always choose the action that has the highest estimated value based on your experience so far.\n",
    "\n",
    "**How it works:**\n",
    "1. Keep track of the average reward received from each arm\n",
    "2. Always select the arm with the highest average reward\n",
    "3. Update the average after each pull\n",
    "\n",
    "**The Fatal Flaw:**\n",
    "\n",
    "The greedy strategy can easily get stuck on a suboptimal arm! Here's why:\n",
    "\n",
    "- Suppose you try arm 3 first and get lucky with a high reward\n",
    "- Now arm 3 has the highest estimated value\n",
    "- The greedy strategy will keep choosing arm 3 forever\n",
    "- You'll never discover that arm 7 is actually better!\n",
    "\n",
    "This is called **premature convergence** - the agent stops exploring too early and misses better options.\n",
    "\n",
    "Let's implement a greedy agent and see this problem in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyAgent:\n",
    "    \"\"\"An agent that always selects the arm with highest estimated value.\"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\"Initialize the agent.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of arms\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.q_estimates = np.zeros(k)  # Estimated value of each arm\n",
    "        self.action_counts = np.zeros(k)  # Number of times each arm was pulled\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select the arm with highest estimated value (greedy).\"\"\"\n",
    "        # Break ties randomly\n",
    "        max_value = np.max(self.q_estimates)\n",
    "        best_arms = np.where(self.q_estimates == max_value)[0]\n",
    "        return np.random.choice(best_arms)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimates after receiving a reward.\n",
    "        \n",
    "        Uses incremental average formula:\n",
    "        NewEstimate = OldEstimate + (1/n) * (Reward - OldEstimate)\n",
    "        \"\"\"\n",
    "        self.action_counts[action] += 1\n",
    "        n = self.action_counts[action]\n",
    "        \n",
    "        # Incremental update of average\n",
    "        self.q_estimates[action] += (1/n) * (reward - self.q_estimates[action])\n",
    "\n",
    "\n",
    "def run_experiment(agent, bandit, steps=1000):\n",
    "    \"\"\"Run an experiment with an agent on a bandit.\n",
    "    \n",
    "    Returns:\n",
    "        rewards: Array of rewards received at each step\n",
    "        optimal_actions: Array indicating if optimal action was chosen\n",
    "    \"\"\"\n",
    "    rewards = np.zeros(steps)\n",
    "    optimal_actions = np.zeros(steps)\n",
    "    \n",
    "    for t in range(steps):\n",
    "        # Agent selects action\n",
    "        action = agent.select_action()\n",
    "        \n",
    "        # Environment provides reward\n",
    "        reward = bandit.pull(action)\n",
    "        \n",
    "        # Agent updates its estimates\n",
    "        agent.update(action, reward)\n",
    "        \n",
    "        # Track results\n",
    "        rewards[t] = reward\n",
    "        optimal_actions[t] = 1 if action == bandit.best_arm else 0\n",
    "    \n",
    "    return rewards, optimal_actions\n",
    "\n",
    "\n",
    "# Demonstrate the greedy strategy's failure\n",
    "print(\"Demonstrating the Greedy Strategy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "greedy_agent = GreedyAgent(k=10)\n",
    "\n",
    "rewards, optimal_actions = run_experiment(greedy_agent, bandit, steps=1000)\n",
    "\n",
    "print(f\"After 1000 steps:\")\n",
    "print(f\"Arm selection counts:\")\n",
    "for i in range(bandit.k):\n",
    "    count = greedy_agent.action_counts[i]\n",
    "    estimate = greedy_agent.q_estimates[i]\n",
    "    true_value = bandit.true_means[i]\n",
    "    marker = \" \u2190 OPTIMAL\" if i == bandit.best_arm else \"\"\n",
    "    print(f\"  Arm {i}: pulled {count:4.0f} times | \"\n",
    "          f\"estimated value: {estimate:6.3f} | true value: {true_value:6.3f}{marker}\")\n",
    "\n",
    "optimal_pct = np.mean(optimal_actions) * 100\n",
    "avg_reward = np.mean(rewards)\n",
    "optimal_reward = bandit.get_optimal_reward()\n",
    "\n",
    "print(f\"Performance:\")\n",
    "print(f\"  Optimal action selected: {optimal_pct:.1f}% of the time\")\n",
    "print(f\"  Average reward: {avg_reward:.3f}\")\n",
    "print(f\"  Optimal reward: {optimal_reward:.3f}\")\n",
    "print(f\"  Regret: {optimal_reward - avg_reward:.3f}\")\n",
    "\n",
    "print(f\"\u26a0\ufe0f  Notice: The greedy agent likely got stuck on a suboptimal arm!\")\n",
    "print(f\"    It stopped exploring and missed the best option.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Greedy Strategy's Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple experiments to see the pattern\n",
    "num_experiments = 100\n",
    "steps = 1000\n",
    "\n",
    "all_rewards = np.zeros((num_experiments, steps))\n",
    "all_optimal = np.zeros((num_experiments, steps))\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(num_experiments):\n",
    "    bandit = MultiArmedBandit(k=10)\n",
    "    agent = GreedyAgent(k=10)\n",
    "    rewards, optimal = run_experiment(agent, bandit, steps)\n",
    "    all_rewards[i] = rewards\n",
    "    all_optimal[i] = optimal\n",
    "\n",
    "# Calculate averages across experiments\n",
    "avg_rewards = np.mean(all_rewards, axis=0)\n",
    "avg_optimal = np.mean(all_optimal, axis=0)\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Average reward over time\n",
    "ax1.plot(avg_rewards, linewidth=2, color='red', alpha=0.8)\n",
    "ax1.set_xlabel('Steps', fontsize=12)\n",
    "ax1.set_ylabel('Average Reward', fontsize=12)\n",
    "ax1.set_title('Greedy Strategy: Average Reward Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=np.mean(avg_rewards), color='red', linestyle='--', alpha=0.5, label=f'Mean: {np.mean(avg_rewards):.3f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Percentage of optimal actions\n",
    "ax2.plot(avg_optimal * 100, linewidth=2, color='red', alpha=0.8)\n",
    "ax2.set_xlabel('Steps', fontsize=12)\n",
    "ax2.set_ylabel('% Optimal Action', fontsize=12)\n",
    "ax2.set_title('Greedy Strategy: Percentage of Optimal Actions', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=np.mean(avg_optimal) * 100, color='red', linestyle='--', alpha=0.5, \n",
    "            label=f'Mean: {np.mean(avg_optimal)*100:.1f}%')\n",
    "ax2.legend()\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Interpretation:\")\n",
    "print(\"   - The greedy strategy quickly settles on an arm (often suboptimal)\")\n",
    "print(\"   - It rarely selects the optimal action because it stopped exploring\")\n",
    "print(\"   - Performance plateaus early and doesn't improve\")\n",
    "print(\"   - This demonstrates why pure exploitation fails!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Epsilon-Greedy Algorithm\n",
    "\n",
    "**A Simple Solution to the Exploration-Exploitation Dilemma**\n",
    "\n",
    "The epsilon-greedy algorithm provides a simple yet effective solution to the greedy strategy's fatal flaw. Instead of always exploiting, it introduces controlled exploration.\n",
    "\n",
    "**How Epsilon-Greedy Works:**\n",
    "\n",
    "With probability $\\epsilon$ (epsilon): Choose a **random** action (explore)\n",
    "\n",
    "With probability $1 - \\epsilon$: Choose the **best known** action (exploit)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "$$\n",
    "A_t = \\begin{cases}\n",
    "\\text{random action} & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_a Q_t(a) & \\text{with probability } 1 - \\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $Q_t(a)$ is the estimated value of action $a$ at time $t$.\n",
    "\n",
    "**Key Parameters:**\n",
    "\n",
    "- $\\epsilon = 0$: Pure exploitation (greedy strategy)\n",
    "- $\\epsilon = 1$: Pure exploration (random selection)\n",
    "- $\\epsilon = 0.1$: A common choice - explore 10% of the time\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to implement\n",
    "- Guarantees all actions are tried infinitely often (in the limit)\n",
    "- Balances exploration and exploitation\n",
    "\n",
    "**Trade-offs:**\n",
    "- Explores uniformly (doesn't prioritize promising actions)\n",
    "- Continues exploring even after finding the best action\n",
    "- Choice of $\\epsilon$ affects performance\n",
    "\n",
    "Let's implement the epsilon-greedy algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent:\n",
    "    \"\"\"An agent that uses epsilon-greedy action selection.\"\"\"\n",
    "    \n",
    "    def __init__(self, k, epsilon=0.1):\n",
    "        \"\"\"Initialize the agent.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of arms\n",
    "            epsilon: Probability of random exploration (0 to 1)\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.q_estimates = np.zeros(k)  # Estimated value of each arm\n",
    "        self.action_counts = np.zeros(k)  # Number of times each arm was pulled\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select action using epsilon-greedy strategy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: choose random action\n",
    "            return np.random.randint(0, self.k)\n",
    "        else:\n",
    "            # Exploit: choose best known action\n",
    "            max_value = np.max(self.q_estimates)\n",
    "            best_arms = np.where(self.q_estimates == max_value)[0]\n",
    "            return np.random.choice(best_arms)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimates after receiving a reward.\"\"\"\n",
    "        self.action_counts[action] += 1\n",
    "        n = self.action_counts[action]\n",
    "        \n",
    "        # Incremental update of average\n",
    "        self.q_estimates[action] += (1/n) * (reward - self.q_estimates[action])\n",
    "\n",
    "\n",
    "# Test epsilon-greedy with different epsilon values\n",
    "print(\"Epsilon-Greedy Algorithm Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "\n",
    "epsilon_values = [0.0, 0.01, 0.1, 0.3]\n",
    "results = {}\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    agent = EpsilonGreedyAgent(k=10, epsilon=eps)\n",
    "    rewards, optimal_actions = run_experiment(agent, bandit, steps=1000)\n",
    "    \n",
    "    results[eps] = {\n",
    "        'rewards': rewards,\n",
    "        'optimal': optimal_actions,\n",
    "        'avg_reward': np.mean(rewards),\n",
    "        'optimal_pct': np.mean(optimal_actions) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"\u03b5 = {eps:.2f}:\")\n",
    "    print(f\"  Average reward: {results[eps]['avg_reward']:.3f}\")\n",
    "    print(f\"  Optimal action: {results[eps]['optimal_pct']:.1f}% of the time\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83d\udca1 Key Insight:\")\n",
    "print(\"   - \u03b5 = 0.0 (greedy) gets stuck on suboptimal actions\")\n",
    "print(\"   - Small \u03b5 values (0.01-0.1) balance exploration and exploitation well\")\n",
    "print(\"   - Large \u03b5 values (0.3) explore too much and waste opportunities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Greedy vs Epsilon-Greedy Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive comparison across multiple experiments\n",
    "num_experiments = 200\n",
    "steps = 1000\n",
    "epsilon_values = [0.0, 0.01, 0.1, 0.3]\n",
    "\n",
    "# Store results for each epsilon value\n",
    "all_results = {eps: {'rewards': [], 'optimal': []} for eps in epsilon_values}\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(num_experiments):\n",
    "    bandit = MultiArmedBandit(k=10)\n",
    "    \n",
    "    for eps in epsilon_values:\n",
    "        agent = EpsilonGreedyAgent(k=10, epsilon=eps)\n",
    "        rewards, optimal = run_experiment(agent, bandit, steps)\n",
    "        all_results[eps]['rewards'].append(rewards)\n",
    "        all_results[eps]['optimal'].append(optimal)\n",
    "\n",
    "# Calculate averages\n",
    "avg_results = {}\n",
    "for eps in epsilon_values:\n",
    "    avg_results[eps] = {\n",
    "        'rewards': np.mean(all_results[eps]['rewards'], axis=0),\n",
    "        'optimal': np.mean(all_results[eps]['optimal'], axis=0)\n",
    "    }\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue']\n",
    "labels = [f'\u03b5 = {eps:.2f}' + (' (Greedy)' if eps == 0 else '') for eps in epsilon_values]\n",
    "\n",
    "# Plot 1: Average reward over time\n",
    "for eps, color, label in zip(epsilon_values, colors, labels):\n",
    "    ax1.plot(avg_results[eps]['rewards'], linewidth=2, color=color, alpha=0.8, label=label)\n",
    "\n",
    "ax1.set_xlabel('Steps', fontsize=12)\n",
    "ax1.set_ylabel('Average Reward', fontsize=12)\n",
    "ax1.set_title('Epsilon-Greedy: Average Reward Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Percentage of optimal actions\n",
    "for eps, color, label in zip(epsilon_values, colors, labels):\n",
    "    ax2.plot(avg_results[eps]['optimal'] * 100, linewidth=2, color=color, alpha=0.8, label=label)\n",
    "\n",
    "ax2.set_xlabel('Steps', fontsize=12)\n",
    "ax2.set_ylabel('% Optimal Action', fontsize=12)\n",
    "ax2.set_title('Epsilon-Greedy: Optimal Action Selection', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\ud83d\udcca Performance Summary (averaged over {} experiments):\".format(num_experiments))\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Strategy':<20} {'Avg Reward':<15} {'Optimal %':<15} {'Final Optimal %'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    strategy = f\"\u03b5 = {eps:.2f}\" + (\" (Greedy)\" if eps == 0 else \"\")\n",
    "    avg_reward = np.mean(avg_results[eps]['rewards'])\n",
    "    avg_optimal = np.mean(avg_results[eps]['optimal']) * 100\n",
    "    final_optimal = np.mean(avg_results[eps]['optimal'][-100:]) * 100  # Last 100 steps\n",
    "    \n",
    "    print(f\"{strategy:<20} {avg_reward:<15.3f} {avg_optimal:<15.1f} {final_optimal:.1f}%\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\u2705 Conclusions:\")\n",
    "print(\"   1. Greedy (\u03b5=0) performs poorly due to lack of exploration\")\n",
    "print(\"   2. Small epsilon (0.01-0.1) achieves good balance\")\n",
    "print(\"   3. \u03b5=0.1 typically performs best in this setting\")\n",
    "print(\"   4. Too much exploration (\u03b5=0.3) wastes opportunities to exploit\")\n",
    "print(\"   5. Epsilon-greedy successfully solves the exploration-exploitation dilemma!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimistic Initial Values: Exploration Through Disappointment\n",
    "\n",
    "**A Clever Alternative to Epsilon-Greedy**\n",
    "\n",
    "The Optimistic Initial Values approach provides a different solution to encourage exploration. Instead of randomly exploring, it uses **disappointment-driven exploration**.\n",
    "\n",
    "**The Key Idea:**\n",
    "\n",
    "Initialize all action-value estimates to be **optimistically high** (higher than any realistic reward). When the agent tries an action and receives a lower-than-expected reward, it becomes \"disappointed\" and tries other actions, naturally encouraging exploration.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "1. Set initial Q-values to a high value (e.g., +5 when true rewards are around 0-1)\n",
    "2. Use a greedy strategy (no epsilon needed!)\n",
    "3. Each action will initially seem promising\n",
    "4. After trying an action, its estimate decreases toward the true value\n",
    "5. The agent naturally tries all actions before settling on the best one\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "\n",
    "If we initialize $Q_0(a) = c$ for all actions where $c$ is large:\n",
    "\n",
    "$$Q_{n+1}(a) = Q_n(a) + \\frac{1}{n+1}(R_n - Q_n(a))$$\n",
    "\n",
    "Since $R_n < Q_n(a)$ initially, the estimate decreases, making other untried actions more attractive.\n",
    "\n",
    "**Advantages:**\n",
    "- No need to tune an epsilon parameter\n",
    "- Exploration happens naturally through the learning process\n",
    "- Simple to implement\n",
    "- Works well for stationary problems\n",
    "\n",
    "**Disadvantages:**\n",
    "- Only explores at the beginning (not suitable for non-stationary problems)\n",
    "- Requires knowing a good initial value\n",
    "- Less flexible than epsilon-greedy\n",
    "\n",
    "Let's implement and compare this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimisticGreedyAgent:\n",
    "    \"\"\"A greedy agent with optimistic initial value estimates.\"\"\"\n",
    "    \n",
    "    def __init__(self, k, initial_value=5.0):\n",
    "        \"\"\"Initialize the agent with optimistic values.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of arms\n",
    "            initial_value: Optimistic initial estimate for all actions\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.initial_value = initial_value\n",
    "        # Initialize all estimates optimistically\n",
    "        self.q_estimates = np.ones(k) * initial_value\n",
    "        self.action_counts = np.zeros(k)\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select action greedily (highest estimated value).\"\"\"\n",
    "        max_value = np.max(self.q_estimates)\n",
    "        best_arms = np.where(self.q_estimates == max_value)[0]\n",
    "        return np.random.choice(best_arms)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimates after receiving a reward.\"\"\"\n",
    "        self.action_counts[action] += 1\n",
    "        n = self.action_counts[action]\n",
    "        \n",
    "        # Incremental update\n",
    "        self.q_estimates[action] += (1/n) * (reward - self.q_estimates[action])\n",
    "\n",
    "\n",
    "# Demonstrate optimistic initial values\n",
    "print(\"Optimistic Initial Values Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "\n",
    "print(f\"True reward range: [{bandit.true_means.min():.2f}, {bandit.true_means.max():.2f}]\")\n",
    "print(f\"Optimal arm: {bandit.best_arm} (mean: {bandit.get_optimal_reward():.3f})\")\n",
    "\n",
    "# Test different initial values\n",
    "initial_values = [0.0, 2.0, 5.0, 10.0]\n",
    "optimistic_results = {}\n",
    "\n",
    "for init_val in initial_values:\n",
    "    agent = OptimisticGreedyAgent(k=10, initial_value=init_val)\n",
    "    rewards, optimal_actions = run_experiment(agent, bandit, steps=1000)\n",
    "    \n",
    "    optimistic_results[init_val] = {\n",
    "        'rewards': rewards,\n",
    "        'optimal': optimal_actions,\n",
    "        'avg_reward': np.mean(rewards),\n",
    "        'optimal_pct': np.mean(optimal_actions) * 100,\n",
    "        'agent': agent\n",
    "    }\n",
    "    \n",
    "    print(f\"Initial Value = {init_val:.1f}:\")\n",
    "    print(f\"  Average reward: {optimistic_results[init_val]['avg_reward']:.3f}\")\n",
    "    print(f\"  Optimal action: {optimistic_results[init_val]['optimal_pct']:.1f}% of the time\")\n",
    "    print(f\"  Final estimates: {agent.q_estimates}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udca1 Key Observations:\")\n",
    "print(\"   - Initial value = 0: Behaves like standard greedy (poor exploration)\")\n",
    "print(\"   - Initial value = 5-10: Encourages exploration through disappointment\")\n",
    "print(\"   - Higher initial values \u2192 more initial exploration\")\n",
    "print(\"   - Eventually converges to true values regardless of initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Optimistic Initial Values with Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison: Optimistic vs Epsilon-Greedy\n",
    "num_experiments = 200\n",
    "steps = 1000\n",
    "\n",
    "# Strategies to compare\n",
    "strategies = {\n",
    "    'Greedy (Q=0)': {'type': 'optimistic', 'init': 0.0},\n",
    "    'Optimistic (Q=5)': {'type': 'optimistic', 'init': 5.0},\n",
    "    '\u03b5-greedy (\u03b5=0.1)': {'type': 'epsilon', 'epsilon': 0.1},\n",
    "    '\u03b5-greedy (\u03b5=0.01)': {'type': 'epsilon', 'epsilon': 0.01}\n",
    "}\n",
    "\n",
    "comparison_results = {name: {'rewards': [], 'optimal': []} for name in strategies.keys()}\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(num_experiments):\n",
    "    bandit = MultiArmedBandit(k=10)\n",
    "    \n",
    "    for name, config in strategies.items():\n",
    "        if config['type'] == 'optimistic':\n",
    "            agent = OptimisticGreedyAgent(k=10, initial_value=config['init'])\n",
    "        else:\n",
    "            agent = EpsilonGreedyAgent(k=10, epsilon=config['epsilon'])\n",
    "        \n",
    "        rewards, optimal = run_experiment(agent, bandit, steps)\n",
    "        comparison_results[name]['rewards'].append(rewards)\n",
    "        comparison_results[name]['optimal'].append(optimal)\n",
    "\n",
    "# Calculate averages\n",
    "avg_comparison = {}\n",
    "for name in strategies.keys():\n",
    "    avg_comparison[name] = {\n",
    "        'rewards': np.mean(comparison_results[name]['rewards'], axis=0),\n",
    "        'optimal': np.mean(comparison_results[name]['optimal'], axis=0)\n",
    "    }\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "colors = ['red', 'purple', 'green', 'orange']\n",
    "linestyles = ['-', '-', '--', '--']\n",
    "\n",
    "# Plot 1: Average reward\n",
    "for (name, color, ls) in zip(strategies.keys(), colors, linestyles):\n",
    "    ax1.plot(avg_comparison[name]['rewards'], linewidth=2, color=color, \n",
    "             linestyle=ls, alpha=0.8, label=name)\n",
    "\n",
    "ax1.set_xlabel('Steps', fontsize=12)\n",
    "ax1.set_ylabel('Average Reward', fontsize=12)\n",
    "ax1.set_title('Optimistic Initial Values vs Epsilon-Greedy: Average Reward', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Optimal action percentage\n",
    "for (name, color, ls) in zip(strategies.keys(), colors, linestyles):\n",
    "    ax2.plot(avg_comparison[name]['optimal'] * 100, linewidth=2, color=color, \n",
    "             linestyle=ls, alpha=0.8, label=name)\n",
    "\n",
    "ax2.set_xlabel('Steps', fontsize=12)\n",
    "ax2.set_ylabel('% Optimal Action', fontsize=12)\n",
    "ax2.set_title('Optimistic Initial Values vs Epsilon-Greedy: Optimal Action Selection', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\ud83d\udcca Performance Comparison (averaged over {} experiments):\".format(num_experiments))\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Strategy':<25} {'Avg Reward':<15} {'Optimal %':<15} {'Early (0-100)':<15} {'Late (900-1000)'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name in strategies.keys():\n",
    "    avg_reward = np.mean(avg_comparison[name]['rewards'])\n",
    "    avg_optimal = np.mean(avg_comparison[name]['optimal']) * 100\n",
    "    early_optimal = np.mean(avg_comparison[name]['optimal'][:100]) * 100\n",
    "    late_optimal = np.mean(avg_comparison[name]['optimal'][-100:]) * 100\n",
    "    \n",
    "    print(f\"{name:<25} {avg_reward:<15.3f} {avg_optimal:<15.1f} {early_optimal:<15.1f} {late_optimal:.1f}%\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\u2705 Key Insights:\")\n",
    "print(\"   1. Optimistic initialization explores more early on\")\n",
    "print(\"   2. Epsilon-greedy maintains consistent exploration throughout\")\n",
    "print(\"   3. Optimistic approach eventually stops exploring (greedy after learning)\")\n",
    "print(\"   4. Both methods significantly outperform standard greedy\")\n",
    "print(\"   5. Choice depends on problem: stationary \u2192 optimistic, non-stationary \u2192 epsilon-greedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upper Confidence Bound (UCB): Uncertainty-Driven Exploration\n",
    "\n",
    "**The Most Sophisticated Bandit Algorithm**\n",
    "\n",
    "The Upper Confidence Bound (UCB) algorithm represents a more principled approach to the exploration-exploitation dilemma. Instead of exploring randomly (epsilon-greedy) or through disappointment (optimistic initialization), UCB explores based on **uncertainty**.\n",
    "\n",
    "**The Core Principle:**\n",
    "\n",
    "\"It's reasonable to be optimistic in the face of uncertainty.\"\n",
    "\n",
    "UCB selects actions based on both:\n",
    "1. **Estimated value** (exploitation)\n",
    "2. **Uncertainty in that estimate** (exploration)\n",
    "\n",
    "Actions that have been tried less often have higher uncertainty, making them more attractive for exploration.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "The UCB action selection rule is:\n",
    "\n",
    "$$A_t = \\arg\\max_a \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]$$\n",
    "\n",
    "where:\n",
    "- $Q_t(a)$ = estimated value of action $a$ at time $t$ (exploitation term)\n",
    "- $c$ = exploration parameter (controls degree of exploration)\n",
    "- $t$ = current time step (total number of actions taken)\n",
    "- $N_t(a)$ = number of times action $a$ has been selected (uncertainty term)\n",
    "- $\\sqrt{\\frac{\\ln t}{N_t(a)}}$ = uncertainty bonus (larger for less-tried actions)\n",
    "\n",
    "**How the Uncertainty Bonus Works:**\n",
    "\n",
    "- Actions tried fewer times have larger $\\sqrt{\\frac{\\ln t}{N_t(a)}}$ (more uncertainty)\n",
    "- As an action is tried more, $N_t(a)$ increases and the bonus decreases\n",
    "- The $\\ln t$ term ensures all actions are eventually tried\n",
    "- The bonus naturally balances exploration and exploitation\n",
    "\n",
    "**Advantages:**\n",
    "- No random exploration - deterministic given the history\n",
    "- Automatically balances exploration and exploitation\n",
    "- Theoretical guarantees on performance (logarithmic regret)\n",
    "- Prioritizes promising actions while ensuring all are tried\n",
    "\n",
    "**Disadvantages:**\n",
    "- More complex to implement\n",
    "- Requires tuning the $c$ parameter\n",
    "- Assumes stationary reward distributions\n",
    "\n",
    "Let's implement UCB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent:\n",
    "    \"\"\"An agent using Upper Confidence Bound action selection.\"\"\"\n",
    "    \n",
    "    def __init__(self, k, c=2.0):\n",
    "        \"\"\"Initialize the UCB agent.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of arms\n",
    "            c: Exploration parameter (typically sqrt(2) or 2)\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.c = c\n",
    "        self.q_estimates = np.zeros(k)\n",
    "        self.action_counts = np.zeros(k)\n",
    "        self.t = 0  # Total time steps\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select action using UCB formula.\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # First, try each action at least once\n",
    "        if self.t <= self.k:\n",
    "            return self.t - 1\n",
    "        \n",
    "        # Calculate UCB values for all actions\n",
    "        ucb_values = np.zeros(self.k)\n",
    "        for a in range(self.k):\n",
    "            if self.action_counts[a] == 0:\n",
    "                # Untried actions get infinite value (shouldn't happen after initial phase)\n",
    "                ucb_values[a] = float('inf')\n",
    "            else:\n",
    "                # UCB formula: Q(a) + c * sqrt(ln(t) / N(a))\n",
    "                exploitation = self.q_estimates[a]\n",
    "                exploration = self.c * np.sqrt(np.log(self.t) / self.action_counts[a])\n",
    "                ucb_values[a] = exploitation + exploration\n",
    "        \n",
    "        # Select action with highest UCB value\n",
    "        max_ucb = np.max(ucb_values)\n",
    "        best_actions = np.where(ucb_values == max_ucb)[0]\n",
    "        return np.random.choice(best_actions)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimates after receiving a reward.\"\"\"\n",
    "        self.action_counts[action] += 1\n",
    "        n = self.action_counts[action]\n",
    "        \n",
    "        # Incremental update\n",
    "        self.q_estimates[action] += (1/n) * (reward - self.q_estimates[action])\n",
    "\n",
    "\n",
    "# Demonstrate UCB algorithm\n",
    "print(\"Upper Confidence Bound (UCB) Algorithm Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "\n",
    "print(f\"Optimal arm: {bandit.best_arm} (mean: {bandit.get_optimal_reward():.3f})\")\n",
    "\n",
    "# Test different c values\n",
    "c_values = [0.5, 1.0, 2.0, 4.0]\n",
    "ucb_results = {}\n",
    "\n",
    "for c in c_values:\n",
    "    agent = UCBAgent(k=10, c=c)\n",
    "    rewards, optimal_actions = run_experiment(agent, bandit, steps=1000)\n",
    "    \n",
    "    ucb_results[c] = {\n",
    "        'rewards': rewards,\n",
    "        'optimal': optimal_actions,\n",
    "        'avg_reward': np.mean(rewards),\n",
    "        'optimal_pct': np.mean(optimal_actions) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"c = {c:.1f}:\")\n",
    "    print(f\"  Average reward: {ucb_results[c]['avg_reward']:.3f}\")\n",
    "    print(f\"  Optimal action: {ucb_results[c]['optimal_pct']:.1f}% of the time\")\n",
    "    print(f\"  Action counts: {agent.action_counts.astype(int)}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udca1 Key Observations:\")\n",
    "print(\"   - Small c (0.5): Less exploration, may converge faster but risk suboptimal\")\n",
    "print(\"   - Medium c (1-2): Good balance, typical choice\")\n",
    "print(\"   - Large c (4): More exploration, ensures thorough search\")\n",
    "print(\"   - UCB naturally tries all actions but focuses on promising ones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprehensive Comparison: All Three Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive comparison of all strategies\n",
    "num_experiments = 200\n",
    "steps = 1000\n",
    "\n",
    "# All strategies to compare\n",
    "all_strategies = {\n",
    "    'Greedy': {'type': 'greedy'},\n",
    "    '\u03b5-greedy (\u03b5=0.1)': {'type': 'epsilon', 'epsilon': 0.1},\n",
    "    '\u03b5-greedy (\u03b5=0.01)': {'type': 'epsilon', 'epsilon': 0.01},\n",
    "    'Optimistic (Q=5)': {'type': 'optimistic', 'init': 5.0},\n",
    "    'UCB (c=2)': {'type': 'ucb', 'c': 2.0},\n",
    "    'UCB (c=1)': {'type': 'ucb', 'c': 1.0}\n",
    "}\n",
    "\n",
    "final_results = {name: {'rewards': [], 'optimal': []} for name in all_strategies.keys()}\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(num_experiments):\n",
    "    bandit = MultiArmedBandit(k=10)\n",
    "    \n",
    "    for name, config in all_strategies.items():\n",
    "        if config['type'] == 'greedy':\n",
    "            agent = GreedyAgent(k=10)\n",
    "        elif config['type'] == 'epsilon':\n",
    "            agent = EpsilonGreedyAgent(k=10, epsilon=config['epsilon'])\n",
    "        elif config['type'] == 'optimistic':\n",
    "            agent = OptimisticGreedyAgent(k=10, initial_value=config['init'])\n",
    "        else:  # ucb\n",
    "            agent = UCBAgent(k=10, c=config['c'])\n",
    "        \n",
    "        rewards, optimal = run_experiment(agent, bandit, steps)\n",
    "        final_results[name]['rewards'].append(rewards)\n",
    "        final_results[name]['optimal'].append(optimal)\n",
    "\n",
    "# Calculate averages\n",
    "avg_final = {}\n",
    "for name in all_strategies.keys():\n",
    "    avg_final[name] = {\n",
    "        'rewards': np.mean(final_results[name]['rewards'], axis=0),\n",
    "        'optimal': np.mean(final_results[name]['optimal'], axis=0)\n",
    "    }\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 11))\n",
    "\n",
    "colors = ['red', 'green', 'lightgreen', 'purple', 'blue', 'lightblue']\n",
    "linestyles = ['-', '-', '--', '-', '-', '--']\n",
    "linewidths = [2, 2.5, 2, 2, 2.5, 2]\n",
    "\n",
    "# Plot 1: Average reward over time\n",
    "for (name, color, ls, lw) in zip(all_strategies.keys(), colors, linestyles, linewidths):\n",
    "    ax1.plot(avg_final[name]['rewards'], linewidth=lw, color=color, \n",
    "             linestyle=ls, alpha=0.8, label=name)\n",
    "\n",
    "ax1.set_xlabel('Steps', fontsize=13)\n",
    "ax1.set_ylabel('Average Reward', fontsize=13)\n",
    "ax1.set_title('Multi-Armed Bandit: Complete Strategy Comparison - Average Reward', \n",
    "              fontsize=15, fontweight='bold')\n",
    "ax1.legend(loc='lower right', fontsize=11, ncol=2)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Percentage of optimal actions\n",
    "for (name, color, ls, lw) in zip(all_strategies.keys(), colors, linestyles, linewidths):\n",
    "    ax2.plot(avg_final[name]['optimal'] * 100, linewidth=lw, color=color, \n",
    "             linestyle=ls, alpha=0.8, label=name)\n",
    "\n",
    "ax2.set_xlabel('Steps', fontsize=13)\n",
    "ax2.set_ylabel('% Optimal Action', fontsize=13)\n",
    "ax2.set_title('Multi-Armed Bandit: Complete Strategy Comparison - Optimal Action Selection', \n",
    "              fontsize=15, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=11, ncol=2)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed performance table\n",
    "print(\"\ud83d\udcca Final Performance Comparison (averaged over {} experiments):\".format(num_experiments))\n",
    "print(\"=\"*95)\n",
    "print(f\"{'Strategy':<25} {'Avg Reward':<15} {'Total Optimal %':<18} {'Early (0-100)':<18} {'Late (900-1000)'}\")\n",
    "print(\"=\"*95)\n",
    "\n",
    "# Sort by average reward for ranking\n",
    "sorted_strategies = sorted(all_strategies.keys(), \n",
    "                          key=lambda x: np.mean(avg_final[x]['rewards']), \n",
    "                          reverse=True)\n",
    "\n",
    "for rank, name in enumerate(sorted_strategies, 1):\n",
    "    avg_reward = np.mean(avg_final[name]['rewards'])\n",
    "    avg_optimal = np.mean(avg_final[name]['optimal']) * 100\n",
    "    early_optimal = np.mean(avg_final[name]['optimal'][:100]) * 100\n",
    "    late_optimal = np.mean(avg_final[name]['optimal'][-100:]) * 100\n",
    "    \n",
    "    rank_str = f\"#{rank} {name}\"\n",
    "    print(f\"{rank_str:<25} {avg_reward:<15.3f} {avg_optimal:<18.1f} {early_optimal:<18.1f} {late_optimal:.1f}%\")\n",
    "\n",
    "print(\"=\"*95)\n",
    "\n",
    "print(\"\ud83c\udfc6 Final Rankings and Insights:\")\n",
    "print(\"1. UCB (c=2) typically performs best overall\")\n",
    "print(\"   - Principled exploration based on uncertainty\")\n",
    "print(\"   - Strong theoretical guarantees\")\n",
    "print(\"   - No random exploration needed\")\n",
    "\n",
    "print(\"2. \u03b5-greedy (\u03b5=0.1) is a close second\")\n",
    "print(\"   - Simple and effective\")\n",
    "print(\"   - Works well in non-stationary environments\")\n",
    "print(\"   - Easy to implement and tune\")\n",
    "\n",
    "print(\"3. Optimistic initialization works well early\")\n",
    "print(\"   - Good for stationary problems\")\n",
    "print(\"   - No parameter tuning needed\")\n",
    "print(\"   - Exploration decreases over time\")\n",
    "\n",
    "print(\"4. Pure greedy fails dramatically\")\n",
    "print(\"   - Gets stuck on first good option\")\n",
    "print(\"   - Demonstrates importance of exploration\")\n",
    "\n",
    "print(\"\ud83d\udca1 Key Takeaway:\")\n",
    "print(\"   The exploration-exploitation dilemma is fundamental to RL.\")\n",
    "print(\"   Different strategies offer different trade-offs, but all successful\")\n",
    "print(\"   approaches balance trying new things with using what works.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mdp'></a>\n",
    "### Core Terminology and MDP Framework\n",
    "\n",
    "Now that we've explored the multi-armed bandit problem, let's expand our understanding to more complex reinforcement learning scenarios. We'll introduce the fundamental terminology and the Markov Decision Process (MDP) framework that underlies most RL algorithms.\n",
    "\n",
    "#### Fundamental RL Terminology\n",
    "\n",
    "Before diving into MDPs, let's clearly define the core concepts that appear in every RL problem:\n",
    "\n",
    "**1. Agent**\n",
    "- The learner and decision maker\n",
    "- Observes the environment and takes actions\n",
    "- Goal: Learn a policy that maximizes cumulative reward\n",
    "- Example: A robot, a game-playing AI, a trading algorithm\n",
    "\n",
    "**2. Environment**\n",
    "- Everything outside the agent\n",
    "- Responds to the agent's actions\n",
    "- Provides observations and rewards\n",
    "- Example: The physical world, a game board, a stock market\n",
    "\n",
    "**3. State (s)**\n",
    "- A representation of the current situation\n",
    "- Contains all relevant information for decision making\n",
    "- Can be fully observable or partially observable\n",
    "- Example: Robot's position and velocity, chess board configuration, account balance\n",
    "\n",
    "**4. Action (a)**\n",
    "- A choice the agent can make\n",
    "- Can be discrete (finite set) or continuous (infinite range)\n",
    "- Available actions may depend on the current state\n",
    "- Example: Move left/right, place chess piece, buy/sell/hold\n",
    "\n",
    "**5. Reward (r)**\n",
    "- Immediate feedback signal from the environment\n",
    "- Scalar value indicating how good/bad an action was\n",
    "- The agent's goal is to maximize cumulative reward\n",
    "- Example: +1 for reaching goal, -1 for collision, profit/loss amount\n",
    "\n",
    "**The Agent-Environment Interface:**\n",
    "\n",
    "```\n",
    "     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "     \u2502  Agent  \u2502\n",
    "     \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502\n",
    "    action\u2502 \u2193\n",
    "     \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "     \u2502 Environment \u2502\n",
    "     \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502\n",
    "  state,  \u2502 \u2191\n",
    "  reward  \u2502\n",
    "     \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n",
    "     \u2502  Agent  \u2502\n",
    "     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "Let's implement a simple environment class to demonstrate these concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnvironment:\n",
    "    \"\"\"A simple grid world environment demonstrating RL concepts.\n",
    "    \n",
    "    The agent navigates a 2D grid to reach a goal while avoiding obstacles.\n",
    "    This demonstrates: states (grid positions), actions (movements),\n",
    "    rewards (goal/obstacle/step), and the agent-environment interaction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=5, goal_pos=(4, 4), obstacles=None):\n",
    "        \"\"\"Initialize the grid world.\n",
    "        \n",
    "        Args:\n",
    "            grid_size: Size of the square grid\n",
    "            goal_pos: (row, col) position of the goal\n",
    "            obstacles: List of (row, col) positions that are obstacles\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.goal_pos = goal_pos\n",
    "        self.obstacles = obstacles if obstacles else [(2, 2), (3, 2)]\n",
    "        \n",
    "        # Action space: 0=up, 1=right, 2=down, 3=left\n",
    "        self.actions = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "        self.action_effects = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        \n",
    "        # Initialize state\n",
    "        self.agent_pos = None\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state.\n",
    "        \n",
    "        Returns:\n",
    "            state: Initial state (agent position)\n",
    "        \"\"\"\n",
    "        # Start at top-left corner\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.agent_pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute an action and return the result.\n",
    "        \n",
    "        Args:\n",
    "            action: Integer 0-3 representing direction\n",
    "            \n",
    "        Returns:\n",
    "            next_state: New agent position\n",
    "            reward: Reward for this transition\n",
    "            done: Whether episode is finished\n",
    "            info: Additional information (dict)\n",
    "        \"\"\"\n",
    "        # Calculate new position\n",
    "        delta = self.action_effects[action]\n",
    "        new_row = self.agent_pos[0] + delta[0]\n",
    "        new_col = self.agent_pos[1] + delta[1]\n",
    "        new_pos = (new_row, new_col)\n",
    "        \n",
    "        # Check if new position is valid\n",
    "        if self._is_valid_position(new_pos):\n",
    "            self.agent_pos = new_pos\n",
    "        # If invalid (wall), agent stays in place\n",
    "        \n",
    "        # Calculate reward and check if done\n",
    "        reward, done, info = self._get_reward_and_done()\n",
    "        \n",
    "        return self.agent_pos, reward, done, info\n",
    "    \n",
    "    def _is_valid_position(self, pos):\n",
    "        \"\"\"Check if position is within bounds and not an obstacle.\"\"\"\n",
    "        row, col = pos\n",
    "        \n",
    "        # Check bounds\n",
    "        if row < 0 or row >= self.grid_size or col < 0 or col >= self.grid_size:\n",
    "            return False\n",
    "        \n",
    "        # Check obstacles\n",
    "        if pos in self.obstacles:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _get_reward_and_done(self):\n",
    "        \"\"\"Calculate reward and check if episode is done.\"\"\"\n",
    "        info = {}\n",
    "        \n",
    "        # Check if reached goal\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            return 10.0, True, {'reason': 'goal_reached'}\n",
    "        \n",
    "        # Check if hit obstacle (shouldn't happen with valid position check)\n",
    "        if self.agent_pos in self.obstacles:\n",
    "            return -10.0, True, {'reason': 'obstacle_hit'}\n",
    "        \n",
    "        # Small negative reward for each step (encourages efficiency)\n",
    "        return -0.1, False, {'reason': 'step'}\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Display the current state of the grid world.\"\"\"\n",
    "        grid = [['.' for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n",
    "        \n",
    "        # Mark obstacles\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs[0]][obs[1]] = 'X'\n",
    "        \n",
    "        # Mark goal\n",
    "        grid[self.goal_pos[0]][self.goal_pos[1]] = 'G'\n",
    "        \n",
    "        # Mark agent\n",
    "        grid[self.agent_pos[0]][self.agent_pos[1]] = 'A'\n",
    "        \n",
    "        # Print grid\n",
    "        print('' + '\u2500' * (self.grid_size * 2 + 1))\n",
    "        for row in grid:\n",
    "            print('\u2502' + ' '.join(row) + '\u2502')\n",
    "        print('\u2500' * (self.grid_size * 2 + 1))\n",
    "        print(f\"Agent at: {self.agent_pos}\")\n",
    "\n",
    "\n",
    "# Demonstrate the environment and core concepts\n",
    "print(\"Demonstrating Core RL Concepts with Grid World\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnvironment(grid_size=5)\n",
    "\n",
    "print(\"\ud83c\udf0d ENVIRONMENT: 5x5 Grid World\")\n",
    "print(\"   - Goal: Reach position (4,4) marked with 'G'\")\n",
    "print(\"   - Obstacles: Positions marked with 'X'\")\n",
    "print(\"   - Agent: Current position marked with 'A'\")\n",
    "\n",
    "print(\"\ud83d\udccd STATE: Agent's position in the grid (row, col)\")\n",
    "print(f\"   - Initial state: {env.agent_pos}\")\n",
    "print(f\"   - State space size: {env.grid_size * env.grid_size} possible positions\")\n",
    "\n",
    "print(\"\ud83c\udfae ACTIONS: Four possible movements\")\n",
    "for i, action_name in enumerate(env.actions):\n",
    "    print(f\"   - Action {i}: {action_name}\")\n",
    "\n",
    "print(\"\ud83c\udf81 REWARDS:\")\n",
    "print(\"   - Reach goal: +10.0\")\n",
    "print(\"   - Each step: -0.1 (encourages efficiency)\")\n",
    "print(\"   - Hit wall: Agent stays in place\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"Initial State:\")\n",
    "env.render()\n",
    "\n",
    "# Simulate a few steps\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"Simulating Agent-Environment Interaction:\")\n",
    "\n",
    "actions_to_take = [1, 1, 2, 2, 1, 1, 2, 2]  # Path to goal\n",
    "total_reward = 0\n",
    "\n",
    "for step, action in enumerate(actions_to_take, 1):\n",
    "    action_name = env.actions[action]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    print(f\"Step {step}:\")\n",
    "    print(f\"  Action: {action_name}\")\n",
    "    print(f\"  New State: {next_state}\")\n",
    "    print(f\"  Reward: {reward:+.1f}\")\n",
    "    print(f\"  Total Reward: {total_reward:+.1f}\")\n",
    "    print(f\"  Done: {done}\")\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\u2713 Episode finished: {info['reason']}\")\n",
    "        env.render()\n",
    "        break\n",
    "    print()\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83d\udca1 Key Observations:\")\n",
    "print(\"   1. STATE: Represents where the agent is\")\n",
    "print(\"   2. ACTION: What the agent chooses to do\")\n",
    "print(\"   3. REWARD: Feedback on how good the action was\")\n",
    "print(\"   4. ENVIRONMENT: Determines next state and reward\")\n",
    "print(\"   5. AGENT: Would learn which actions to take in each state\")\n",
    "print(\"   This interaction loop is the foundation of all RL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Agent-Environment Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of the agent-environment interaction\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_grid_world(env, trajectory=None):\n",
    "    \"\"\"Visualize the grid world and optionally a trajectory.\n",
    "    \n",
    "    Args:\n",
    "        env: GridWorldEnvironment instance\n",
    "        trajectory: List of (state, action) tuples to visualize\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Draw grid\n",
    "    for i in range(env.grid_size + 1):\n",
    "        ax.plot([0, env.grid_size], [i, i], 'k-', linewidth=1)\n",
    "        ax.plot([i, i], [0, env.grid_size], 'k-', linewidth=1)\n",
    "    \n",
    "    # Draw obstacles\n",
    "    for obs in env.obstacles:\n",
    "        rect = patches.Rectangle((obs[1], env.grid_size - obs[0] - 1), 1, 1, \n",
    "                                 linewidth=2, edgecolor='black', facecolor='gray', alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(obs[1] + 0.5, env.grid_size - obs[0] - 0.5, 'X', \n",
    "               ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # Draw goal\n",
    "    goal = env.goal_pos\n",
    "    rect = patches.Rectangle((goal[1], env.grid_size - goal[0] - 1), 1, 1, \n",
    "                             linewidth=2, edgecolor='green', facecolor='lightgreen', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(goal[1] + 0.5, env.grid_size - goal[0] - 0.5, 'G', \n",
    "           ha='center', va='center', fontsize=20, fontweight='bold', color='darkgreen')\n",
    "    \n",
    "    # Draw start position\n",
    "    start = (0, 0)\n",
    "    rect = patches.Rectangle((start[1], env.grid_size - start[0] - 1), 1, 1, \n",
    "                             linewidth=2, edgecolor='blue', facecolor='lightblue', alpha=0.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(start[1] + 0.5, env.grid_size - start[0] - 0.5, 'S', \n",
    "           ha='center', va='center', fontsize=20, fontweight='bold', color='darkblue')\n",
    "    \n",
    "    # Draw trajectory if provided\n",
    "    if trajectory:\n",
    "        for i, (state, action) in enumerate(trajectory):\n",
    "            row, col = state\n",
    "            # Convert to plot coordinates\n",
    "            x = col + 0.5\n",
    "            y = env.grid_size - row - 0.5\n",
    "            \n",
    "            # Draw step number\n",
    "            ax.text(x, y, str(i+1), ha='center', va='center', \n",
    "                   fontsize=12, color='red', fontweight='bold',\n",
    "                   bbox=dict(boxstyle='circle', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Draw arrow for action\n",
    "            if action is not None:\n",
    "                delta = env.action_effects[action]\n",
    "                dx = delta[1] * 0.3\n",
    "                dy = -delta[0] * 0.3  # Negative because y-axis is flipped\n",
    "                ax.arrow(x, y, dx, dy, head_width=0.15, head_length=0.1, \n",
    "                        fc='red', ec='red', alpha=0.6, linewidth=2)\n",
    "    \n",
    "    ax.set_xlim(0, env.grid_size)\n",
    "    ax.set_ylim(0, env.grid_size)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xticks(range(env.grid_size + 1))\n",
    "    ax.set_yticks(range(env.grid_size + 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title('Grid World EnvironmentS=Start, G=Goal, X=Obstacle', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# Visualize the environment\n",
    "env = GridWorldEnvironment(grid_size=5)\n",
    "\n",
    "# Create a sample trajectory\n",
    "trajectory = []\n",
    "state = env.reset()\n",
    "actions = [1, 1, 2, 2, 1, 1, 2, 2]  # Path to goal\n",
    "\n",
    "for action in actions:\n",
    "    trajectory.append((state, action))\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        trajectory.append((state, None))  # Final state, no action\n",
    "        break\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = visualize_grid_world(env, trajectory)\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Visualization shows:\")\n",
    "print(\"   - Blue 'S': Starting state\")\n",
    "print(\"   - Green 'G': Goal state\")\n",
    "print(\"   - Gray 'X': Obstacles\")\n",
    "print(\"   - Red numbers: Step sequence\")\n",
    "print(\"   - Red arrows: Actions taken\")\n",
    "print(\"This illustrates how the agent navigates through states\")\n",
    "print(\"by taking actions to reach the goal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Decision Processes (MDPs)\n",
    "\n",
    "**The Mathematical Framework for Reinforcement Learning**\n",
    "\n",
    "Now that we understand the basic terminology, let's formalize these concepts using the **Markov Decision Process (MDP)** framework. MDPs provide the mathematical foundation for most reinforcement learning algorithms.\n",
    "\n",
    "**What is an MDP?**\n",
    "\n",
    "A Markov Decision Process is a mathematical model for sequential decision-making under uncertainty. It's defined by a tuple $(S, A, P, R, \\gamma)$:\n",
    "\n",
    "**MDP Components:**\n",
    "\n",
    "1. **$S$: State Space**\n",
    "   - Set of all possible states\n",
    "   - Can be finite (grid positions) or infinite (continuous positions)\n",
    "   - Example: $S = \\{(0,0), (0,1), ..., (4,4)\\}$ for 5\u00d75 grid\n",
    "\n",
    "2. **$A$: Action Space**\n",
    "   - Set of all possible actions\n",
    "   - Can be state-dependent: $A(s)$ = actions available in state $s$\n",
    "   - Example: $A = \\{\\text{UP, DOWN, LEFT, RIGHT}\\}$\n",
    "\n",
    "3. **$P$: Transition Probability Function**\n",
    "   - $P(s'|s,a)$ = probability of reaching state $s'$ from state $s$ after taking action $a$\n",
    "   - Defines the dynamics of the environment\n",
    "   - Must satisfy: $\\sum_{s' \\in S} P(s'|s,a) = 1$ for all $s, a$\n",
    "   - Example: In deterministic grid world, $P(s'|s,a) = 1$ for one $s'$ and 0 for others\n",
    "\n",
    "4. **$R$: Reward Function**\n",
    "   - $R(s, a, s')$ = immediate reward for transition from $s$ to $s'$ via action $a$\n",
    "   - Sometimes simplified as $R(s)$ or $R(s,a)$\n",
    "   - Defines the objective the agent should optimize\n",
    "   - Example: $R(s_{goal}) = +10$, $R(s_{other}) = -0.1$\n",
    "\n",
    "5. **$\\gamma$: Discount Factor**\n",
    "   - Value between 0 and 1 that determines importance of future rewards\n",
    "   - $\\gamma = 0$: Only immediate rewards matter (myopic)\n",
    "   - $\\gamma = 1$: All future rewards equally important (far-sighted)\n",
    "   - Typical values: 0.9, 0.95, 0.99\n",
    "\n",
    "**The Markov Property**\n",
    "\n",
    "The \"Markov\" in MDP refers to the **Markov Property** (also called the memoryless property):\n",
    "\n",
    "$P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1} | s_t, a_t)$\n",
    "\n",
    "**In plain English:** The future depends only on the present, not on the past.\n",
    "\n",
    "**Why is the Markov Property Important?**\n",
    "\n",
    "1. **Tractability**: Makes the problem computationally feasible\n",
    "   - Don't need to remember entire history\n",
    "   - State contains all relevant information\n",
    "\n",
    "2. **Simplifies Learning**: Agent only needs to learn from current state\n",
    "   - No need to condition on past states\n",
    "   - Enables dynamic programming and temporal difference learning\n",
    "\n",
    "3. **Theoretical Guarantees**: Most RL theory assumes Markov property\n",
    "   - Convergence proofs rely on it\n",
    "   - Optimal policies exist under this assumption\n",
    "\n",
    "**Example - Chess:**\n",
    "- **Markov**: Current board position is the state (contains all relevant info)\n",
    "- **Non-Markov**: Only knowing the last move (need full game history)\n",
    "\n",
    "**When the Markov Property Doesn't Hold:**\n",
    "\n",
    "In practice, many problems are **Partially Observable MDPs (POMDPs)** where:\n",
    "- Agent doesn't see the full state\n",
    "- Must infer state from observations\n",
    "- Example: Robot with limited sensors, poker (can't see opponent's cards)\n",
    "\n",
    "Let's implement an MDP simulator to demonstrate these concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMDP:\n",
    "    \"\"\"A simple MDP simulator with explicit transition probabilities.\n",
    "    \n",
    "    This class demonstrates the core MDP components:\n",
    "    - State space S\n",
    "    - Action space A  \n",
    "    - Transition probabilities P(s'|s,a)\n",
    "    - Reward function R(s,a,s')\n",
    "    - Discount factor gamma\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, states, actions, transitions, rewards, gamma=0.9):\n",
    "        \"\"\"Initialize the MDP.\n",
    "        \n",
    "        Args:\n",
    "            states: List of state identifiers\n",
    "            actions: List of action identifiers\n",
    "            transitions: Dict mapping (state, action) -> {next_state: probability}\n",
    "            rewards: Dict mapping (state, action, next_state) -> reward\n",
    "            gamma: Discount factor (0 to 1)\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.current_state = None\n",
    "        \n",
    "        # Verify transition probabilities sum to 1\n",
    "        self._verify_transitions()\n",
    "    \n",
    "    def _verify_transitions(self):\n",
    "        \"\"\"Verify that transition probabilities are valid.\"\"\"\n",
    "        for (state, action), next_states in self.transitions.items():\n",
    "            total_prob = sum(next_states.values())\n",
    "            if not np.isclose(total_prob, 1.0):\n",
    "                raise ValueError(\n",
    "                    f\"Transition probabilities for ({state}, {action}) sum to {total_prob}, not 1.0\"\n",
    "                )\n",
    "    \n",
    "    def reset(self, initial_state=None):\n",
    "        \"\"\"Reset to initial state.\n",
    "        \n",
    "        Args:\n",
    "            initial_state: Starting state (random if None)\n",
    "            \n",
    "        Returns:\n",
    "            state: The initial state\n",
    "        \"\"\"\n",
    "        if initial_state is None:\n",
    "            self.current_state = np.random.choice(self.states)\n",
    "        else:\n",
    "            self.current_state = initial_state\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and transition to next state.\n",
    "        \n",
    "        Args:\n",
    "            action: Action to take\n",
    "            \n",
    "        Returns:\n",
    "            next_state: The resulting state\n",
    "            reward: Reward received\n",
    "            info: Additional information\n",
    "        \"\"\"\n",
    "        if self.current_state is None:\n",
    "            raise ValueError(\"Must call reset() before step()\")\n",
    "        \n",
    "        # Get transition probabilities for current state and action\n",
    "        next_state_probs = self.transitions.get((self.current_state, action), {})\n",
    "        \n",
    "        if not next_state_probs:\n",
    "            raise ValueError(f\"No transitions defined for state {self.current_state}, action {action}\")\n",
    "        \n",
    "        # Sample next state according to transition probabilities\n",
    "        next_states = list(next_state_probs.keys())\n",
    "        probabilities = list(next_state_probs.values())\n",
    "        next_state = np.random.choice(next_states, p=probabilities)\n",
    "        \n",
    "        # Get reward\n",
    "        reward = self.rewards.get((self.current_state, action, next_state), 0.0)\n",
    "        \n",
    "        # Update current state\n",
    "        old_state = self.current_state\n",
    "        self.current_state = next_state\n",
    "        \n",
    "        info = {\n",
    "            'old_state': old_state,\n",
    "            'action': action,\n",
    "            'probability': next_state_probs[next_state]\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, info\n",
    "    \n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\"Get P(next_state | state, action).\"\"\"\n",
    "        return self.transitions.get((state, action), {}).get(next_state, 0.0)\n",
    "    \n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Get R(state, action, next_state).\"\"\"\n",
    "        return self.rewards.get((state, action, next_state), 0.0)\n",
    "\n",
    "\n",
    "# Create a simple 2x2 grid world MDP\n",
    "print(\"Simple 2x2 Grid World MDP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define the MDP components\n",
    "# States: positions in 2x2 grid\n",
    "states = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\n",
    "\n",
    "# Actions: move right or down\n",
    "actions = ['RIGHT', 'DOWN']\n",
    "\n",
    "# Transitions: P(s'|s,a)\n",
    "# In this simple example, actions are deterministic\n",
    "transitions = {\n",
    "    ('(0,0)', 'RIGHT'): {'(0,1)': 1.0},\n",
    "    ('(0,0)', 'DOWN'): {'(1,0)': 1.0},\n",
    "    ('(0,1)', 'RIGHT'): {'(0,1)': 1.0},  # Hit wall, stay in place\n",
    "    ('(0,1)', 'DOWN'): {'(1,1)': 1.0},\n",
    "    ('(1,0)', 'RIGHT'): {'(1,1)': 1.0},\n",
    "    ('(1,0)', 'DOWN'): {'(1,0)': 1.0},  # Hit wall, stay in place\n",
    "    ('(1,1)', 'RIGHT'): {'(1,1)': 1.0},  # Goal state, stay\n",
    "    ('(1,1)', 'DOWN'): {'(1,1)': 1.0},   # Goal state, stay\n",
    "}\n",
    "\n",
    "# Rewards: R(s,a,s')\n",
    "rewards = {\n",
    "    ('(0,0)', 'RIGHT', '(0,1)'): -1,\n",
    "    ('(0,0)', 'DOWN', '(1,0)'): -1,\n",
    "    ('(0,1)', 'RIGHT', '(0,1)'): -1,\n",
    "    ('(0,1)', 'DOWN', '(1,1)'): 10,  # Reaching goal\n",
    "    ('(1,0)', 'RIGHT', '(1,1)'): 10,  # Reaching goal\n",
    "    ('(1,0)', 'DOWN', '(1,0)'): -1,\n",
    "    ('(1,1)', 'RIGHT', '(1,1)'): 0,  # At goal\n",
    "    ('(1,1)', 'DOWN', '(1,1)'): 0,   # At goal\n",
    "}\n",
    "\n",
    "# Create MDP\n",
    "mdp = SimpleMDP(states, actions, transitions, rewards, gamma=0.9)\n",
    "\n",
    "print(\"MDP Components:\")\n",
    "print(f\"1. State Space S: {states}\")\n",
    "print(f\"   |S| = {len(states)} states\")\n",
    "\n",
    "print(f\"2. Action Space A: {actions}\")\n",
    "print(f\"   |A| = {len(actions)} actions\")\n",
    "\n",
    "print(f\"3. Discount Factor \u03b3: {mdp.gamma}\")\n",
    "\n",
    "print(\"4. Transition Function P(s'|s,a):\")\n",
    "print(\"   Example: P((0,1) | (0,0), RIGHT) =\", mdp.get_transition_prob('(0,0)', 'RIGHT', '(0,1)'))\n",
    "print(\"   Example: P((1,0) | (0,0)', DOWN) =\", mdp.get_transition_prob('(0,0)', 'DOWN', '(1,0)'))\n",
    "\n",
    "print(\"5. Reward Function R(s,a,s'):\")\n",
    "print(\"   Example: R((0,0), RIGHT, (0,1)) =\", mdp.get_reward('(0,0)', 'RIGHT', '(0,1)'))\n",
    "print(\"   Example: R((0,1), DOWN, (1,1)) =\", mdp.get_reward('(0,1)', 'DOWN', '(1,1)'))\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"Grid Layout:\")\n",
    "print(\"  (0,0) \u2192 (0,1)\")\n",
    "print(\"    \u2193       \u2193\")\n",
    "print(\"  (1,0) \u2192 (1,1) [GOAL]\")\n",
    "print(\"Goal: Reach (1,1) from (0,0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate episodes in the MDP\n",
    "print(\"Simulating MDP Episodes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run a few episodes with random actions\n",
    "num_episodes = 3\n",
    "max_steps = 5\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    print(f\"Episode {episode}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    state = mdp.reset(initial_state='(0,0)')\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(f\"Initial state: {state}\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Random action selection\n",
    "        action = np.random.choice(mdp.actions)\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, info = mdp.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        print(f\"  Step {step+1}: {info['old_state']} --[{action}]--> {next_state}\")\n",
    "        print(f\"           Reward: {reward:+.0f}, Total: {total_reward:+.0f}\")\n",
    "        \n",
    "        # Check if reached goal\n",
    "        if next_state == '(1,1)':\n",
    "            print(f\"  \u2713 Reached goal in {step+1} steps!\")\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    if state != '(1,1)':\n",
    "        print(f\"  \u2717 Did not reach goal in {max_steps} steps\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83d\udca1 Key MDP Concepts Demonstrated:\")\n",
    "print(\"   1. States: Discrete positions in the grid\")\n",
    "print(\"   2. Actions: RIGHT and DOWN movements\")\n",
    "print(\"   3. Transitions: Deterministic (probability = 1.0)\")\n",
    "print(\"   4. Rewards: Negative for steps, positive for goal\")\n",
    "print(\"   5. Markov Property: Next state depends only on current state and action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing State Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the MDP as a state transition diagram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def visualize_mdp_transitions(mdp):\n",
    "    \"\"\"Create a visualization of MDP state transitions.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Define state positions for visualization\n",
    "    state_positions = {\n",
    "        '(0,0)': (1, 3),\n",
    "        '(0,1)': (3, 3),\n",
    "        '(1,0)': (1, 1),\n",
    "        '(1,1)': (3, 1)\n",
    "    }\n",
    "    \n",
    "    # Draw states\n",
    "    for state, (x, y) in state_positions.items():\n",
    "        if state == '(1,1)':\n",
    "            # Goal state - green\n",
    "            circle = plt.Circle((x, y), 0.3, color='lightgreen', ec='darkgreen', linewidth=3)\n",
    "            ax.add_patch(circle)\n",
    "            ax.text(x, y, state + 'GOAL', ha='center', va='center', \n",
    "                   fontsize=11, fontweight='bold')\n",
    "        elif state == '(0,0)':\n",
    "            # Start state - blue\n",
    "            circle = plt.Circle((x, y), 0.3, color='lightblue', ec='darkblue', linewidth=3)\n",
    "            ax.add_patch(circle)\n",
    "            ax.text(x, y, state + 'START', ha='center', va='center', \n",
    "                   fontsize=11, fontweight='bold')\n",
    "        else:\n",
    "            # Regular state - white\n",
    "            circle = plt.Circle((x, y), 0.3, color='white', ec='black', linewidth=2)\n",
    "            ax.add_patch(circle)\n",
    "            ax.text(x, y, state, ha='center', va='center', \n",
    "                   fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Draw transitions\n",
    "    for (state, action), next_states in mdp.transitions.items():\n",
    "        for next_state, prob in next_states.items():\n",
    "            if state == next_state:\n",
    "                # Self-loop (hitting wall or at goal)\n",
    "                continue\n",
    "            \n",
    "            x1, y1 = state_positions[state]\n",
    "            x2, y2 = state_positions[next_state]\n",
    "            \n",
    "            # Calculate arrow position\n",
    "            dx = x2 - x1\n",
    "            dy = y2 - y1\n",
    "            length = np.sqrt(dx**2 + dy**2)\n",
    "            \n",
    "            # Normalize and shorten to account for circle radius\n",
    "            dx_norm = dx / length\n",
    "            dy_norm = dy / length\n",
    "            \n",
    "            start_x = x1 + dx_norm * 0.35\n",
    "            start_y = y1 + dy_norm * 0.35\n",
    "            end_x = x2 - dx_norm * 0.35\n",
    "            end_y = y2 - dy_norm * 0.35\n",
    "            \n",
    "            # Get reward for this transition\n",
    "            reward = mdp.get_reward(state, action, next_state)\n",
    "            \n",
    "            # Color based on action\n",
    "            color = 'blue' if action == 'RIGHT' else 'red'\n",
    "            \n",
    "            # Draw arrow\n",
    "            ax.annotate('', xy=(end_x, end_y), xytext=(start_x, start_y),\n",
    "                       arrowprops=dict(arrowstyle='->', lw=2, color=color, alpha=0.7))\n",
    "            \n",
    "            # Add label\n",
    "            mid_x = (start_x + end_x) / 2\n",
    "            mid_y = (start_y + end_y) / 2\n",
    "            label = f\"{action}R={reward:+.0f}\"\n",
    "            ax.text(mid_x, mid_y, label, ha='center', va='center',\n",
    "                   fontsize=9, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Add legend\n",
    "    right_patch = mpatches.Patch(color='blue', label='RIGHT action')\n",
    "    down_patch = mpatches.Patch(color='red', label='DOWN action')\n",
    "    ax.legend(handles=[right_patch, down_patch], loc='upper right', fontsize=11)\n",
    "    \n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('MDP State Transition Diagram(Arrows show actions and rewards)', \n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = visualize_mdp_transitions(mdp)\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Transition Diagram shows:\")\n",
    "print(\"   - Blue arrows: RIGHT actions\")\n",
    "print(\"   - Red arrows: DOWN actions\")\n",
    "print(\"   - Labels show: Action name and Reward\")\n",
    "print(\"   - Green circle: Goal state (1,1)\")\n",
    "print(\"   - Blue circle: Start state (0,0)\")\n",
    "print(\"This visualizes the complete MDP structure:\")\n",
    "print(\"how states connect through actions and what rewards are received!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discounted Return and Value Functions\n",
    "\n",
    "**From Immediate Rewards to Long-Term Value**\n",
    "\n",
    "In reinforcement learning, we don't just care about immediate rewards - we want to maximize the **total reward over time**. This leads us to the concepts of return and value functions.\n",
    "\n",
    "**The Return (Cumulative Reward)**\n",
    "\n",
    "The **return** $G_t$ at time $t$ is the total discounted reward from that point forward:\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ...$\n",
    "\n",
    "Or more compactly:\n",
    "\n",
    "$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
    "\n",
    "**The Discount Factor \u03b3 (Gamma)**\n",
    "\n",
    "The discount factor $\\gamma \\in [0, 1]$ determines how much we value future rewards:\n",
    "\n",
    "- **$\\gamma = 0$**: Only immediate reward matters (myopic)\n",
    "  - $G_t = R_{t+1}$\n",
    "  - Agent is short-sighted\n",
    "\n",
    "- **$\\gamma = 1$**: All future rewards equally important (far-sighted)\n",
    "  - $G_t = R_{t+1} + R_{t+2} + R_{t+3} + ...$\n",
    "  - Can lead to infinite returns in continuing tasks\n",
    "\n",
    "- **$\\gamma \\in (0, 1)$**: Balance between immediate and future rewards\n",
    "  - Typical values: 0.9, 0.95, 0.99\n",
    "  - Ensures finite returns even in infinite horizons\n",
    "\n",
    "**Why Discount Future Rewards?**\n",
    "\n",
    "1. **Mathematical Convenience**: Ensures convergence for infinite horizons\n",
    "2. **Uncertainty**: Future is uncertain, so future rewards are less reliable\n",
    "3. **Preference**: Often prefer rewards sooner rather than later\n",
    "4. **Computational**: Makes the problem tractable\n",
    "\n",
    "**Example - Effect of Gamma:**\n",
    "\n",
    "Suppose we receive rewards: [1, 1, 1, 1, 1]\n",
    "\n",
    "- $\\gamma = 0.0$: $G = 1$ (only first reward)\n",
    "- $\\gamma = 0.5$: $G = 1 + 0.5 + 0.25 + 0.125 + 0.0625 = 1.9375$\n",
    "- $\\gamma = 0.9$: $G = 1 + 0.9 + 0.81 + 0.729 + 0.6561 = 4.0951$\n",
    "- $\\gamma = 1.0$: $G = 5$ (all rewards equally)\n",
    "\n",
    "Let's implement a function to calculate discounted returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discounted_return(rewards, gamma):\n",
    "    \"\"\"Calculate the discounted return for a sequence of rewards.\n",
    "    \n",
    "    Args:\n",
    "        rewards: List or array of rewards [r1, r2, r3, ...]\n",
    "        gamma: Discount factor (0 to 1)\n",
    "        \n",
    "    Returns:\n",
    "        G: The discounted return\n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    for t, reward in enumerate(rewards):\n",
    "        G += (gamma ** t) * reward\n",
    "    return G\n",
    "\n",
    "\n",
    "def calculate_returns_to_go(rewards, gamma):\n",
    "    \"\"\"Calculate return-to-go for each time step.\n",
    "    \n",
    "    Return-to-go at time t is the discounted sum of rewards from t onward.\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards [r1, r2, r3, ...]\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        returns: List of returns-to-go [G0, G1, G2, ...]\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    \n",
    "    # Calculate backwards for efficiency\n",
    "    for reward in reversed(rewards):\n",
    "        G = reward + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    return returns\n",
    "\n",
    "\n",
    "# Demonstrate discounted return calculation\n",
    "print(\"Discounted Return Calculation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example reward sequence\n",
    "rewards = [1, 1, 1, 1, 1]\n",
    "print(f\"Reward sequence: {rewards}\")\n",
    "\n",
    "# Calculate for different gamma values\n",
    "gamma_values = [0.0, 0.5, 0.9, 0.99, 1.0]\n",
    "\n",
    "print(\"Effect of Discount Factor \u03b3:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'\u03b3':<10} {'Discounted Return':<20} {'Interpretation'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    G = calculate_discounted_return(rewards, gamma)\n",
    "    \n",
    "    if gamma == 0.0:\n",
    "        interp = \"Only immediate reward\"\n",
    "    elif gamma == 1.0:\n",
    "        interp = \"All rewards equally\"\n",
    "    elif gamma < 0.5:\n",
    "        interp = \"Very myopic\"\n",
    "    elif gamma < 0.9:\n",
    "        interp = \"Moderately far-sighted\"\n",
    "    else:\n",
    "        interp = \"Very far-sighted\"\n",
    "    \n",
    "    print(f\"{gamma:<10.2f} {G:<20.4f} {interp}\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "\n",
    "# Example with varying rewards\n",
    "print(\"Example with Varying Rewards:\")\n",
    "rewards2 = [1, 2, 3, 4, 5]\n",
    "gamma = 0.9\n",
    "\n",
    "print(f\"Rewards: {rewards2}\")\n",
    "print(f\"\u03b3 = {gamma}\")\n",
    "\n",
    "G = calculate_discounted_return(rewards2, gamma)\n",
    "print(f\"Total discounted return: {G:.4f}\")\n",
    "\n",
    "# Show the calculation step by step\n",
    "print(\"Step-by-step calculation:\")\n",
    "print(f\"G = {rewards2[0]} + {gamma}\u00d7{rewards2[1]} + {gamma}\u00b2\u00d7{rewards2[2]} + {gamma}\u00b3\u00d7{rewards2[3]} + {gamma}\u2074\u00d7{rewards2[4]}\")\n",
    "print(f\"G = {rewards2[0]} + {gamma*rewards2[1]:.2f} + {gamma**2*rewards2[2]:.2f} + {gamma**3*rewards2[3]:.2f} + {gamma**4*rewards2[4]:.2f}\")\n",
    "print(f\"G = {G:.4f}\")\n",
    "\n",
    "# Calculate returns-to-go\n",
    "returns_to_go = calculate_returns_to_go(rewards2, gamma)\n",
    "print(\"Returns-to-go at each time step:\")\n",
    "for t, (r, G_t) in enumerate(zip(rewards2, returns_to_go)):\n",
    "    print(f\"  t={t}: Reward={r}, Return-to-go G_{t}={G_t:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Functions: Estimating Long-Term Value\n",
    "\n",
    "**From Returns to Value Functions**\n",
    "\n",
    "While the return $G_t$ tells us the actual cumulative reward from a specific trajectory, **value functions** tell us the **expected** return from a state or state-action pair.\n",
    "\n",
    "**State-Value Function V(s)**\n",
    "\n",
    "The **state-value function** $V^\\pi(s)$ is the expected return starting from state $s$ and following policy $\\pi$:\n",
    "\n",
    "$V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$\n",
    "\n",
    "$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]$\n",
    "\n",
    "**Interpretation:**\n",
    "- \"How good is it to be in state $s$?\"\n",
    "- Expected cumulative reward if we start in $s$ and follow policy $\\pi$\n",
    "- Depends on the policy being followed\n",
    "\n",
    "**Action-Value Function Q(s,a)**\n",
    "\n",
    "The **action-value function** $Q^\\pi(s,a)$ is the expected return starting from state $s$, taking action $a$, then following policy $\\pi$:\n",
    "\n",
    "$Q^\\pi(s,a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]$\n",
    "\n",
    "$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a\\right]$\n",
    "\n",
    "**Interpretation:**\n",
    "- \"How good is it to take action $a$ in state $s$?\"\n",
    "- Expected cumulative reward if we start in $s$, take action $a$, then follow $\\pi$\n",
    "- Also called Q-values (hence \"Q-learning\")\n",
    "\n",
    "**Relationship Between V and Q:**\n",
    "\n",
    "The state-value is the expected action-value under the policy:\n",
    "\n",
    "$V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a)$\n",
    "\n",
    "For a deterministic policy that always chooses action $a^*$ in state $s$:\n",
    "\n",
    "$V^\\pi(s) = Q^\\pi(s, a^*)$\n",
    "\n",
    "**Optimal Value Functions:**\n",
    "\n",
    "The **optimal state-value function** $V^*(s)$ is the maximum value achievable in state $s$:\n",
    "\n",
    "$V^*(s) = \\max_\\pi V^\\pi(s)$\n",
    "\n",
    "The **optimal action-value function** $Q^*(s,a)$ is the maximum value achievable by taking action $a$ in state $s$:\n",
    "\n",
    "$Q^*(s,a) = \\max_\\pi Q^\\pi(s,a)$\n",
    "\n",
    "**Key Insight:**\n",
    "If we know $Q^*(s,a)$ for all states and actions, we can act optimally by choosing:\n",
    "\n",
    "$\\pi^*(s) = \\arg\\max_a Q^*(s,a)$\n",
    "\n",
    "This is why Q-learning is so powerful - it learns $Q^*$ directly!\n",
    "\n",
    "Let's demonstrate these concepts with examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate value functions with simple examples\n",
    "print(\"Value Functions: V(s) and Q(s,a)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simple example: 3-state chain\n",
    "# States: S0 -> S1 -> S2 (terminal)\n",
    "# Actions: FORWARD (deterministic)\n",
    "# Rewards: 0, 0, +10 (only at terminal)\n",
    "\n",
    "print(\"Example: Simple 3-State Chain\")\n",
    "print(\"States: S0 \u2192 S1 \u2192 S2 (terminal)\")\n",
    "print(\"Action: FORWARD (deterministic)\")\n",
    "print(\"Rewards: R(S0\u2192S1)=0, R(S1\u2192S2)=10\")\n",
    "print(\"\" + \"-\"*60)\n",
    "\n",
    "gamma = 0.9\n",
    "print(f\"Discount factor \u03b3 = {gamma}\")\n",
    "\n",
    "# Calculate V(s) for each state\n",
    "# V(S2) = 0 (terminal state, no future rewards)\n",
    "# V(S1) = 0 + \u03b3 * 10 = 9.0\n",
    "# V(S0) = 0 + \u03b3 * V(S1) = 0 + 0.9 * 9.0 = 8.1\n",
    "\n",
    "V_S2 = 0\n",
    "V_S1 = 0 + gamma * 10\n",
    "V_S0 = 0 + gamma * V_S1\n",
    "\n",
    "print(\"State-Value Function V(s):\")\n",
    "print(f\"  V(S0) = {V_S0:.2f}  (2 steps to reward)\")\n",
    "print(f\"  V(S1) = {V_S1:.2f}  (1 step to reward)\")\n",
    "print(f\"  V(S2) = {V_S2:.2f}  (terminal state)\")\n",
    "\n",
    "print(\"\ud83d\udca1 Interpretation:\")\n",
    "print(\"   - V(S0) < V(S1) because S0 is farther from the reward\")\n",
    "print(\"   - Each step away reduces value by factor of \u03b3\")\n",
    "print(\"   - V(s) tells us 'how good' each state is\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "\n",
    "# Example with multiple actions\n",
    "print(\"Example: Grid World with Multiple Actions\")\n",
    "print(\"Consider state S with two actions:\")\n",
    "print(\"  - Action A1: Leads to goal (reward +10) with prob 0.8\")\n",
    "print(\"  - Action A2: Leads to goal (reward +10) with prob 0.3\")\n",
    "print(\"Both actions give -1 reward if they don't reach goal\")\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "# Q(S, A1) = 0.8 * 10 + 0.2 * (-1) = 7.8\n",
    "# Q(S, A2) = 0.3 * 10 + 0.7 * (-1) = 2.3\n",
    "\n",
    "Q_S_A1 = 0.8 * 10 + 0.2 * (-1)\n",
    "Q_S_A2 = 0.3 * 10 + 0.7 * (-1)\n",
    "\n",
    "print(\"Action-Value Function Q(s,a):\")\n",
    "print(f\"  Q(S, A1) = {Q_S_A1:.2f}  (high success rate)\")\n",
    "print(f\"  Q(S, A2) = {Q_S_A2:.2f}  (low success rate)\")\n",
    "\n",
    "print(\"\ud83d\udca1 Interpretation:\")\n",
    "print(\"   - Q(S, A1) > Q(S, A2) because A1 is more likely to succeed\")\n",
    "print(\"   - Optimal action: A1 (higher Q-value)\")\n",
    "print(\"   - Q(s,a) tells us 'how good' each action is in each state\")\n",
    "\n",
    "# If following a policy that chooses A1 with prob 0.7 and A2 with prob 0.3\n",
    "V_S = 0.7 * Q_S_A1 + 0.3 * Q_S_A2\n",
    "print(f\"If policy \u03c0(A1|S)=0.7, \u03c0(A2|S)=0.3:\")\n",
    "print(f\"  V(S) = 0.7 \u00d7 Q(S,A1) + 0.3 \u00d7 Q(S,A2)\")\n",
    "print(f\"  V(S) = 0.7 \u00d7 {Q_S_A1:.2f} + 0.3 \u00d7 {Q_S_A2:.2f}\")\n",
    "print(f\"  V(S) = {V_S:.2f}\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83c\udfaf Key Takeaways:\")\n",
    "print(\"   1. V(s): Expected return from state s\")\n",
    "print(\"   2. Q(s,a): Expected return from taking action a in state s\")\n",
    "print(\"   3. V(s) = \u03a3 \u03c0(a|s) Q(s,a) (weighted average over actions)\")\n",
    "print(\"   4. Optimal policy: Choose action with highest Q-value\")\n",
    "print(\"   5. Value functions are the foundation of RL algorithms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Effect of Discount Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how discount factor affects returns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a reward sequence\n",
    "num_steps = 20\n",
    "rewards = np.ones(num_steps)  # Constant reward of 1 at each step\n",
    "\n",
    "# Calculate returns for different gamma values\n",
    "gamma_values = [0.5, 0.7, 0.9, 0.95, 0.99]\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Discount weights over time\n",
    "time_steps = np.arange(num_steps)\n",
    "for gamma, color in zip(gamma_values, colors):\n",
    "    weights = gamma ** time_steps\n",
    "    ax1.plot(time_steps, weights, linewidth=2, color=color, \n",
    "            label=f'\u03b3 = {gamma}', marker='o', markersize=4, alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Time Steps into Future', fontsize=12)\n",
    "ax1.set_ylabel('Discount Weight (\u03b3\u1d57)', fontsize=12)\n",
    "ax1.set_title('How Discount Factor Weights Future Rewards', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper right', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 1.1])\n",
    "\n",
    "# Plot 2: Total discounted return\n",
    "returns = []\n",
    "for gamma in gamma_values:\n",
    "    G = calculate_discounted_return(rewards, gamma)\n",
    "    returns.append(G)\n",
    "\n",
    "bars = ax2.bar(range(len(gamma_values)), returns, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax2.set_xlabel('Discount Factor \u03b3', fontsize=12)\n",
    "ax2.set_ylabel('Total Discounted Return', fontsize=12)\n",
    "ax2.set_title('Total Return for Constant Reward Sequence', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(range(len(gamma_values)))\n",
    "ax2.set_xticklabels([f'{g}' for g in gamma_values])\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, returns):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Visualization Insights:\")\n",
    "print(\"Left Plot - Discount Weights:\")\n",
    "print(\"   - Shows how much each future reward is weighted\")\n",
    "print(\"   - Lower \u03b3: Future rewards decay quickly\")\n",
    "print(\"   - Higher \u03b3: Future rewards remain important longer\")\n",
    "print(\"Right Plot - Total Returns:\")\n",
    "print(\"   - Shows cumulative effect of discounting\")\n",
    "print(\"   - \u03b3=0.5: Only considers ~2 steps ahead effectively\")\n",
    "print(\"   - \u03b3=0.99: Considers ~100 steps ahead effectively\")\n",
    "print(\"\ud83d\udca1 Rule of thumb: Effective horizon \u2248 1/(1-\u03b3) steps\")\n",
    "for gamma in gamma_values:\n",
    "    horizon = 1 / (1 - gamma)\n",
    "    print(f\"   \u03b3={gamma}: ~{horizon:.0f} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='policies'></a>\n",
    "#### Policies: Mapping States to Actions\n",
    "\n",
    "**What is a Policy?**\n",
    "\n",
    "A **policy** $\\pi$ is a strategy that defines how the agent behaves - it maps states to actions. The policy is what the agent learns in reinforcement learning.\n",
    "\n",
    "**Types of Policies:**\n",
    "\n",
    "**1. Deterministic Policy**\n",
    "\n",
    "A deterministic policy $\\pi: S \\rightarrow A$ maps each state to a single action:\n",
    "\n",
    "$a = \\pi(s)$\n",
    "\n",
    "**Example:**\n",
    "- In grid world: \"Always move RIGHT in state (0,0)\"\n",
    "- In chess: \"Always make the move that captures the most valuable piece\"\n",
    "\n",
    "**2. Stochastic Policy**\n",
    "\n",
    "A stochastic policy $\\pi(a|s)$ gives a probability distribution over actions for each state:\n",
    "\n",
    "$\\pi(a|s) = P(A_t = a | S_t = s)$\n",
    "\n",
    "where $\\sum_a \\pi(a|s) = 1$ for all states $s$\n",
    "\n",
    "**Example:**\n",
    "- In grid world: \"Move RIGHT with 70% probability, DOWN with 30% in state (0,0)\"\n",
    "- Epsilon-greedy: \"Take best action with probability 1-\u03b5, random action with probability \u03b5\"\n",
    "\n",
    "**Why Stochastic Policies?**\n",
    "\n",
    "1. **Exploration**: Randomness helps explore the environment\n",
    "2. **Partial Observability**: When state is uncertain, randomization can help\n",
    "3. **Game Theory**: In competitive settings, randomization prevents exploitation\n",
    "4. **Continuous Actions**: Natural representation for continuous action spaces\n",
    "\n",
    "**Optimal Policy**\n",
    "\n",
    "The **optimal policy** $\\pi^*$ maximizes the expected return from every state:\n",
    "\n",
    "$\\pi^* = \\arg\\max_\\pi V^\\pi(s) \\text{ for all } s \\in S$\n",
    "\n",
    "**Key Theorem:** For any MDP, there exists an optimal deterministic policy!\n",
    "\n",
    "This means we can always find a policy that doesn't need randomness to be optimal (though stochastic policies are still useful during learning).\n",
    "\n",
    "Let's implement policy representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicPolicy:\n",
    "    \"\"\"A deterministic policy that maps states to actions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize empty policy.\"\"\"\n",
    "        self.policy = {}  # state -> action mapping\n",
    "    \n",
    "    def set_action(self, state, action):\n",
    "        \"\"\"Set the action for a given state.\"\"\"\n",
    "        self.policy[state] = action\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Get the action for a given state.\"\"\"\n",
    "        return self.policy.get(state, None)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"DeterministicPolicy({len(self.policy)} states)\"\n",
    "\n",
    "\n",
    "class StochasticPolicy:\n",
    "    \"\"\"A stochastic policy that gives probability distributions over actions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize empty policy.\"\"\"\n",
    "        self.policy = {}  # state -> {action: probability} mapping\n",
    "    \n",
    "    def set_action_probs(self, state, action_probs):\n",
    "        \"\"\"Set action probabilities for a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state\n",
    "            action_probs: Dict mapping actions to probabilities\n",
    "        \"\"\"\n",
    "        # Verify probabilities sum to 1\n",
    "        total = sum(action_probs.values())\n",
    "        if not np.isclose(total, 1.0):\n",
    "            raise ValueError(f\"Action probabilities must sum to 1, got {total}\")\n",
    "        self.policy[state] = action_probs.copy()\n",
    "    \n",
    "    def get_action_prob(self, state, action):\n",
    "        \"\"\"Get probability of taking action in state.\"\"\"\n",
    "        return self.policy.get(state, {}).get(action, 0.0)\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"Sample an action according to the policy.\"\"\"\n",
    "        action_probs = self.policy.get(state, {})\n",
    "        if not action_probs:\n",
    "            return None\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"StochasticPolicy({len(self.policy)} states)\"\n",
    "\n",
    "\n",
    "# Demonstrate policy representations\n",
    "print(\"Policy Representations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example 1: Deterministic policy for 2x2 grid\n",
    "print(\"Example 1: Deterministic Policy for 2x2 Grid\")\n",
    "\n",
    "det_policy = DeterministicPolicy()\n",
    "det_policy.set_action('(0,0)', 'RIGHT')\n",
    "det_policy.set_action('(0,1)', 'DOWN')\n",
    "det_policy.set_action('(1,0)', 'RIGHT')\n",
    "det_policy.set_action('(1,1)', 'STAY')  # Goal state\n",
    "\n",
    "print(\"Deterministic Policy \u03c0(s):\")\n",
    "for state in ['(0,0)', '(0,1)', '(1,0)', '(1,1)']:\n",
    "    action = det_policy.get_action(state)\n",
    "    print(f\"  \u03c0({state}) = {action}\")\n",
    "\n",
    "print(\"\ud83d\udca1 This policy always takes the same action in each state\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "\n",
    "# Example 2: Stochastic policy\n",
    "print(\"Example 2: Stochastic Policy (Exploration)\")\n",
    "\n",
    "stoch_policy = StochasticPolicy()\n",
    "\n",
    "# State (0,0): Prefer RIGHT but sometimes go DOWN\n",
    "stoch_policy.set_action_probs('(0,0)', {'RIGHT': 0.7, 'DOWN': 0.3})\n",
    "\n",
    "# State (0,1): Prefer DOWN\n",
    "stoch_policy.set_action_probs('(0,1)', {'RIGHT': 0.1, 'DOWN': 0.9})\n",
    "\n",
    "# State (1,0): Prefer RIGHT\n",
    "stoch_policy.set_action_probs('(1,0)', {'RIGHT': 0.9, 'DOWN': 0.1})\n",
    "\n",
    "print(\"Stochastic Policy \u03c0(a|s):\")\n",
    "for state in ['(0,0)', '(0,1)', '(1,0)']:\n",
    "    print(f\"  State {state}:\")\n",
    "    for action in ['RIGHT', 'DOWN']:\n",
    "        prob = stoch_policy.get_action_prob(state, action)\n",
    "        if prob > 0:\n",
    "            print(f\"    \u03c0({action}|{state}) = {prob:.1f}\")\n",
    "\n",
    "print(\"\ud83d\udca1 This policy has randomness - different actions with different probabilities\")\n",
    "\n",
    "# Sample actions from stochastic policy\n",
    "print(\"Sampling 10 actions from state (0,0):\")\n",
    "samples = [stoch_policy.sample_action('(0,0)') for _ in range(10)]\n",
    "print(f\"  Actions: {samples}\")\n",
    "right_count = samples.count('RIGHT')\n",
    "down_count = samples.count('DOWN')\n",
    "print(f\"  RIGHT: {right_count}/10 ({right_count*10}%), DOWN: {down_count}/10 ({down_count*10}%)\")\n",
    "print(f\"  Expected: RIGHT: 70%, DOWN: 30%\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83c\udfaf Key Points:\")\n",
    "print(\"   1. Deterministic: \u03c0(s) \u2192 single action\")\n",
    "print(\"   2. Stochastic: \u03c0(a|s) \u2192 probability distribution\")\n",
    "print(\"   3. Optimal policies can be deterministic\")\n",
    "print(\"   4. Stochastic policies useful for exploration during learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equations: The Foundation of RL Algorithms\n",
    "\n",
    "**The Bellman Equations**\n",
    "\n",
    "The **Bellman equations** are fundamental recursive relationships that express value functions in terms of themselves. They are the mathematical foundation for most RL algorithms.\n",
    "\n",
    "**Bellman Equation for V(s):**\n",
    "\n",
    "The value of a state equals the expected immediate reward plus the discounted value of the next state:\n",
    "\n",
    "$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V^\\pi(s')\\right]$\n",
    "\n",
    "**In words:**\n",
    "1. Consider all possible actions under policy $\\pi$\n",
    "2. For each action, consider all possible next states\n",
    "3. Sum up: immediate reward + discounted value of next state\n",
    "4. Weight by probabilities\n",
    "\n",
    "**Bellman Equation for Q(s,a):**\n",
    "\n",
    "$Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a')\\right]$\n",
    "\n",
    "**Bellman Optimality Equations:**\n",
    "\n",
    "For the optimal value functions:\n",
    "\n",
    "$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V^*(s')\\right]$\n",
    "\n",
    "$Q^*(s,a) = \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')\\right]$\n",
    "\n",
    "**Why Are Bellman Equations Important?**\n",
    "\n",
    "1. **Recursive Structure**: Break down long-term value into immediate reward + future value\n",
    "2. **Dynamic Programming**: Enable iterative computation of value functions\n",
    "3. **Temporal Difference Learning**: Basis for TD learning and Q-learning\n",
    "4. **Optimality**: Optimal policies satisfy the Bellman optimality equations\n",
    "\n",
    "**The Bellman Deadlock**\n",
    "\n",
    "The Bellman equations create a system of equations where:\n",
    "- Each value depends on other values\n",
    "- We have $|S|$ equations with $|S|$ unknowns (for V)\n",
    "- Or $|S| \\times |A|$ equations with $|S| \\times |A|$ unknowns (for Q)\n",
    "\n",
    "**The Problem:**\n",
    "- Can't solve directly because values are defined in terms of each other\n",
    "- This circular dependency is called the \"Bellman deadlock\"\n",
    "\n",
    "**Solutions:**\n",
    "1. **Iterative Methods**: Dynamic Programming (policy evaluation, value iteration)\n",
    "2. **Sampling Methods**: Monte Carlo, Temporal Difference learning\n",
    "3. **Function Approximation**: Neural networks for large state spaces\n",
    "\n",
    "**The Curse of Dimensionality**\n",
    "\n",
    "As the state space grows, computational requirements explode:\n",
    "\n",
    "- **Tabular Methods**: Need to store value for every state\n",
    "  - 10 binary features \u2192 $2^{10} = 1,024$ states\n",
    "  - 20 binary features \u2192 $2^{20} = 1,048,576$ states\n",
    "  - 30 binary features \u2192 $2^{30} = 1,073,741,824$ states\n",
    "\n",
    "- **Continuous States**: Infinite states (e.g., robot position)\n",
    "\n",
    "**Addressing the Curse:**\n",
    "1. **Function Approximation**: Learn V(s) or Q(s,a) with neural networks\n",
    "2. **Sampling**: Don't visit all states, learn from experience\n",
    "3. **Generalization**: Use features to generalize across similar states\n",
    "4. **Hierarchical Methods**: Break problem into subproblems\n",
    "\n",
    "Let's demonstrate the Bellman equations with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Bellman equations with policy evaluation\n",
    "print(\"Bellman Equations: Policy Evaluation Example\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use our 2x2 grid MDP from earlier\n",
    "states = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\n",
    "gamma = 0.9\n",
    "\n",
    "# Define a simple policy: always go RIGHT from (0,0) and (1,0), DOWN from (0,1)\n",
    "policy = {\n",
    "    '(0,0)': {'RIGHT': 1.0},\n",
    "    '(0,1)': {'DOWN': 1.0},\n",
    "    '(1,0)': {'RIGHT': 1.0},\n",
    "    '(1,1)': {'RIGHT': 1.0}  # Terminal, doesn't matter\n",
    "}\n",
    "\n",
    "# Transitions and rewards (from earlier MDP)\n",
    "transitions = {\n",
    "    ('(0,0)', 'RIGHT'): {'(0,1)': 1.0},\n",
    "    ('(0,1)', 'DOWN'): {'(1,1)': 1.0},\n",
    "    ('(1,0)', 'RIGHT'): {'(1,1)': 1.0},\n",
    "    ('(1,1)', 'RIGHT'): {'(1,1)': 1.0},\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    ('(0,0)', 'RIGHT', '(0,1)'): -1,\n",
    "    ('(0,1)', 'DOWN', '(1,1)'): 10,\n",
    "    ('(1,0)', 'RIGHT', '(1,1)'): 10,\n",
    "    ('(1,1)', 'RIGHT', '(1,1)'): 0,\n",
    "}\n",
    "\n",
    "print(\"MDP Setup:\")\n",
    "print(\"  States: (0,0) \u2192 (0,1) \u2192 (1,1) [GOAL]\")\n",
    "print(\"           \u2193       \u2193\")\n",
    "print(\"         (1,0) \u2192 (1,1) [GOAL]\")\n",
    "print(f\"  Discount factor \u03b3 = {gamma}\")\n",
    "print(\"  Policy \u03c0:\")\n",
    "for state, actions in policy.items():\n",
    "    for action, prob in actions.items():\n",
    "        if prob > 0:\n",
    "            print(f\"    \u03c0({state}) = {action}\")\n",
    "\n",
    "print(\"\" + \"-\"*60)\n",
    "print(\"Applying Bellman Equation: V(s) = \u03a3 \u03c0(a|s) \u03a3 P(s'|s,a)[R + \u03b3V(s')]\")\n",
    "\n",
    "# Iterative policy evaluation\n",
    "V = {s: 0.0 for s in states}  # Initialize values to 0\n",
    "V['(1,1)'] = 0.0  # Terminal state\n",
    "\n",
    "print(\"Iteration 0 (Initial):\")\n",
    "for state in states:\n",
    "    print(f\"  V({state}) = {V[state]:.2f}\")\n",
    "\n",
    "# Perform a few iterations\n",
    "for iteration in range(1, 6):\n",
    "    V_new = V.copy()\n",
    "    \n",
    "    for state in states:\n",
    "        if state == '(1,1)':  # Terminal state\n",
    "            continue\n",
    "        \n",
    "        # Apply Bellman equation\n",
    "        v = 0.0\n",
    "        for action, action_prob in policy[state].items():\n",
    "            # Get transitions for this state-action pair\n",
    "            next_states = transitions.get((state, action), {})\n",
    "            \n",
    "            for next_state, trans_prob in next_states.items():\n",
    "                reward = rewards.get((state, action, next_state), 0.0)\n",
    "                # Bellman equation: R + \u03b3 * V(s')\n",
    "                v += action_prob * trans_prob * (reward + gamma * V[next_state])\n",
    "        \n",
    "        V_new[state] = v\n",
    "    \n",
    "    V = V_new\n",
    "    \n",
    "    print(f\"Iteration {iteration}:\")\n",
    "    for state in states:\n",
    "        print(f\"  V({state}) = {V[state]:.2f}\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83d\udca1 Observations:\")\n",
    "print(\"   1. Values converge through iterative application of Bellman equation\")\n",
    "print(\"   2. V(1,1) = 0 (terminal state, no future rewards)\")\n",
    "print(\"   3. V(0,1) and V(1,0) are high (one step from goal)\")\n",
    "print(\"   4. V(0,0) is lower (two steps from goal, more discounting)\")\n",
    "print(\"   5. Each iteration uses previous values to compute new values\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83c\udfaf Key Insights:\")\n",
    "print(\"   1. Bellman equations express values recursively\")\n",
    "print(\"   2. Can't solve directly (circular dependency = Bellman deadlock)\")\n",
    "print(\"   3. Iterative methods converge to true values\")\n",
    "print(\"   4. This is the foundation of Dynamic Programming!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the curse of dimensionality\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate state space sizes for different scenarios\n",
    "# Store as (name, exponent) to avoid overflow\n",
    "scenarios = [\n",
    "    ('Grid 5\u00d75', np.log10(25)),\n",
    "    ('Grid 10\u00d710', np.log10(100)),\n",
    "    ('Grid 20\u00d720', np.log10(400)),\n",
    "    ('10 binary features', np.log10(2**10)),\n",
    "    ('15 binary features', np.log10(2**15)),\n",
    "    ('20 binary features', np.log10(2**20)),\n",
    "    ('Chess (approx)', 43),\n",
    "    ('Go (approx)', 170)\n",
    "]\n",
    "\n",
    "names = [s[0] for s in scenarios]\n",
    "log_sizes = [s[1] for s in scenarios]\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Use log scale for y-axis\n",
    "y_pos = np.arange(len(names))\n",
    "colors = ['green', 'green', 'yellow', 'yellow', 'orange', 'red', 'darkred', 'darkred']\n",
    "\n",
    "bars = ax.barh(y_pos, log_sizes, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(names, fontsize=11)\n",
    "ax.set_xlabel('Number of States (log\u2081\u2080 scale)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('The Curse of Dimensionality in Reinforcement Learning', \n",
    "            fontsize=15, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, log_size) in enumerate(zip(bars, log_sizes)):\n",
    "    exp = int(log_size)\n",
    "    label = f'10^{exp}'\n",
    "    \n",
    "    ax.text(log_size, bar.get_y() + bar.get_height()/2, f'  {label}',\n",
    "           va='center', ha='left', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Add annotations\n",
    "ax.text(0.02, 0.98, 'Tractable withtabular methods', \n",
    "       transform=ax.transAxes, fontsize=11, va='top',\n",
    "       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "ax.text(0.02, 0.50, 'Need functionapproximation', \n",
    "       transform=ax.transAxes, fontsize=11, va='top',\n",
    "       bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "\n",
    "ax.text(0.02, 0.15, 'Extremelychallenging', \n",
    "       transform=ax.transAxes, fontsize=11, va='top',\n",
    "       bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca The Curse of Dimensionality:\")\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"State Space Growth:\")\n",
    "# Recreate actual sizes for printing\n",
    "actual_scenarios = [\n",
    "    ('Grid 5\u00d75', 25),\n",
    "    ('Grid 10\u00d710', 100),\n",
    "    ('Grid 20\u00d720', 400),\n",
    "    ('10 binary features', 2**10),\n",
    "    ('15 binary features', 2**15),\n",
    "    ('20 binary features', 2**20),\n",
    "    ('Chess (approx)', 43),\n",
    "    ('Go (approx)', 170)\n",
    "]\n",
    "for name, size in actual_scenarios:\n",
    "    if isinstance(size, int) and size < 10**10:\n",
    "        print(f\"  {name:<25} {size:>20,} states\")\n",
    "    else:\n",
    "        if isinstance(size, int) and size >= 10**10:\n",
    "            exp = int(np.log10(size))\n",
    "        else:\n",
    "            exp = size\n",
    "        print(f\"  {name:<25} ~10^{exp} states\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83d\udca1 Key Insights:\")\n",
    "print(\"   1. State space grows exponentially with features\")\n",
    "print(\"   2. Tabular methods only work for small state spaces\")\n",
    "print(\"   3. Real-world problems need function approximation\")\n",
    "print(\"   4. Deep RL uses neural networks to handle large spaces\")\n",
    "print(\"   5. Sampling and generalization are essential!\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83c\udfaf Summary of MDP Framework:\")\n",
    "print(\"   We've covered the complete MDP framework:\")\n",
    "print(\"   \u2713 Core terminology (agent, environment, state, action, reward)\")\n",
    "print(\"   \u2713 MDP components (S, A, P, R, \u03b3)\")\n",
    "print(\"   \u2713 Markov Property and its importance\")\n",
    "print(\"   \u2713 Discounted returns and value functions\")\n",
    "print(\"   \u2713 Policies (deterministic and stochastic)\")\n",
    "print(\"   \u2713 Bellman equations (foundation of RL algorithms)\")\n",
    "print(\"   \u2713 Challenges (Bellman deadlock, curse of dimensionality)\")\n",
    "print(\"   Next: We'll learn algorithms to solve MDPs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dynamic-programming'></a>\n",
    "### Dynamic Programming: Solving MDPs with Perfect Knowledge\n",
    "\n",
    "**From Theory to Algorithms**\n",
    "\n",
    "Now that we understand the Bellman equations, we can use them to solve MDPs! **Dynamic Programming (DP)** methods provide exact solutions when we have perfect knowledge of the environment's dynamics.\n",
    "\n",
    "**What is Dynamic Programming?**\n",
    "\n",
    "Dynamic Programming is a general approach to solving complex problems by:\n",
    "1. Breaking them into simpler subproblems\n",
    "2. Solving each subproblem once\n",
    "3. Storing solutions to avoid recomputation\n",
    "4. Combining solutions to solve the original problem\n",
    "\n",
    "In RL, DP uses the Bellman equations to iteratively compute value functions.\n",
    "\n",
    "**Key Assumptions for DP:**\n",
    "\n",
    "1. **Perfect Model**: We know $P(s'|s,a)$ and $R(s,a,s')$ for all states and actions\n",
    "2. **Finite State/Action Spaces**: Can enumerate all states and actions\n",
    "3. **Markov Property**: Future depends only on current state\n",
    "\n",
    "**Two Main DP Algorithms:**\n",
    "\n",
    "1. **Policy Evaluation**: Compute $V^\\pi(s)$ for a given policy $\\pi$\n",
    "2. **Policy Improvement**: Find a better policy given $V^\\pi(s)$\n",
    "\n",
    "Combining these gives us **Policy Iteration** and **Value Iteration** algorithms.\n",
    "\n",
    "**Why Study DP?**\n",
    "\n",
    "Even though DP requires perfect knowledge (rarely available in practice), it's important because:\n",
    "- Provides theoretical foundation for RL\n",
    "- Many RL algorithms are approximate DP methods\n",
    "- Helps understand convergence and optimality\n",
    "- Works well for planning problems (e.g., robotics with simulators)\n",
    "\n",
    "Let's start with Policy Evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Evaluation: Computing the Value Function\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "Given a policy $\\pi$, compute the state-value function $V^\\pi(s)$ for all states.\n",
    "\n",
    "**The Bellman Equation for Policy Evaluation:**\n",
    "\n",
    "$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V^\\pi(s')\\right]$\n",
    "\n",
    "This is a system of $|S|$ linear equations with $|S|$ unknowns. We could solve it directly, but for large state spaces, we use an **iterative approach**.\n",
    "\n",
    "**Iterative Policy Evaluation Algorithm:**\n",
    "\n",
    "1. Initialize $V(s) = 0$ for all states (or any arbitrary values)\n",
    "2. Repeat until convergence:\n",
    "   - For each state $s$:\n",
    "     - $V_{k+1}(s) \\leftarrow \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V_k(s')\\right]$\n",
    "3. Stop when $\\max_s |V_{k+1}(s) - V_k(s)| < \\theta$ (small threshold)\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "- Each iteration applies the Bellman equation as an update rule\n",
    "- Uses old values $V_k(s')$ to compute new values $V_{k+1}(s)$\n",
    "- Guaranteed to converge to $V^\\pi$ as $k \\rightarrow \\infty$\n",
    "- Called \"bootstrapping\" - using estimates to update estimates\n",
    "\n",
    "**Two Variants:**\n",
    "\n",
    "1. **Synchronous**: Update all states using old values, then replace all at once\n",
    "2. **Asynchronous**: Update states one at a time, using most recent values\n",
    "\n",
    "Asynchronous often converges faster in practice.\n",
    "\n",
    "Let's implement policy evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(mdp, policy, gamma=0.9, theta=0.0001, max_iterations=1000):\n",
    "    \"\"\"Evaluate a policy using iterative policy evaluation.\n",
    "    \n",
    "    Args:\n",
    "        mdp: MDP object with states, actions, transitions, rewards\n",
    "        policy: Dict mapping state -> {action: probability}\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold\n",
    "        max_iterations: Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        V: Dict mapping state -> value\n",
    "        iterations: Number of iterations until convergence\n",
    "    \"\"\"\n",
    "    # Initialize value function to zero\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0  # Track maximum change in value\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        # Update value for each state\n",
    "        for state in mdp.states:\n",
    "            v = 0.0\n",
    "            \n",
    "            # Sum over all actions according to policy\n",
    "            for action, action_prob in policy.get(state, {}).items():\n",
    "                # Sum over all possible next states\n",
    "                next_states = mdp.transitions.get((state, action), {})\n",
    "                \n",
    "                for next_state, trans_prob in next_states.items():\n",
    "                    reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                    # Bellman equation\n",
    "                    v += action_prob * trans_prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            V_new[state] = v\n",
    "            delta = max(delta, abs(V_new[state] - V[state]))\n",
    "        \n",
    "        V = V_new\n",
    "        \n",
    "        # Check for convergence\n",
    "        if delta < theta:\n",
    "            print(f\"Policy evaluation converged in {iteration + 1} iterations\")\n",
    "            return V, iteration + 1\n",
    "    \n",
    "    print(f\"Policy evaluation reached max iterations ({max_iterations})\")\n",
    "    return V, max_iterations\n",
    "\n",
    "\n",
    "# Demonstrate policy evaluation on our 2x2 grid MDP\n",
    "print(\"Policy Evaluation Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Recreate the 2x2 grid MDP\n",
    "states = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\n",
    "actions = ['RIGHT', 'DOWN']\n",
    "\n",
    "transitions = {\n",
    "    ('(0,0)', 'RIGHT'): {'(0,1)': 1.0},\n",
    "    ('(0,0)', 'DOWN'): {'(1,0)': 1.0},\n",
    "    ('(0,1)', 'RIGHT'): {'(0,1)': 1.0},\n",
    "    ('(0,1)', 'DOWN'): {'(1,1)': 1.0},\n",
    "    ('(1,0)', 'RIGHT'): {'(1,1)': 1.0},\n",
    "    ('(1,0)', 'DOWN'): {'(1,0)': 1.0},\n",
    "    ('(1,1)', 'RIGHT'): {'(1,1)': 1.0},\n",
    "    ('(1,1)', 'DOWN'): {'(1,1)': 1.0},\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    ('(0,0)', 'RIGHT', '(0,1)'): -1,\n",
    "    ('(0,0)', 'DOWN', '(1,0)'): -1,\n",
    "    ('(0,1)', 'RIGHT', '(0,1)'): -1,\n",
    "    ('(0,1)', 'DOWN', '(1,1)'): 10,\n",
    "    ('(1,0)', 'RIGHT', '(1,1)'): 10,\n",
    "    ('(1,0)', 'DOWN', '(1,0)'): -1,\n",
    "    ('(1,1)', 'RIGHT', '(1,1)'): 0,\n",
    "    ('(1,1)', 'DOWN', '(1,1)'): 0,\n",
    "}\n",
    "\n",
    "mdp = SimpleMDP(states, actions, transitions, rewards, gamma=0.9)\n",
    "\n",
    "# Define a policy: always go RIGHT from (0,0) and (1,0), DOWN from (0,1)\n",
    "policy = {\n",
    "    '(0,0)': {'RIGHT': 1.0},\n",
    "    '(0,1)': {'DOWN': 1.0},\n",
    "    '(1,0)': {'RIGHT': 1.0},\n",
    "    '(1,1)': {'RIGHT': 1.0}\n",
    "}\n",
    "\n",
    "print(\"MDP: 2x2 Grid World\")\n",
    "print(\"  (0,0) \u2192 (0,1)\")\n",
    "print(\"    \u2193       \u2193\")\n",
    "print(\"  (1,0) \u2192 (1,1) [GOAL]\")\n",
    "print(f\"Discount factor \u03b3 = {mdp.gamma}\")\n",
    "\n",
    "print(\"Policy \u03c0:\")\n",
    "for state, actions_dict in policy.items():\n",
    "    for action, prob in actions_dict.items():\n",
    "        if prob > 0:\n",
    "            print(f\"  \u03c0({state}) = {action}\")\n",
    "\n",
    "print(\"\" + \"-\"*60)\n",
    "print(\"Running Policy Evaluation...\")\n",
    "\n",
    "# Evaluate the policy\n",
    "V, num_iterations = policy_evaluation(mdp, policy, gamma=0.9, theta=0.0001)\n",
    "\n",
    "print(\"Final Value Function V^\u03c0(s):\")\n",
    "print(\"-\"*60)\n",
    "for state in states:\n",
    "    print(f\"  V^\u03c0({state}) = {V[state]:7.4f}\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83d\udca1 Interpretation:\")\n",
    "print(f\"   - V^\u03c0(1,1) = {V['(1,1)']:.4f} (terminal state, no future rewards)\")\n",
    "print(f\"   - V^\u03c0(0,1) = {V['(0,1)']:.4f} (one step from goal via DOWN)\")\n",
    "print(f\"   - V^\u03c0(1,0) = {V['(1,0)']:.4f} (one step from goal via RIGHT)\")\n",
    "print(f\"   - V^\u03c0(0,0) = {V['(0,0)']:.4f} (two steps from goal)\")\n",
    "print(\"   Values reflect expected cumulative reward following policy \u03c0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Policy Evaluation Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how values converge during policy evaluation\n",
    "def policy_evaluation_with_history(mdp, policy, gamma=0.9, theta=0.0001, max_iterations=1000):\n",
    "    \"\"\"Policy evaluation that tracks value history for visualization.\"\"\"\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "    history = {s: [0.0] for s in mdp.states}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for state in mdp.states:\n",
    "            v = 0.0\n",
    "            for action, action_prob in policy.get(state, {}).items():\n",
    "                next_states = mdp.transitions.get((state, action), {})\n",
    "                for next_state, trans_prob in next_states.items():\n",
    "                    reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                    v += action_prob * trans_prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            V_new[state] = v\n",
    "            history[state].append(v)\n",
    "            delta = max(delta, abs(V_new[state] - V[state]))\n",
    "        \n",
    "        V = V_new\n",
    "        \n",
    "        if delta < theta:\n",
    "            return V, history, iteration + 1\n",
    "    \n",
    "    return V, history, max_iterations\n",
    "\n",
    "\n",
    "# Run policy evaluation with history tracking\n",
    "V, history, num_iters = policy_evaluation_with_history(mdp, policy, gamma=0.9, theta=0.0001)\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "markers = ['o', 's', '^', 'd']\n",
    "\n",
    "for state, color, marker in zip(states, colors, markers):\n",
    "    iterations = range(len(history[state]))\n",
    "    values = history[state]\n",
    "    ax.plot(iterations, values, linewidth=2.5, color=color, marker=marker,\n",
    "           markersize=6, markevery=max(1, len(iterations)//10), \n",
    "           label=f'V({state})', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Iteration', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Value V(s)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Policy Evaluation: Value Function Convergence', \n",
    "            fontsize=15, fontweight='bold', pad=15)\n",
    "ax.legend(loc='right', fontsize=12, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "# Add convergence annotation\n",
    "ax.axvline(x=num_iters, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "ax.text(num_iters, ax.get_ylim()[1]*0.9, f'Converged(iter {num_iters})',\n",
    "       ha='center', fontsize=11, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Convergence Analysis:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Converged in {num_iters} iterations\")\n",
    "print(\"Final values:\")\n",
    "for state in states:\n",
    "    print(f\"  V({state}) = {V[state]:7.4f}\")\n",
    "\n",
    "print(\"\ud83d\udca1 Observations:\")\n",
    "print(\"   1. Values start at 0 and converge to true values\")\n",
    "print(\"   2. Terminal state (1,1) stays at 0\")\n",
    "print(\"   3. States closer to goal converge to higher values\")\n",
    "print(\"   4. Convergence is exponentially fast\")\n",
    "print(\"   5. Each iteration uses Bellman equation as update rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrating Policy Evaluation on a Larger Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger 4x4 grid world for more interesting demonstration\n",
    "def create_grid_world_mdp(size=4, goal=(3, 3), obstacles=None, gamma=0.9):\n",
    "    \"\"\"Create a grid world MDP.\n",
    "    \n",
    "    Args:\n",
    "        size: Grid size (size x size)\n",
    "        goal: Goal position (row, col)\n",
    "        obstacles: List of obstacle positions\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        mdp: SimpleMDP object\n",
    "    \"\"\"\n",
    "    if obstacles is None:\n",
    "        obstacles = []\n",
    "    \n",
    "    # Generate all states\n",
    "    states = []\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if (i, j) not in obstacles:\n",
    "                states.append(f'({i},{j})')\n",
    "    \n",
    "    actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    action_effects = {'UP': (-1, 0), 'DOWN': (1, 0), 'LEFT': (0, -1), 'RIGHT': (0, 1)}\n",
    "    \n",
    "    transitions = {}\n",
    "    rewards = {}\n",
    "    \n",
    "    for state_str in states:\n",
    "        # Parse state\n",
    "        state = eval(state_str)\n",
    "        \n",
    "        for action in actions:\n",
    "            # Calculate next state\n",
    "            delta = action_effects[action]\n",
    "            next_state = (state[0] + delta[0], state[1] + delta[1])\n",
    "            \n",
    "            # Check if next state is valid\n",
    "            if (0 <= next_state[0] < size and 0 <= next_state[1] < size and \n",
    "                next_state not in obstacles):\n",
    "                next_state_str = f'({next_state[0]},{next_state[1]})'\n",
    "            else:\n",
    "                # Hit wall or obstacle, stay in place\n",
    "                next_state_str = state_str\n",
    "            \n",
    "            transitions[(state_str, action)] = {next_state_str: 1.0}\n",
    "            \n",
    "            # Set rewards\n",
    "            if next_state == goal:\n",
    "                rewards[(state_str, action, next_state_str)] = 10.0\n",
    "            elif next_state_str == state_str and state != goal:\n",
    "                rewards[(state_str, action, next_state_str)] = -1.0  # Hit wall\n",
    "            else:\n",
    "                rewards[(state_str, action, next_state_str)] = -0.1  # Step cost\n",
    "    \n",
    "    return SimpleMDP(states, actions, transitions, rewards, gamma)\n",
    "\n",
    "\n",
    "# Create 4x4 grid world\n",
    "print(\"Policy Evaluation on 4x4 Grid World\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "grid_mdp = create_grid_world_mdp(size=4, goal=(3, 3), obstacles=[(1, 1), (2, 2)], gamma=0.9)\n",
    "\n",
    "print(\"Grid World: 4x4 with obstacles at (1,1) and (2,2)\")\n",
    "print(\"Goal: (3,3)\")\n",
    "print(f\"States: {len(grid_mdp.states)} states\")\n",
    "print(f\"Discount factor: \u03b3 = {grid_mdp.gamma}\")\n",
    "\n",
    "# Create a simple policy: move towards goal (right and down preferred)\n",
    "grid_policy = {}\n",
    "for state_str in grid_mdp.states:\n",
    "    state = eval(state_str)\n",
    "    \n",
    "    if state == (3, 3):  # Goal state\n",
    "        grid_policy[state_str] = {'RIGHT': 0.25, 'DOWN': 0.25, 'LEFT': 0.25, 'UP': 0.25}\n",
    "    else:\n",
    "        # Prefer moving towards goal\n",
    "        if state[0] < 3 and state[1] < 3:\n",
    "            grid_policy[state_str] = {'RIGHT': 0.4, 'DOWN': 0.4, 'LEFT': 0.1, 'UP': 0.1}\n",
    "        elif state[0] < 3:\n",
    "            grid_policy[state_str] = {'DOWN': 0.7, 'RIGHT': 0.1, 'LEFT': 0.1, 'UP': 0.1}\n",
    "        elif state[1] < 3:\n",
    "            grid_policy[state_str] = {'RIGHT': 0.7, 'DOWN': 0.1, 'LEFT': 0.1, 'UP': 0.1}\n",
    "        else:\n",
    "            grid_policy[state_str] = {'RIGHT': 0.25, 'DOWN': 0.25, 'LEFT': 0.25, 'UP': 0.25}\n",
    "\n",
    "print(\"Policy: Stochastic policy favoring movement towards goal\")\n",
    "print(\"Running policy evaluation...\")\n",
    "\n",
    "# Evaluate policy\n",
    "V_grid, num_iters = policy_evaluation(grid_mdp, grid_policy, gamma=0.9, theta=0.001)\n",
    "\n",
    "# Visualize value function as a grid\n",
    "print(\"Value Function V^\u03c0(s) as Grid:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "value_grid = np.full((4, 4), np.nan)\n",
    "for state_str, value in V_grid.items():\n",
    "    state = eval(state_str)\n",
    "    value_grid[state[0], state[1]] = value\n",
    "\n",
    "# Print as formatted grid\n",
    "print(\"     Col 0    Col 1    Col 2    Col 3\")\n",
    "print(\"   \" + \"-\"*42)\n",
    "for i in range(4):\n",
    "    row_str = f\"Row {i} |\"  \n",
    "    for j in range(4):\n",
    "        if np.isnan(value_grid[i, j]):\n",
    "            row_str += \"   XXX   \"\n",
    "        else:\n",
    "            row_str += f\" {value_grid[i, j]:6.2f}  \"\n",
    "    print(row_str)\n",
    "\n",
    "print(\"(XXX = obstacle)\")\n",
    "\n",
    "# Create heatmap visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "# Mask obstacles\n",
    "masked_grid = np.ma.masked_where(np.isnan(value_grid), value_grid)\n",
    "\n",
    "im = ax.imshow(masked_grid, cmap='RdYlGn', aspect='auto', interpolation='nearest')\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_yticks(range(4))\n",
    "ax.set_xticklabels(range(4), fontsize=12)\n",
    "ax.set_yticklabels(range(4), fontsize=12)\n",
    "ax.set_xlabel('Column', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Row', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Value Function V^\u03c0(s) Heatmap(Brighter = Higher Value)', \n",
    "            fontsize=15, fontweight='bold', pad=15)\n",
    "\n",
    "# Add value labels\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if not np.isnan(value_grid[i, j]):\n",
    "            text = ax.text(j, i, f'{value_grid[i, j]:.2f}',\n",
    "                         ha=\"center\", va=\"center\", color=\"black\", \n",
    "                         fontsize=11, fontweight='bold')\n",
    "        else:\n",
    "            text = ax.text(j, i, 'X',\n",
    "                         ha=\"center\", va=\"center\", color=\"white\", \n",
    "                         fontsize=20, fontweight='bold')\n",
    "\n",
    "# Mark goal\n",
    "ax.add_patch(plt.Rectangle((2.5, 2.5), 1, 1, fill=False, edgecolor='blue', linewidth=4))\n",
    "ax.text(3, 3.8, 'GOAL', ha='center', fontsize=12, fontweight='bold', color='blue')\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udca1 Key Observations:\")\n",
    "print(\"   1. Goal state (3,3) has highest value\")\n",
    "print(\"   2. Values decrease with distance from goal\")\n",
    "print(\"   3. Obstacles create 'shadows' in value function\")\n",
    "print(\"   4. Policy evaluation successfully computed V^\u03c0 for all states\")\n",
    "print(\"   5. This tells us how good each state is under the given policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Improvement: Finding Better Policies\n",
    "\n",
    "**From Evaluation to Improvement**\n",
    "\n",
    "Now that we can evaluate a policy, the natural question is: **Can we find a better policy?**\n",
    "\n",
    "The answer is yes, using the **Policy Improvement Theorem**!\n",
    "\n",
    "**Policy Improvement Theorem:**\n",
    "\n",
    "Given a policy $\\pi$ and its value function $V^\\pi$, we can create an improved policy $\\pi'$ by acting greedily with respect to $V^\\pi$:\n",
    "\n",
    "$\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V^\\pi(s')\\right]$\n",
    "\n",
    "Or equivalently, using Q-values:\n",
    "\n",
    "$\\pi'(s) = \\arg\\max_a Q^\\pi(s,a)$\n",
    "\n",
    "**The theorem guarantees:** $V^{\\pi'}(s) \\geq V^\\pi(s)$ for all states $s$\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "- We have $V^\\pi(s)$ telling us how good each state is under policy $\\pi$\n",
    "- For each state, we look one step ahead and choose the action that leads to the best expected value\n",
    "- This greedy policy must be at least as good as $\\pi$\n",
    "\n",
    "**Generalized Policy Iteration (GPI)**\n",
    "\n",
    "Combining policy evaluation and policy improvement gives us a powerful framework:\n",
    "\n",
    "```\n",
    "1. Initialize policy \u03c0 arbitrarily\n",
    "2. Repeat:\n",
    "   a. Policy Evaluation: Compute V^\u03c0\n",
    "   b. Policy Improvement: \u03c0' \u2190 greedy(V^\u03c0)\n",
    "   c. If \u03c0' = \u03c0, stop (optimal policy found)\n",
    "   d. \u03c0 \u2190 \u03c0'\n",
    "```\n",
    "\n",
    "This is called **Policy Iteration** and is guaranteed to converge to the optimal policy $\\pi^*$!\n",
    "\n",
    "**Why GPI Works:**\n",
    "\n",
    "- Evaluation makes the value function consistent with the current policy\n",
    "- Improvement makes the policy greedy with respect to the current value function\n",
    "- These two processes work together, pushing towards optimality\n",
    "- Convergence is guaranteed for finite MDPs\n",
    "\n",
    "Let's implement policy improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(mdp, V, gamma=0.9):\n",
    "    \"\"\"Improve a policy by acting greedily with respect to value function.\n",
    "    \n",
    "    Args:\n",
    "        mdp: MDP object\n",
    "        V: Value function (dict: state -> value)\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        new_policy: Improved policy (dict: state -> {action: probability})\n",
    "        policy_stable: Boolean indicating if policy changed\n",
    "    \"\"\"\n",
    "    new_policy = {}\n",
    "    policy_stable = True\n",
    "    \n",
    "    for state in mdp.states:\n",
    "        # Calculate Q(s,a) for all actions\n",
    "        q_values = {}\n",
    "        \n",
    "        for action in mdp.actions:\n",
    "            q = 0.0\n",
    "            next_states = mdp.transitions.get((state, action), {})\n",
    "            \n",
    "            for next_state, trans_prob in next_states.items():\n",
    "                reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                q += trans_prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            q_values[action] = q\n",
    "        \n",
    "        # Choose action(s) with maximum Q-value\n",
    "        if q_values:\n",
    "            max_q = max(q_values.values())\n",
    "            best_actions = [a for a, q in q_values.items() if np.isclose(q, max_q)]\n",
    "            \n",
    "            # Create deterministic policy (or uniform over best actions if tie)\n",
    "            new_policy[state] = {a: 1.0/len(best_actions) for a in best_actions}\n",
    "    \n",
    "    return new_policy, policy_stable\n",
    "\n",
    "\n",
    "def policy_iteration(mdp, gamma=0.9, theta=0.0001, max_iterations=100):\n",
    "    \"\"\"Find optimal policy using policy iteration.\n",
    "    \n",
    "    Args:\n",
    "        mdp: MDP object\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold for policy evaluation\n",
    "        max_iterations: Maximum number of policy iterations\n",
    "        \n",
    "    Returns:\n",
    "        policy: Optimal policy\n",
    "        V: Optimal value function\n",
    "        num_iterations: Number of iterations\n",
    "    \"\"\"\n",
    "    # Initialize with random policy\n",
    "    policy = {}\n",
    "    for state in mdp.states:\n",
    "        # Uniform random policy\n",
    "        policy[state] = {a: 1.0/len(mdp.actions) for a in mdp.actions}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Policy Evaluation\n",
    "        V, _ = policy_evaluation(mdp, policy, gamma, theta, max_iterations=1000)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        new_policy, policy_stable = policy_improvement(mdp, V, gamma)\n",
    "        \n",
    "        # Check if policy has converged\n",
    "        if policies_equal(policy, new_policy):\n",
    "            print(f\"Policy iteration converged in {iteration + 1} iterations\")\n",
    "            return new_policy, V, iteration + 1\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    print(f\"Policy iteration reached max iterations ({max_iterations})\")\n",
    "    return policy, V, max_iterations\n",
    "\n",
    "\n",
    "def policies_equal(policy1, policy2):\n",
    "    \"\"\"Check if two policies are equal.\"\"\"\n",
    "    if set(policy1.keys()) != set(policy2.keys()):\n",
    "        return False\n",
    "    \n",
    "    for state in policy1:\n",
    "        actions1 = policy1[state]\n",
    "        actions2 = policy2.get(state, {})\n",
    "        \n",
    "        if set(actions1.keys()) != set(actions2.keys()):\n",
    "            return False\n",
    "        \n",
    "        for action in actions1:\n",
    "            if not np.isclose(actions1[action], actions2.get(action, 0)):\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# Demonstrate policy iteration on 2x2 grid\n",
    "print(\"Policy Iteration Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Finding optimal policy for 2x2 Grid World...\")\n",
    "\n",
    "# Run policy iteration\n",
    "optimal_policy, optimal_V, num_iters = policy_iteration(mdp, gamma=0.9, theta=0.0001)\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"Optimal Policy \u03c0*:\")\n",
    "print(\"-\"*60)\n",
    "for state in states:\n",
    "    actions_str = \", \".join([f\"{a}({p:.2f})\" for a, p in optimal_policy[state].items() if p > 0])\n",
    "    print(f\"  \u03c0*({state}) = {actions_str}\")\n",
    "\n",
    "print(\"Optimal Value Function V*:\")\n",
    "print(\"-\"*60)\n",
    "for state in states:\n",
    "    print(f\"  V*({state}) = {optimal_V[state]:7.4f}\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83d\udca1 Interpretation:\")\n",
    "print(\"   - Policy iteration found the optimal policy\")\n",
    "print(\"   - From (0,0): Go RIGHT to (0,1)\")\n",
    "print(\"   - From (0,1): Go DOWN to goal (1,1)\")\n",
    "print(\"   - From (1,0): Go RIGHT to goal (1,1)\")\n",
    "print(\"   - This is the shortest path to the goal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration: A More Efficient Approach\n",
    "\n",
    "**Combining Evaluation and Improvement**\n",
    "\n",
    "Policy iteration works well but can be slow because it fully evaluates each policy. **Value iteration** provides a more efficient alternative by combining evaluation and improvement into a single update.\n",
    "\n",
    "**Value Iteration Algorithm:**\n",
    "\n",
    "Instead of alternating between full policy evaluation and improvement, value iteration updates values using the Bellman optimality equation:\n",
    "\n",
    "$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V_k(s')\\right]$\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "```\n",
    "1. Initialize V(s) = 0 for all states\n",
    "2. Repeat until convergence:\n",
    "   For each state s:\n",
    "     V(s) \u2190 max_a \u03a3 P(s'|s,a)[R(s,a,s') + \u03b3V(s')]\n",
    "3. Extract policy: \u03c0(s) = argmax_a \u03a3 P(s'|s,a)[R(s,a,s') + \u03b3V(s')]\n",
    "```\n",
    "\n",
    "**Key Differences from Policy Iteration:**\n",
    "\n",
    "1. **No explicit policy**: Works directly with value function\n",
    "2. **Single update**: Combines evaluation and improvement\n",
    "3. **Faster convergence**: Often requires fewer iterations\n",
    "4. **Simpler implementation**: No need to track policy during iteration\n",
    "\n",
    "**Why Value Iteration Works:**\n",
    "\n",
    "- Each update moves V closer to V*\n",
    "- The max operator implicitly improves the policy\n",
    "- Guaranteed to converge to V* (and thus \u03c0*)\n",
    "- Convergence rate is exponential in \u03b3\n",
    "\n",
    "**Relationship to Policy Iteration:**\n",
    "\n",
    "Value iteration is like policy iteration with just one sweep of policy evaluation per iteration. Both converge to the same optimal solution, but value iteration is often faster in practice.\n",
    "\n",
    "Let's implement value iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, gamma=0.9, theta=0.0001, max_iterations=1000):\n",
    "    \"\"\"Find optimal value function and policy using value iteration.\n",
    "    \n",
    "    Args:\n",
    "        mdp: MDP object\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold\n",
    "        max_iterations: Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        V: Optimal value function\n",
    "        policy: Optimal policy\n",
    "        num_iterations: Number of iterations\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        # Update each state\n",
    "        for state in mdp.states:\n",
    "            # Calculate max over actions\n",
    "            action_values = []\n",
    "            \n",
    "            for action in mdp.actions:\n",
    "                q = 0.0\n",
    "                next_states = mdp.transitions.get((state, action), {})\n",
    "                \n",
    "                for next_state, trans_prob in next_states.items():\n",
    "                    reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                    q += trans_prob * (reward + gamma * V[next_state])\n",
    "                \n",
    "                action_values.append(q)\n",
    "            \n",
    "            # Bellman optimality update\n",
    "            if action_values:\n",
    "                V_new[state] = max(action_values)\n",
    "                delta = max(delta, abs(V_new[state] - V[state]))\n",
    "        \n",
    "        V = V_new\n",
    "        \n",
    "        # Check convergence\n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged in {iteration + 1} iterations\")\n",
    "            \n",
    "            # Extract optimal policy\n",
    "            policy = {}\n",
    "            for state in mdp.states:\n",
    "                q_values = {}\n",
    "                for action in mdp.actions:\n",
    "                    q = 0.0\n",
    "                    next_states = mdp.transitions.get((state, action), {})\n",
    "                    for next_state, trans_prob in next_states.items():\n",
    "                        reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                        q += trans_prob * (reward + gamma * V[next_state])\n",
    "                    q_values[action] = q\n",
    "                \n",
    "                # Greedy policy\n",
    "                if q_values:\n",
    "                    max_q = max(q_values.values())\n",
    "                    best_actions = [a for a, q in q_values.items() if np.isclose(q, max_q)]\n",
    "                    policy[state] = {a: 1.0/len(best_actions) for a in best_actions}\n",
    "            \n",
    "            return V, policy, iteration + 1\n",
    "    \n",
    "    print(f\"Value iteration reached max iterations ({max_iterations})\")\n",
    "    \n",
    "    # Extract policy even if not converged\n",
    "    policy = {}\n",
    "    for state in mdp.states:\n",
    "        q_values = {}\n",
    "        for action in mdp.actions:\n",
    "            q = 0.0\n",
    "            next_states = mdp.transitions.get((state, action), {})\n",
    "            for next_state, trans_prob in next_states.items():\n",
    "                reward = mdp.rewards.get((state, action, next_state), 0.0)\n",
    "                q += trans_prob * (reward + gamma * V[next_state])\n",
    "            q_values[action] = q\n",
    "        \n",
    "        if q_values:\n",
    "            max_q = max(q_values.values())\n",
    "            best_actions = [a for a, q in q_values.items() if np.isclose(q, max_q)]\n",
    "            policy[state] = {a: 1.0/len(best_actions) for a in best_actions}\n",
    "    \n",
    "    return V, policy, max_iterations\n",
    "\n",
    "\n",
    "# Demonstrate value iteration\n",
    "print(\"Value Iteration Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Finding optimal policy for 2x2 Grid World using Value Iteration...\")\n",
    "\n",
    "# Run value iteration\n",
    "V_opt, policy_opt, num_iters_vi = value_iteration(mdp, gamma=0.9, theta=0.0001)\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"Optimal Policy \u03c0* (from Value Iteration):\")\n",
    "print(\"-\"*60)\n",
    "for state in states:\n",
    "    actions_str = \", \".join([f\"{a}({p:.2f})\" for a, p in policy_opt[state].items() if p > 0])\n",
    "    print(f\"  \u03c0*({state}) = {actions_str}\")\n",
    "\n",
    "print(\"Optimal Value Function V*:\")\n",
    "print(\"-\"*60)\n",
    "for state in states:\n",
    "    print(f\"  V*({state}) = {V_opt[state]:7.4f}\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83d\udca1 Comparison: Policy Iteration vs Value Iteration\")\n",
    "print(\"  Both methods found the same optimal solution!\")\n",
    "print(f\"  Policy Iteration: {num_iters} iterations\")\n",
    "print(f\"  Value Iteration: {num_iters_vi} iterations\")\n",
    "print(\"  Value iteration is often faster because it doesn't\")\n",
    "print(\"  fully evaluate each intermediate policy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Value Iteration on 4x4 Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply value iteration to the larger 4x4 grid world\n",
    "print(\"Value Iteration on 4x4 Grid World\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Running value iteration on 4x4 grid with obstacles...\")\n",
    "\n",
    "# Run value iteration\n",
    "V_grid_opt, policy_grid_opt, num_iters_grid = value_iteration(grid_mdp, gamma=0.9, theta=0.001)\n",
    "\n",
    "# Visualize optimal value function\n",
    "value_grid_opt = np.full((4, 4), np.nan)\n",
    "for state_str, value in V_grid_opt.items():\n",
    "    state = eval(state_str)\n",
    "    value_grid_opt[state[0], state[1]] = value\n",
    "\n",
    "# Create visualization with optimal policy arrows\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot 1: Optimal Value Function\n",
    "masked_grid = np.ma.masked_where(np.isnan(value_grid_opt), value_grid_opt)\n",
    "im1 = ax1.imshow(masked_grid, cmap='RdYlGn', aspect='auto', interpolation='nearest')\n",
    "ax1.set_xticks(range(4))\n",
    "ax1.set_yticks(range(4))\n",
    "ax1.set_xlabel('Column', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Row', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Optimal Value Function V*(s)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if not np.isnan(value_grid_opt[i, j]):\n",
    "            ax1.text(j, i, f'{value_grid_opt[i, j]:.2f}',\n",
    "                    ha=\"center\", va=\"center\", color=\"black\", \n",
    "                    fontsize=10, fontweight='bold')\n",
    "        else:\n",
    "            ax1.text(j, i, 'X', ha=\"center\", va=\"center\", \n",
    "                    color=\"white\", fontsize=18, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im1, ax=ax1, label='Value')\n",
    "\n",
    "# Plot 2: Optimal Policy\n",
    "ax2.set_xlim(-0.5, 3.5)\n",
    "ax2.set_ylim(-0.5, 3.5)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_xticks(range(4))\n",
    "ax2.set_yticks(range(4))\n",
    "ax2.set_xlabel('Column', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Row', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Optimal Policy \u03c0*(s)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# Draw policy arrows\n",
    "arrow_map = {'UP': (0, -0.3), 'DOWN': (0, 0.3), 'LEFT': (-0.3, 0), 'RIGHT': (0.3, 0)}\n",
    "\n",
    "for state_str, actions in policy_grid_opt.items():\n",
    "    state = eval(state_str)\n",
    "    i, j = state\n",
    "    \n",
    "    # Skip obstacles\n",
    "    if (i, j) in [(1, 1), (2, 2)]:\n",
    "        ax2.add_patch(plt.Rectangle((j-0.4, i-0.4), 0.8, 0.8, \n",
    "                                    fill=True, facecolor='gray', edgecolor='black', linewidth=2))\n",
    "        ax2.text(j, i, 'X', ha='center', va='center', \n",
    "                color='white', fontsize=18, fontweight='bold')\n",
    "        continue\n",
    "    \n",
    "    # Draw arrows for best action(s)\n",
    "    for action, prob in actions.items():\n",
    "        if prob > 0.1:  # Only draw if significant probability\n",
    "            dx, dy = arrow_map.get(action, (0, 0))\n",
    "            ax2.arrow(j, i, dx, dy, head_width=0.15, head_length=0.1,\n",
    "                     fc='blue', ec='blue', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Mark goal\n",
    "ax2.add_patch(plt.Circle((3, 3), 0.3, fill=True, facecolor='gold', \n",
    "                         edgecolor='darkgreen', linewidth=3))\n",
    "ax2.text(3, 3, 'G', ha='center', va='center', \n",
    "        color='darkgreen', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Converged in {num_iters_grid} iterations\")\n",
    "print(\"Optimal policy shows the best action in each state\")\n",
    "print(\"Arrows point towards the goal, avoiding obstacles\")\n",
    "\n",
    "print(\"\ud83d\udca1 Key Insights:\")\n",
    "print(\"   1. Value iteration found the optimal policy efficiently\")\n",
    "print(\"   2. Policy directs agent towards goal from any state\")\n",
    "print(\"   3. Obstacles are naturally avoided\")\n",
    "print(\"   4. V*(s) reflects optimal expected return from each state\")\n",
    "print(\"   5. This is the foundation for solving MDPs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='learning-paradigms'></a>\n",
    "#### Model-Based vs Model-Free Reinforcement Learning\n",
    "\n",
    "**A Fundamental Distinction in RL**\n",
    "\n",
    "Now that we've seen Dynamic Programming in action, it's important to understand a fundamental distinction in reinforcement learning: **model-based** vs **model-free** approaches.\n",
    "\n",
    "**Model-Based Reinforcement Learning**\n",
    "\n",
    "**Definition:** The agent has (or learns) a model of the environment's dynamics.\n",
    "\n",
    "**What is a \"model\"?**\n",
    "- Transition probabilities: $P(s'|s,a)$\n",
    "- Reward function: $R(s,a,s')$\n",
    "- Essentially, knowledge of how the environment works\n",
    "\n",
    "**Examples:**\n",
    "- Dynamic Programming (what we just learned!)\n",
    "- Planning algorithms\n",
    "- Simulators (e.g., chess, Go, robotics simulators)\n",
    "- Learned models (agent learns $P$ and $R$ from experience)\n",
    "\n",
    "**Advantages of Model-Based RL:**\n",
    "\n",
    "1. **Sample Efficiency**: Can plan without interacting with environment\n",
    "   - Simulate many trajectories mentally\n",
    "   - No need to try every action in every state\n",
    "   - Particularly valuable when real-world interactions are expensive\n",
    "\n",
    "2. **Faster Learning**: Can use planning algorithms\n",
    "   - Dynamic Programming guarantees optimal solution\n",
    "   - Can reason about consequences before acting\n",
    "   - Update values for all states simultaneously\n",
    "\n",
    "3. **Generalization**: Model can be used for multiple tasks\n",
    "   - Same model, different reward functions\n",
    "   - Transfer learning across related problems\n",
    "   - What-if analysis and counterfactual reasoning\n",
    "\n",
    "4. **Interpretability**: Can understand and debug the model\n",
    "   - Inspect transition probabilities\n",
    "   - Verify model correctness\n",
    "   - Explain agent's reasoning\n",
    "\n",
    "**Disadvantages of Model-Based RL:**\n",
    "\n",
    "1. **Model Errors**: If model is wrong, policy will be suboptimal\n",
    "   - \"All models are wrong, but some are useful\"\n",
    "   - Model errors compound over long horizons\n",
    "   - Difficult to model complex, stochastic environments\n",
    "\n",
    "2. **Computational Cost**: Planning can be expensive\n",
    "   - Need to solve Bellman equations\n",
    "   - Scales poorly with state/action space size\n",
    "   - Curse of dimensionality\n",
    "\n",
    "3. **Model Learning**: Learning accurate models is hard\n",
    "   - Requires lots of data\n",
    "   - High-dimensional state spaces are challenging\n",
    "   - Stochastic environments are difficult to model\n",
    "\n",
    "4. **Availability**: Many real-world problems lack good models\n",
    "   - Human behavior is hard to model\n",
    "   - Complex physical systems\n",
    "   - Unknown or changing dynamics\n",
    "\n",
    "**Model-Free Reinforcement Learning**\n",
    "\n",
    "**Definition:** The agent learns directly from experience without a model.\n",
    "\n",
    "**What does \"model-free\" mean?**\n",
    "- No knowledge of $P(s'|s,a)$ or $R(s,a,s')$\n",
    "- Learns value functions or policies directly\n",
    "- Trial-and-error learning\n",
    "\n",
    "**Examples:**\n",
    "- Monte Carlo methods\n",
    "- Temporal Difference learning (TD, SARSA, Q-learning)\n",
    "- Policy gradient methods\n",
    "- Deep RL (DQN, A3C, PPO)\n",
    "\n",
    "**Advantages of Model-Free RL:**\n",
    "\n",
    "1. **No Model Required**: Works when model is unknown or complex\n",
    "   - Don't need to know environment dynamics\n",
    "   - Can handle black-box environments\n",
    "   - Robust to model misspecification\n",
    "\n",
    "2. **Simpler**: Often easier to implement\n",
    "   - Direct learning from experience\n",
    "   - No need to learn or maintain a model\n",
    "   - Fewer components to debug\n",
    "\n",
    "3. **Scalability**: Can use function approximation\n",
    "   - Neural networks for large state spaces\n",
    "   - Generalization across states\n",
    "   - Handles continuous state/action spaces\n",
    "\n",
    "4. **Robustness**: Less sensitive to model errors\n",
    "   - Learns from actual experience\n",
    "   - Adapts to environment changes\n",
    "   - No compounding of model errors\n",
    "\n",
    "**Disadvantages of Model-Free RL:**\n",
    "\n",
    "1. **Sample Inefficiency**: Requires many interactions\n",
    "   - Must try actions to learn their value\n",
    "   - Can't simulate or plan ahead\n",
    "   - Expensive in real-world applications\n",
    "\n",
    "2. **Slower Learning**: No planning capability\n",
    "   - Must experience each state-action pair\n",
    "   - Can't reason about consequences\n",
    "   - Updates are local (one state at a time)\n",
    "\n",
    "3. **Exploration Challenges**: Must balance exploration/exploitation\n",
    "   - Risk of getting stuck in local optima\n",
    "   - May miss better strategies\n",
    "   - Requires careful exploration strategy\n",
    "\n",
    "4. **No Generalization Across Tasks**: Learns for specific reward\n",
    "   - Must relearn if reward function changes\n",
    "   - Limited transfer learning\n",
    "   - Task-specific knowledge\n",
    "\n",
    "**Why Model-Free Methods Are Needed**\n",
    "\n",
    "Despite the advantages of model-based RL, model-free methods are essential because:\n",
    "\n",
    "1. **Real-World Complexity**: Most real-world environments are too complex to model accurately\n",
    "   - Human interactions, market dynamics, weather patterns\n",
    "   - High-dimensional, stochastic, non-stationary\n",
    "\n",
    "2. **Unknown Dynamics**: Often we don't know how the environment works\n",
    "   - Black-box systems\n",
    "   - Proprietary or inaccessible internals\n",
    "\n",
    "3. **Model Errors Are Costly**: Wrong models lead to wrong policies\n",
    "   - Model-free methods learn from ground truth\n",
    "   - More robust in practice\n",
    "\n",
    "4. **Scalability**: Model-free + function approximation handles large spaces\n",
    "   - Deep RL successes (Atari, Go, robotics)\n",
    "   - Continuous control problems\n",
    "\n",
    "5. **Simplicity**: Easier to implement and debug\n",
    "   - Fewer moving parts\n",
    "   - Direct optimization of objective\n",
    "\n",
    "**The Spectrum: Hybrid Approaches**\n",
    "\n",
    "Modern RL often combines both approaches:\n",
    "\n",
    "- **Dyna**: Model-free learning + model-based planning\n",
    "- **Model-Based RL with Learned Models**: Learn model from data, use for planning\n",
    "- **Imagination-Augmented Agents**: Use model for auxiliary predictions\n",
    "- **World Models**: Learn compact model, train policy in model\n",
    "\n",
    "**When to Use Each Approach?**\n",
    "\n",
    "**Use Model-Based RL when:**\n",
    "- You have an accurate model (simulator, known dynamics)\n",
    "- Sample efficiency is critical (expensive interactions)\n",
    "- State space is small enough for planning\n",
    "- You need interpretability\n",
    "- Multiple tasks with same dynamics\n",
    "\n",
    "**Use Model-Free RL when:**\n",
    "- Model is unknown or too complex\n",
    "- Environment is high-dimensional\n",
    "- You have abundant data/interactions\n",
    "- Robustness to model errors is important\n",
    "- Simplicity is preferred\n",
    "\n",
    "Let's visualize this distinction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison visualization\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Model-Based RL Diagram\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Model-Based Reinforcement Learning', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Agent\n",
    "agent_box = mpatches.FancyBboxPatch((1, 7), 2, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                    edgecolor='blue', facecolor='lightblue', linewidth=3)\n",
    "ax1.add_patch(agent_box)\n",
    "ax1.text(2, 7.75, 'Agent', ha='center', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Model\n",
    "model_box = mpatches.FancyBboxPatch((4, 7), 2, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                    edgecolor='green', facecolor='lightgreen', linewidth=3)\n",
    "ax1.add_patch(model_box)\n",
    "ax1.text(5, 7.75, 'ModelP(s\\'|s,a)R(s,a,s\\')', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Environment\n",
    "env_box = mpatches.FancyBboxPatch((7, 7), 2, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                  edgecolor='red', facecolor='lightcoral', linewidth=3)\n",
    "ax1.add_patch(env_box)\n",
    "ax1.text(8, 7.75, 'Environment', ha='center', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Arrows\n",
    "ax1.annotate('', xy=(4, 7.75), xytext=(3, 7.75), \n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "ax1.text(3.5, 8.2, 'Query', ha='center', fontsize=10)\n",
    "\n",
    "ax1.annotate('', xy=(7, 7.75), xytext=(6, 7.75), \n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "ax1.text(6.5, 8.2, 'Action', ha='center', fontsize=10)\n",
    "\n",
    "ax1.annotate('', xy=(8, 7), xytext=(8, 6.5), \n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "ax1.annotate('', xy=(2, 7), xytext=(2, 6.5), \n",
    "            arrowprops=dict(arrowstyle='<-', lw=2, color='black'))\n",
    "ax1.text(5, 6.2, 'State, Reward', ha='center', fontsize=10)\n",
    "\n",
    "# Planning\n",
    "plan_box = mpatches.FancyBboxPatch((1, 4), 5, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                   edgecolor='purple', facecolor='plum', linewidth=3, linestyle='--')\n",
    "ax1.add_patch(plan_box)\n",
    "ax1.text(3.5, 4.75, 'Planning(DP, Value Iteration)', ha='center', va='center', \n",
    "        fontsize=12, fontweight='bold')\n",
    "\n",
    "ax1.annotate('', xy=(3.5, 5.5), xytext=(3.5, 7), \n",
    "            arrowprops=dict(arrowstyle='<->', lw=2, color='purple', linestyle='--'))\n",
    "\n",
    "# Advantages/Disadvantages\n",
    "ax1.text(5, 2.5, 'Advantages:', fontsize=12, fontweight='bold', color='green')\n",
    "ax1.text(5, 2, '\u2022 Sample efficient', fontsize=10, ha='center')\n",
    "ax1.text(5, 1.6, '\u2022 Can plan ahead', fontsize=10, ha='center')\n",
    "ax1.text(5, 1.2, '\u2022 Fast learning', fontsize=10, ha='center')\n",
    "\n",
    "ax1.text(5, 0.5, 'Disadvantages:', fontsize=12, fontweight='bold', color='red')\n",
    "ax1.text(5, 0, '\u2022 Requires accurate model', fontsize=10, ha='center')\n",
    "ax1.text(5, -0.4, '\u2022 Model errors compound', fontsize=10, ha='center')\n",
    "\n",
    "# Model-Free RL Diagram\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Model-Free Reinforcement Learning', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Agent\n",
    "agent_box2 = mpatches.FancyBboxPatch((2, 7), 2.5, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                     edgecolor='blue', facecolor='lightblue', linewidth=3)\n",
    "ax2.add_patch(agent_box2)\n",
    "ax2.text(3.25, 7.75, 'Agent(Q-learning,Policy Gradient)', ha='center', va='center', \n",
    "        fontsize=11, fontweight='bold')\n",
    "\n",
    "# Environment\n",
    "env_box2 = mpatches.FancyBboxPatch((5.5, 7), 2.5, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                   edgecolor='red', facecolor='lightcoral', linewidth=3)\n",
    "ax2.add_patch(env_box2)\n",
    "ax2.text(6.75, 7.75, 'Environment', ha='center', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Direct interaction arrows\n",
    "ax2.annotate('', xy=(5.5, 8), xytext=(4.5, 8), \n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='black'))\n",
    "ax2.text(5, 8.5, 'Action', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax2.annotate('', xy=(4.5, 7.5), xytext=(5.5, 7.5), \n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='black'))\n",
    "ax2.text(5, 7, 'State, Reward', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Direct learning\n",
    "learn_box = mpatches.FancyBboxPatch((2, 4.5), 6, 1.5, boxstyle=\"round,pad=0.1\", \n",
    "                                    edgecolor='orange', facecolor='lightyellow', linewidth=3)\n",
    "ax2.add_patch(learn_box)\n",
    "ax2.text(5, 5.25, 'Direct Learning from Experience(No Model)', ha='center', va='center', \n",
    "        fontsize=12, fontweight='bold')\n",
    "\n",
    "ax2.annotate('', xy=(5, 6), xytext=(5, 7), \n",
    "            arrowprops=dict(arrowstyle='<->', lw=2, color='orange'))\n",
    "\n",
    "# Advantages/Disadvantages\n",
    "ax2.text(5, 2.5, 'Advantages:', fontsize=12, fontweight='bold', color='green')\n",
    "ax2.text(5, 2, '\u2022 No model required', fontsize=10, ha='center')\n",
    "ax2.text(5, 1.6, '\u2022 Robust to model errors', fontsize=10, ha='center')\n",
    "ax2.text(5, 1.2, '\u2022 Simpler implementation', fontsize=10, ha='center')\n",
    "\n",
    "ax2.text(5, 0.5, 'Disadvantages:', fontsize=12, fontweight='bold', color='red')\n",
    "ax2.text(5, 0, '\u2022 Sample inefficient', fontsize=10, ha='center')\n",
    "ax2.text(5, -0.4, '\u2022 Slower learning', fontsize=10, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Model-Based vs Model-Free RL\")\n",
    "print(\"=\"*60)\n",
    "print(\"Model-Based RL (e.g., Dynamic Programming):\")\n",
    "print(\"  \u2022 Uses model of environment (P, R)\")\n",
    "print(\"  \u2022 Can plan without interacting\")\n",
    "print(\"  \u2022 Sample efficient but requires accurate model\")\n",
    "print(\"  \u2022 Examples: DP, Dyna, Model-based planning\")\n",
    "\n",
    "print(\"Model-Free RL (e.g., Q-learning):\")\n",
    "print(\"  \u2022 Learns directly from experience\")\n",
    "print(\"  \u2022 No model of environment needed\")\n",
    "print(\"  \u2022 Sample inefficient but robust\")\n",
    "print(\"  \u2022 Examples: Monte Carlo, TD, Q-learning, Policy Gradients\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83c\udfaf Key Takeaway:\")\n",
    "print(\"   Dynamic Programming is model-based - it requires perfect\")\n",
    "print(\"   knowledge of the environment. In the next sections, we'll\")\n",
    "print(\"   learn model-free methods that work without this knowledge!\")\n",
    "print(\"   Model-free methods are essential for real-world RL where\")\n",
    "print(\"   we don't have access to perfect models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary: Dynamic Programming and Learning Paradigms**\n",
    "\n",
    "In this section, we've covered:\n",
    "\n",
    "1. **Policy Evaluation**: Computing $V^\\pi(s)$ for a given policy using iterative Bellman updates\n",
    "2. **Policy Improvement**: Finding better policies by acting greedily with respect to value functions\n",
    "3. **Value Iteration**: Efficiently finding optimal policies by combining evaluation and improvement\n",
    "4. **Model-Based vs Model-Free**: Understanding when we need models and when we can learn without them\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "- Dynamic Programming provides exact solutions when we have perfect models\n",
    "- Policy iteration and value iteration both converge to optimal policies\n",
    "- The Bellman equations are the foundation for all these algorithms\n",
    "- Model-based methods are sample-efficient but require accurate models\n",
    "- Model-free methods are more practical for real-world problems\n",
    "\n",
    "**What's Next:**\n",
    "\n",
    "In the following sections, we'll explore model-free methods that learn directly from experience:\n",
    "- Monte Carlo methods (learn from complete episodes)\n",
    "- Temporal Difference learning (learn from every step)\n",
    "- Q-learning (off-policy TD control)\n",
    "- Deep RL (handling large state spaces)\n",
    "\n",
    "These methods form the foundation of modern reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Section 2: Core Algorithms\n",
    "\n",
    "In this section, we'll explore the fundamental algorithms that enable agents to learn optimal policies. We'll start with Monte Carlo methods, which learn from complete episodes, then progress to Temporal Difference methods that can learn from individual steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='monte-carlo'></a>\n",
    "### Monte Carlo Methods\n",
    "\n",
    "**Learning from Complete Episodes**\n",
    "\n",
    "Monte Carlo (MC) methods are a class of reinforcement learning algorithms that learn by averaging sample returns from complete episodes. Unlike Dynamic Programming, MC methods don't require a model of the environment - they learn directly from experience.\n",
    "\n",
    "**The Core Principle:**\n",
    "\n",
    "Monte Carlo methods estimate value functions by **averaging the actual returns** observed after visiting states. The key insight is:\n",
    "\n",
    "*\"The value of a state is the expected return starting from that state. If we experience many episodes and average the returns, we'll get a good estimate of the true value.\"*\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "1. **Episode-Based Learning**: Must wait until episode ends to update values\n",
    "2. **Model-Free**: Don't need to know transition probabilities or rewards\n",
    "3. **Sample-Based**: Learn from actual experience, not from a model\n",
    "4. **Unbiased Estimates**: Returns are actual outcomes, not bootstrapped estimates\n",
    "5. **High Variance**: Individual returns can vary significantly\n",
    "\n",
    "**When to Use Monte Carlo Methods:**\n",
    "\n",
    "- \u2713 Episodic tasks (games, simulations with clear endings)\n",
    "- \u2713 When you don't have a model of the environment\n",
    "- \u2713 When you can simulate or experience complete episodes\n",
    "- \u2717 Continuing tasks (no natural episode boundaries)\n",
    "- \u2717 When episodes are very long (slow learning)\n",
    "\n",
    "**Comparison with Dynamic Programming:**\n",
    "\n",
    "| Aspect | Dynamic Programming | Monte Carlo |\n",
    "|--------|-------------------|-------------|\n",
    "| Model Required | Yes (need P and R) | No (model-free) |\n",
    "| Learning | From model | From experience |\n",
    "| Updates | Every state | Only visited states |\n",
    "| Bootstrapping | Yes (use estimates) | No (use actual returns) |\n",
    "| Variance | Low | High |\n",
    "| Bias | Depends on initialization | Unbiased |\n",
    "\n",
    "Let's explore Monte Carlo methods in detail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Prediction: Estimating Value Functions\n",
    "\n",
    "**The Goal:** Estimate the state-value function $V^\\pi(s)$ for a given policy $\\pi$.\n",
    "\n",
    "**The Approach:** \n",
    "1. Follow policy $\\pi$ to generate episodes\n",
    "2. For each state visited, record the return (cumulative discounted reward)\n",
    "3. Average the returns to estimate the value\n",
    "\n",
    "**Two Variants: First-Visit vs Every-Visit MC**\n",
    "\n",
    "**First-Visit Monte Carlo:**\n",
    "- Only count the **first time** a state is visited in an episode\n",
    "- Average returns from first visits only\n",
    "- Theoretically guaranteed to converge to true value\n",
    "- More commonly used in practice\n",
    "\n",
    "**Every-Visit Monte Carlo:**\n",
    "- Count **every time** a state is visited in an episode\n",
    "- Average returns from all visits\n",
    "- Also converges to true value\n",
    "- Can learn faster in some cases\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "For a state $s$ visited at time $t$ in an episode:\n",
    "\n",
    "**Return from that visit:**\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
    "\n",
    "**Value estimate (after $n$ visits):**\n",
    "$V(s) = \\frac{1}{n} \\sum_{i=1}^{n} G_i(s)$\n",
    "\n",
    "where $G_i(s)$ is the return following the $i$-th visit to state $s$.\n",
    "\n",
    "**Incremental Update Formula:**\n",
    "\n",
    "Instead of storing all returns and averaging, we can update incrementally:\n",
    "\n",
    "$V(s) \\leftarrow V(s) + \\frac{1}{N(s)} [G - V(s)]$\n",
    "\n",
    "where:\n",
    "- $N(s)$ = number of times state $s$ has been visited\n",
    "- $G$ = return observed from this visit\n",
    "- $\\frac{1}{N(s)}$ = step size (learning rate)\n",
    "\n",
    "This is equivalent to:\n",
    "$V(s) \\leftarrow V(s) + \\alpha [G - V(s)]$\n",
    "\n",
    "where $\\alpha$ is a constant step size (useful for non-stationary problems).\n",
    "\n",
    "Let's implement both first-visit and every-visit Monte Carlo prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy, max_steps=100):\n",
    "    \"\"\"Generate an episode following a given policy.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment with reset() and step() methods\n",
    "        policy: Function that takes state and returns action\n",
    "        max_steps: Maximum steps per episode\n",
    "        \n",
    "    Returns:\n",
    "        episode: List of (state, action, reward) tuples\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = policy(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        episode.append((state, action, reward))\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return episode\n",
    "\n",
    "\n",
    "def calculate_returns(episode, gamma=0.9):\n",
    "    \"\"\"Calculate returns for each step in an episode.\n",
    "    \n",
    "    Args:\n",
    "        episode: List of (state, action, reward) tuples\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        returns: List of returns, one for each step\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    \n",
    "    # Calculate returns backwards from end of episode\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = reward + gamma * G\n",
    "        returns.insert(0, G)  # Insert at beginning to maintain order\n",
    "    \n",
    "    return returns\n",
    "\n",
    "\n",
    "def mc_prediction_first_visit(env, policy, num_episodes=1000, gamma=0.9):\n",
    "    \"\"\"First-visit Monte Carlo prediction.\n",
    "    \n",
    "    Estimates V(s) by averaging returns from first visits to each state.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        policy: Policy to evaluate (function: state -> action)\n",
    "        num_episodes: Number of episodes to generate\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        V: Dictionary mapping states to estimated values\n",
    "        returns_history: List of returns for each episode (for visualization)\n",
    "    \"\"\"\n",
    "    # Initialize value function and visit counts\n",
    "    V = defaultdict(float)\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    returns_history = []\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode\n",
    "        episode = generate_episode(env, policy)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Track total return for this episode\n",
    "        returns_history.append(returns[0] if returns else 0)\n",
    "        \n",
    "        # Track which states we've seen in this episode (for first-visit)\n",
    "        visited_states = set()\n",
    "        \n",
    "        # Update value estimates\n",
    "        for t, (state, action, reward) in enumerate(episode):\n",
    "            # First-visit: only update if this is the first time seeing this state\n",
    "            if state not in visited_states:\n",
    "                visited_states.add(state)\n",
    "                \n",
    "                # Add return to sum\n",
    "                returns_sum[state] += returns[t]\n",
    "                returns_count[state] += 1\n",
    "                \n",
    "                # Update value estimate (average of returns)\n",
    "                V[state] = returns_sum[state] / returns_count[state]\n",
    "    \n",
    "    return dict(V), returns_history\n",
    "\n",
    "\n",
    "def mc_prediction_every_visit(env, policy, num_episodes=1000, gamma=0.9):\n",
    "    \"\"\"Every-visit Monte Carlo prediction.\n",
    "    \n",
    "    Estimates V(s) by averaging returns from all visits to each state.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        policy: Policy to evaluate (function: state -> action)\n",
    "        num_episodes: Number of episodes to generate\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        V: Dictionary mapping states to estimated values\n",
    "        returns_history: List of returns for each episode (for visualization)\n",
    "    \"\"\"\n",
    "    # Initialize value function and visit counts\n",
    "    V = defaultdict(float)\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    returns_history = []\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode\n",
    "        episode = generate_episode(env, policy)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Track total return for this episode\n",
    "        returns_history.append(returns[0] if returns else 0)\n",
    "        \n",
    "        # Update value estimates\n",
    "        for t, (state, action, reward) in enumerate(episode):\n",
    "            # Every-visit: update for every occurrence of the state\n",
    "            returns_sum[state] += returns[t]\n",
    "            returns_count[state] += 1\n",
    "            \n",
    "            # Update value estimate (average of returns)\n",
    "            V[state] = returns_sum[state] / returns_count[state]\n",
    "    \n",
    "    return dict(V), returns_history\n",
    "\n",
    "\n",
    "print(\"Monte Carlo Prediction Implementation\")\n",
    "print(\"=\"*60)\n",
    "print(\"Implemented:\")\n",
    "print(\"  \u2713 First-Visit MC Prediction\")\n",
    "print(\"  \u2713 Every-Visit MC Prediction\")\n",
    "print(\"  \u2713 Episode generation\")\n",
    "print(\"  \u2713 Return calculation\")\n",
    "print(\"Ready to evaluate policies on episodic environments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrating Monte Carlo Prediction on Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple policy for the grid world\n",
    "def random_policy(state):\n",
    "    \"\"\"Random policy: choose actions uniformly at random.\"\"\"\n",
    "    return np.random.randint(0, 4)  # 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "\n",
    "\n",
    "def greedy_policy(state):\n",
    "    \"\"\"Greedy policy: always move toward goal (4,4).\"\"\"\n",
    "    row, col = state\n",
    "    goal_row, goal_col = 4, 4\n",
    "    \n",
    "    # Move right if not at rightmost column\n",
    "    if col < goal_col:\n",
    "        return 1  # RIGHT\n",
    "    # Move down if not at bottom row\n",
    "    elif row < goal_row:\n",
    "        return 2  # DOWN\n",
    "    # Otherwise move randomly\n",
    "    else:\n",
    "        return np.random.randint(0, 4)\n",
    "\n",
    "\n",
    "print(\"Evaluating Policies with Monte Carlo Prediction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnvironment(grid_size=5)\n",
    "\n",
    "print(\"Environment: 5x5 Grid World\")\n",
    "print(f\"  Start: (0,0)\")\n",
    "print(f\"  Goal: {env.goal_pos}\")\n",
    "print(f\"  Obstacles: {env.obstacles}\")\n",
    "\n",
    "# Evaluate random policy\n",
    "print(\"\" + \"-\"*60)\n",
    "print(\"Evaluating Random Policy with First-Visit MC...\")\n",
    "V_random_fv, returns_random_fv = mc_prediction_first_visit(\n",
    "    env, random_policy, num_episodes=5000, gamma=0.9\n",
    ")\n",
    "\n",
    "print(\"Evaluating Random Policy with Every-Visit MC...\")\n",
    "V_random_ev, returns_random_ev = mc_prediction_every_visit(\n",
    "    env, random_policy, num_episodes=5000, gamma=0.9\n",
    ")\n",
    "\n",
    "# Evaluate greedy policy\n",
    "print(\"Evaluating Greedy Policy with First-Visit MC...\")\n",
    "V_greedy_fv, returns_greedy_fv = mc_prediction_first_visit(\n",
    "    env, greedy_policy, num_episodes=5000, gamma=0.9\n",
    ")\n",
    "\n",
    "print(\"Evaluating Greedy Policy with Every-Visit MC...\")\n",
    "V_greedy_ev, returns_greedy_ev = mc_prediction_every_visit(\n",
    "    env, greedy_policy, num_episodes=5000, gamma=0.9\n",
    ")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\u2713 Evaluation complete!\")\n",
    "print(f\"Random Policy:\")\n",
    "print(f\"  States evaluated: {len(V_random_fv)}\")\n",
    "print(f\"  Start state value (First-Visit): {V_random_fv.get((0,0), 0):.2f}\")\n",
    "print(f\"  Start state value (Every-Visit): {V_random_ev.get((0,0), 0):.2f}\")\n",
    "\n",
    "print(f\"Greedy Policy:\")\n",
    "print(f\"  States evaluated: {len(V_greedy_fv)}\")\n",
    "print(f\"  Start state value (First-Visit): {V_greedy_fv.get((0,0), 0):.2f}\")\n",
    "print(f\"  Start state value (Every-Visit): {V_greedy_ev.get((0,0), 0):.2f}\")\n",
    "\n",
    "print(\"\ud83d\udca1 Observation: Greedy policy has higher value (reaches goal faster)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Value Function Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how value estimates converge over episodes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Random Policy - Returns over episodes\n",
    "ax = axes[0, 0]\n",
    "window = 100\n",
    "smoothed_random_fv = np.convolve(returns_random_fv, np.ones(window)/window, mode='valid')\n",
    "smoothed_random_ev = np.convolve(returns_random_ev, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax.plot(smoothed_random_fv, label='First-Visit', linewidth=2, alpha=0.8)\n",
    "ax.plot(smoothed_random_ev, label='Every-Visit', linewidth=2, alpha=0.8)\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Average Return', fontsize=11)\n",
    "ax.set_title('Random Policy: Return Convergence', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Greedy Policy - Returns over episodes\n",
    "ax = axes[0, 1]\n",
    "smoothed_greedy_fv = np.convolve(returns_greedy_fv, np.ones(window)/window, mode='valid')\n",
    "smoothed_greedy_ev = np.convolve(returns_greedy_ev, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax.plot(smoothed_greedy_fv, label='First-Visit', linewidth=2, alpha=0.8, color='green')\n",
    "ax.plot(smoothed_greedy_ev, label='Every-Visit', linewidth=2, alpha=0.8, color='lightgreen')\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Average Return', fontsize=11)\n",
    "ax.set_title('Greedy Policy: Return Convergence', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Value function heatmap for Random Policy (First-Visit)\n",
    "ax = axes[1, 0]\n",
    "value_grid = np.zeros((5, 5))\n",
    "for (row, col), value in V_random_fv.items():\n",
    "    value_grid[row, col] = value\n",
    "\n",
    "im = ax.imshow(value_grid, cmap='RdYlGn', aspect='auto')\n",
    "ax.set_title('Random Policy: Value Function (First-Visit)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Column', fontsize=11)\n",
    "ax.set_ylabel('Row', fontsize=11)\n",
    "\n",
    "# Add value labels\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        text = ax.text(j, i, f'{value_grid[i, j]:.1f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 4: Value function heatmap for Greedy Policy (First-Visit)\n",
    "ax = axes[1, 1]\n",
    "value_grid = np.zeros((5, 5))\n",
    "for (row, col), value in V_greedy_fv.items():\n",
    "    value_grid[row, col] = value\n",
    "\n",
    "im = ax.imshow(value_grid, cmap='RdYlGn', aspect='auto')\n",
    "ax.set_title('Greedy Policy: Value Function (First-Visit)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Column', fontsize=11)\n",
    "ax.set_ylabel('Row', fontsize=11)\n",
    "\n",
    "# Add value labels\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        text = ax.text(j, i, f'{value_grid[i, j]:.1f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Visualization Insights:\")\n",
    "print(\"1. Top Row: Returns converge as more episodes are sampled\")\n",
    "print(\"   - First-visit and every-visit produce similar estimates\")\n",
    "print(\"   - Greedy policy achieves higher returns than random\")\n",
    "\n",
    "print(\"2. Bottom Row: Value function heatmaps\")\n",
    "print(\"   - Brighter colors = higher values (closer to goal)\")\n",
    "print(\"   - Values increase as we approach goal state (4,4)\")\n",
    "print(\"   - Greedy policy has higher values overall\")\n",
    "\n",
    "print(\"3. Key Takeaway:\")\n",
    "print(\"   Monte Carlo successfully estimates state values from experience!\")\n",
    "print(\"   No model required - just sample episodes and average returns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On-Policy Monte Carlo Control\n",
    "\n",
    "**From Prediction to Control: Learning Optimal Policies**\n",
    "\n",
    "So far, we've used Monte Carlo methods for **prediction** - evaluating a given policy. Now we'll use MC for **control** - finding the optimal policy.\n",
    "\n",
    "**The Control Problem:**\n",
    "\n",
    "Given: An environment (MDP without model)\n",
    "Goal: Find the optimal policy $\\pi^*$ that maximizes expected return\n",
    "\n",
    "**On-Policy Learning:**\n",
    "\n",
    "In **on-policy** learning, we learn about and improve the same policy that we're using to generate behavior. The agent:\n",
    "1. Acts according to policy $\\pi$\n",
    "2. Learns from that experience\n",
    "3. Improves policy $\\pi$\n",
    "4. Repeats\n",
    "\n",
    "This contrasts with **off-policy** learning (covered later), where the agent learns about one policy while following another.\n",
    "\n",
    "**The Algorithm: Monte Carlo Control with Epsilon-Greedy**\n",
    "\n",
    "We can't use a purely greedy policy (always exploit) because we need exploration. The solution: **epsilon-greedy exploration**.\n",
    "\n",
    "**Key Idea:** Instead of learning $V(s)$, we learn $Q(s,a)$ (action-values), which tells us the value of taking action $a$ in state $s$.\n",
    "\n",
    "**Algorithm Steps:**\n",
    "\n",
    "1. **Initialize**: \n",
    "   - $Q(s,a) = 0$ for all states and actions\n",
    "   - $\\pi$ = epsilon-greedy policy based on $Q$\n",
    "\n",
    "2. **Repeat** for many episodes:\n",
    "   - **Generate episode** following $\\pi$: $S_0, A_0, R_1, S_1, A_1, R_2, ..., S_T$\n",
    "   - **For each** state-action pair $(s,a)$ in the episode:\n",
    "     - Calculate return $G$ following first visit to $(s,a)$\n",
    "     - Update: $Q(s,a) \\leftarrow \\text{average of returns following } (s,a)$\n",
    "   - **Improve policy**: $\\pi \\leftarrow$ epsilon-greedy with respect to $Q$\n",
    "\n",
    "**Epsilon-Greedy Policy:**\n",
    "\n",
    "$\\pi(a|s) = \\begin{cases}\n",
    "1 - \\epsilon + \\frac{\\epsilon}{|A(s)|} & \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\\n",
    "\\frac{\\epsilon}{|A(s)|} & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "**Why This Works:**\n",
    "\n",
    "1. **Exploration**: Epsilon-greedy ensures all actions are tried\n",
    "2. **Exploitation**: Mostly choose actions with highest Q-values\n",
    "3. **Improvement**: Policy gets better as Q-values become more accurate\n",
    "4. **Convergence**: Under certain conditions, converges to optimal epsilon-greedy policy\n",
    "\n",
    "**Generalized Policy Iteration (GPI):**\n",
    "\n",
    "MC Control is an instance of GPI:\n",
    "- **Policy Evaluation**: Estimate $Q^\\pi$ using MC sampling\n",
    "- **Policy Improvement**: Make policy greedy with respect to $Q$\n",
    "- **Iterate**: These two processes work together to find $\\pi^*$\n",
    "\n",
    "Let's implement on-policy MC control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "    \"\"\"Epsilon-greedy policy based on Q-values.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon=0.1, num_actions=4):\n",
    "        \"\"\"Initialize epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            epsilon: Probability of random action\n",
    "            num_actions: Number of possible actions\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.num_actions = num_actions\n",
    "        self.Q = defaultdict(lambda: np.zeros(num_actions))\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy strategy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            \n",
    "        Returns:\n",
    "            action: Selected action\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return np.random.randint(0, self.num_actions)\n",
    "        else:\n",
    "            # Exploit: best action according to Q\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        \"\"\"Get the greedy action (for evaluation).\"\"\"\n",
    "        return np.argmax(self.Q[state])\n",
    "\n",
    "\n",
    "def mc_control_on_policy(env, num_episodes=10000, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"On-policy Monte Carlo control with epsilon-greedy exploration.\n",
    "    \n",
    "    Learns optimal policy by:\n",
    "    1. Generating episodes with epsilon-greedy policy\n",
    "    2. Updating Q-values from returns\n",
    "    3. Improving policy to be greedy w.r.t. Q\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        num_episodes: Number of episodes to run\n",
    "        gamma: Discount factor\n",
    "        epsilon: Exploration probability\n",
    "        \n",
    "    Returns:\n",
    "        policy: Learned epsilon-greedy policy\n",
    "        Q: Learned action-value function\n",
    "        stats: Dictionary with learning statistics\n",
    "    \"\"\"\n",
    "    # Initialize policy\n",
    "    policy = EpsilonGreedyPolicy(epsilon=epsilon, num_actions=4)\n",
    "    \n",
    "    # Track returns for each state-action pair\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    \n",
    "    # Statistics for tracking progress\n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode using current policy\n",
    "        episode = generate_episode(env, policy.get_action, max_steps=100)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Track statistics\n",
    "        episode_returns.append(returns[0] if returns else 0)\n",
    "        episode_lengths.append(len(episode))\n",
    "        \n",
    "        # Track visited state-action pairs (for first-visit)\n",
    "        visited_pairs = set()\n",
    "        \n",
    "        # Update Q-values\n",
    "        for t, (state, action, reward) in enumerate(episode):\n",
    "            pair = (state, action)\n",
    "            \n",
    "            # First-visit MC\n",
    "            if pair not in visited_pairs:\n",
    "                visited_pairs.add(pair)\n",
    "                \n",
    "                # Update return statistics\n",
    "                returns_sum[pair] += returns[t]\n",
    "                returns_count[pair] += 1\n",
    "                \n",
    "                # Update Q-value (average of returns)\n",
    "                policy.Q[state][action] = returns_sum[pair] / returns_count[pair]\n",
    "        \n",
    "        # Policy improvement happens automatically through epsilon-greedy\n",
    "        # (policy always acts epsilon-greedy w.r.t. current Q)\n",
    "    \n",
    "    stats = {\n",
    "        'episode_returns': episode_returns,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'states_visited': len(policy.Q)\n",
    "    }\n",
    "    \n",
    "    return policy, dict(policy.Q), stats\n",
    "\n",
    "\n",
    "print(\"On-Policy Monte Carlo Control Implementation\")\n",
    "print(\"=\"*60)\n",
    "print(\"Implemented:\")\n",
    "print(\"  \u2713 Epsilon-greedy policy class\")\n",
    "print(\"  \u2713 On-policy MC control algorithm\")\n",
    "print(\"  \u2713 Q-value learning from episodes\")\n",
    "print(\"  \u2713 Policy improvement through GPI\")\n",
    "print(\"Ready to learn optimal policies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Optimal Policy in Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn optimal policy using MC control\n",
    "print(\"Learning Optimal Policy with On-Policy MC Control\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnvironment(grid_size=5)\n",
    "\n",
    "print(\"Environment: 5x5 Grid World\")\n",
    "print(f\"  Start: (0,0)\")\n",
    "print(f\"  Goal: {env.goal_pos}\")\n",
    "print(f\"  Obstacles: {env.obstacles}\")\n",
    "print(f\"  Actions: {env.actions}\")\n",
    "\n",
    "print(\"Training agent with MC control...\")\n",
    "print(\"  Episodes: 10,000\")\n",
    "print(\"  Epsilon: 0.1\")\n",
    "print(\"  Gamma: 0.9\")\n",
    "\n",
    "# Train agent\n",
    "policy, Q, stats = mc_control_on_policy(\n",
    "    env, \n",
    "    num_episodes=10000, \n",
    "    gamma=0.9, \n",
    "    epsilon=0.1\n",
    ")\n",
    "\n",
    "print(\"\u2713 Training complete!\")\n",
    "print(f\"Learning Statistics:\")\n",
    "print(f\"  States visited: {stats['states_visited']}\")\n",
    "print(f\"  Final average return: {np.mean(stats['episode_returns'][-100:]):.2f}\")\n",
    "print(f\"  Final average episode length: {np.mean(stats['episode_lengths'][-100:]):.1f}\")\n",
    "\n",
    "# Show learned policy for some key states\n",
    "print(\"Learned Policy (greedy actions):\")\n",
    "print(\"-\" * 40)\n",
    "key_states = [(0,0), (0,1), (1,0), (2,0), (3,3), (4,3)]\n",
    "for state in key_states:\n",
    "    if state in Q:\n",
    "        action = policy.get_greedy_action(state)\n",
    "        action_name = env.actions[action]\n",
    "        q_values = Q[state]\n",
    "        print(f\"  State {state}: {action_name} (Q-values: {q_values})\")\n",
    "\n",
    "print(\"\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Policy Improvement Over Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Episode returns over time\n",
    "ax = axes[0, 0]\n",
    "window = 100\n",
    "smoothed_returns = np.convolve(stats['episode_returns'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(smoothed_returns, linewidth=2, color='blue', alpha=0.8)\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Average Return', fontsize=11)\n",
    "ax.set_title('Learning Progress: Episode Returns', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=np.mean(smoothed_returns[-100:]), color='red', linestyle='--', \n",
    "           alpha=0.5, label=f'Final: {np.mean(smoothed_returns[-100:]):.1f}')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Episode lengths over time\n",
    "ax = axes[0, 1]\n",
    "smoothed_lengths = np.convolve(stats['episode_lengths'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(smoothed_lengths, linewidth=2, color='green', alpha=0.8)\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Episode Length (steps)', fontsize=11)\n",
    "ax.set_title('Learning Progress: Episode Lengths', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=np.mean(smoothed_lengths[-100:]), color='red', linestyle='--', \n",
    "           alpha=0.5, label=f'Final: {np.mean(smoothed_lengths[-100:]):.1f}')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Learned Q-values heatmap (max Q for each state)\n",
    "ax = axes[1, 0]\n",
    "q_grid = np.zeros((5, 5))\n",
    "for (row, col), q_values in Q.items():\n",
    "    q_grid[row, col] = np.max(q_values)\n",
    "\n",
    "im = ax.imshow(q_grid, cmap='RdYlGn', aspect='auto')\n",
    "ax.set_title('Learned Q-Values (max over actions)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Column', fontsize=11)\n",
    "ax.set_ylabel('Row', fontsize=11)\n",
    "\n",
    "# Add value labels\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        text = ax.text(j, i, f'{q_grid[i, j]:.1f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 4: Learned policy visualization\n",
    "ax = axes[1, 1]\n",
    "policy_grid = np.full((5, 5), -1)\n",
    "for (row, col), q_values in Q.items():\n",
    "    policy_grid[row, col] = np.argmax(q_values)\n",
    "\n",
    "# Create custom colormap for actions\n",
    "from matplotlib.colors import ListedColormap\n",
    "colors = ['white', 'lightblue', 'lightgreen', 'lightyellow', 'lightcoral']\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "im = ax.imshow(policy_grid, cmap=cmap, aspect='auto', vmin=-1, vmax=3)\n",
    "ax.set_title('Learned Policy (Greedy Actions)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Column', fontsize=11)\n",
    "ax.set_ylabel('Row', fontsize=11)\n",
    "\n",
    "# Add action arrows\n",
    "arrow_map = {0: '\u2191', 1: '\u2192', 2: '\u2193', 3: '\u2190', -1: ''}\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        action = int(policy_grid[i, j])\n",
    "        arrow = arrow_map.get(action, '')\n",
    "        ax.text(j, i, arrow, ha=\"center\", va=\"center\", \n",
    "               fontsize=20, fontweight='bold', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Visualization Insights:\")\n",
    "print(\"1. Top Left: Returns increase as policy improves\")\n",
    "print(\"   - Agent learns to reach goal more efficiently\")\n",
    "print(\"   - Converges to near-optimal performance\")\n",
    "\n",
    "print(\"2. Top Right: Episode lengths decrease\")\n",
    "print(\"   - Shorter episodes = more efficient paths to goal\")\n",
    "print(\"   - Agent learns to avoid obstacles and reach goal quickly\")\n",
    "\n",
    "print(\"3. Bottom Left: Q-values show state quality\")\n",
    "print(\"   - Higher values near goal (green)\")\n",
    "print(\"   - Lower values far from goal (red)\")\n",
    "\n",
    "print(\"4. Bottom Right: Learned policy\")\n",
    "print(\"   - Arrows show best action in each state\")\n",
    "print(\"   - Policy guides agent toward goal\")\n",
    "print(\"   - Successfully learned from experience!\")\n",
    "\n",
    "print(\"\u2705 On-Policy MC Control Success:\")\n",
    "print(\"   The agent learned an effective policy without any model!\")\n",
    "print(\"   Just from trial and error with epsilon-greedy exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Off-Policy Learning with Importance Sampling\n",
    "\n",
    "**Learning About One Policy While Following Another**\n",
    "\n",
    "So far, we've used **on-policy** learning where we learn about the same policy we're following. But what if we want to:\n",
    "- Learn an optimal (deterministic) policy while exploring (stochastic behavior)?\n",
    "- Learn from data generated by a different policy (e.g., human demonstrations)?\n",
    "- Reuse old experience even after the policy has changed?\n",
    "\n",
    "This is where **off-policy** learning comes in!\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "**Target Policy ($\\pi$):**\n",
    "- The policy we want to learn about\n",
    "- Often deterministic and greedy\n",
    "- Example: Always take the best action\n",
    "\n",
    "**Behavior Policy ($b$):**\n",
    "- The policy we actually follow to generate experience\n",
    "- Must be exploratory (try all actions)\n",
    "- Example: Epsilon-greedy or random policy\n",
    "\n",
    "**The Challenge:**\n",
    "\n",
    "Episodes are generated by $b$, but we want to estimate values for $\\pi$. The returns we observe are from $b$'s distribution, not $\\pi$'s!\n",
    "\n",
    "**The Solution: Importance Sampling**\n",
    "\n",
    "Importance sampling is a technique from statistics that allows us to estimate expectations under one distribution using samples from another.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "We want to estimate $\\mathbb{E}_\\pi[G_t]$ (expected return under $\\pi$), but we only have samples from $b$.\n",
    "\n",
    "**Importance Sampling Ratio:**\n",
    "\n",
    "For a trajectory $\\tau = (S_t, A_t, S_{t+1}, A_{t+1}, ..., S_T)$, the importance sampling ratio is:\n",
    "\n",
    "$\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}$\n",
    "\n",
    "This ratio weights the return to correct for the difference between policies.\n",
    "\n",
    "**Intuition:**\n",
    "- If $\\pi$ is more likely to take the actions than $b$: ratio > 1 (weight up)\n",
    "- If $\\pi$ is less likely to take the actions than $b$: ratio < 1 (weight down)\n",
    "- If $\\pi$ would never take an action that $b$ took: ratio = 0 (ignore)\n",
    "\n",
    "**Off-Policy MC Prediction with Importance Sampling:**\n",
    "\n",
    "$V(s) = \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1} G_t}{|\\mathcal{T}(s)|}$\n",
    "\n",
    "where $\\mathcal{T}(s)$ is the set of all time steps where state $s$ was visited.\n",
    "\n",
    "**Coverage Assumption:**\n",
    "\n",
    "For off-policy learning to work, we need:\n",
    "$\\pi(a|s) > 0 \\implies b(a|s) > 0$\n",
    "\n",
    "In words: The behavior policy must try all actions that the target policy might take.\n",
    "\n",
    "**Advantages of Off-Policy Learning:**\n",
    "1. Can learn optimal policy while exploring\n",
    "2. Can learn from observational data\n",
    "3. Can reuse experience from old policies\n",
    "4. More flexible than on-policy methods\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Higher variance (importance sampling ratios can be large)\n",
    "2. Slower convergence\n",
    "3. Requires more data\n",
    "4. Can be unstable if policies are very different\n",
    "\n",
    "Let's implement off-policy MC with importance sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_importance_sampling_ratio(episode, target_policy, behavior_policy):\n",
    "    \"\"\"Compute importance sampling ratio for an episode.\n",
    "    \n",
    "    Args:\n",
    "        episode: List of (state, action, reward) tuples\n",
    "        target_policy: Function that returns probability of action given state\n",
    "        behavior_policy: Function that returns probability of action given state\n",
    "        \n",
    "    Returns:\n",
    "        ratios: List of cumulative importance sampling ratios for each step\n",
    "    \"\"\"\n",
    "    ratios = []\n",
    "    cumulative_ratio = 1.0\n",
    "    \n",
    "    for state, action, reward in episode:\n",
    "        # Get probabilities under both policies\n",
    "        pi_prob = target_policy(action, state)\n",
    "        b_prob = behavior_policy(action, state)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if b_prob == 0:\n",
    "            cumulative_ratio = 0\n",
    "            break\n",
    "        \n",
    "        # Update cumulative ratio\n",
    "        cumulative_ratio *= (pi_prob / b_prob)\n",
    "        ratios.append(cumulative_ratio)\n",
    "    \n",
    "    return ratios\n",
    "\n",
    "\n",
    "def mc_prediction_off_policy(env, target_policy_func, behavior_policy_func,\n",
    "                             target_policy_probs, behavior_policy_probs,\n",
    "                             num_episodes=5000, gamma=0.9):\n",
    "    \"\"\"Off-policy Monte Carlo prediction with ordinary importance sampling.\n",
    "    \n",
    "    Learns value function for target policy using episodes from behavior policy.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        target_policy_func: Function that selects actions for target policy\n",
    "        behavior_policy_func: Function that selects actions for behavior policy\n",
    "        target_policy_probs: Function that returns P(a|s) for target policy\n",
    "        behavior_policy_probs: Function that returns P(a|s) for behavior policy\n",
    "        num_episodes: Number of episodes to generate\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        V: Estimated value function for target policy\n",
    "        stats: Learning statistics\n",
    "    \"\"\"\n",
    "    V = defaultdict(float)\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    \n",
    "    episode_returns = []\n",
    "    importance_ratios = []\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode using behavior policy\n",
    "        episode = generate_episode(env, behavior_policy_func, max_steps=100)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Calculate importance sampling ratios\n",
    "        ratios = compute_importance_sampling_ratio(\n",
    "            episode, target_policy_probs, behavior_policy_probs\n",
    "        )\n",
    "        \n",
    "        # Track statistics\n",
    "        if returns:\n",
    "            episode_returns.append(returns[0])\n",
    "        if ratios:\n",
    "            importance_ratios.append(ratios[-1])  # Final ratio\n",
    "        \n",
    "        # Update value estimates (first-visit)\n",
    "        visited_states = set()\n",
    "        \n",
    "        for t, (state, action, reward) in enumerate(episode):\n",
    "            if state not in visited_states and t < len(ratios):\n",
    "                visited_states.add(state)\n",
    "                \n",
    "                # Weight return by importance sampling ratio\n",
    "                weighted_return = ratios[t] * returns[t]\n",
    "                \n",
    "                returns_sum[state] += weighted_return\n",
    "                returns_count[state] += 1\n",
    "                \n",
    "                # Update value estimate\n",
    "                V[state] = returns_sum[state] / returns_count[state]\n",
    "    \n",
    "    stats = {\n",
    "        'episode_returns': episode_returns,\n",
    "        'importance_ratios': importance_ratios,\n",
    "        'states_visited': len(V)\n",
    "    }\n",
    "    \n",
    "    return dict(V), stats\n",
    "\n",
    "\n",
    "print(\"Off-Policy Monte Carlo with Importance Sampling\")\n",
    "print(\"=\"*60)\n",
    "print(\"Implemented:\")\n",
    "print(\"  \u2713 Importance sampling ratio computation\")\n",
    "print(\"  \u2713 Off-policy MC prediction\")\n",
    "print(\"  \u2713 Target vs behavior policy separation\")\n",
    "print(\"Ready to learn from off-policy data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrating Off-Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and behavior policies\n",
    "print(\"Off-Policy Learning Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnvironment(grid_size=5)\n",
    "\n",
    "# Target policy: Greedy (deterministic)\n",
    "def target_policy_action(state):\n",
    "    \"\"\"Greedy policy - always move toward goal.\"\"\"\n",
    "    row, col = state\n",
    "    goal_row, goal_col = 4, 4\n",
    "    \n",
    "    if col < goal_col:\n",
    "        return 1  # RIGHT\n",
    "    elif row < goal_row:\n",
    "        return 2  # DOWN\n",
    "    else:\n",
    "        return 1  # Default\n",
    "\n",
    "def target_policy_prob(action, state):\n",
    "    \"\"\"Probability of action under target policy (deterministic).\"\"\"\n",
    "    return 1.0 if action == target_policy_action(state) else 0.0\n",
    "\n",
    "# Behavior policy: Epsilon-greedy (exploratory)\n",
    "epsilon_behavior = 0.3\n",
    "\n",
    "def behavior_policy_action(state):\n",
    "    \"\"\"Epsilon-greedy behavior policy.\"\"\"\n",
    "    if np.random.random() < epsilon_behavior:\n",
    "        return np.random.randint(0, 4)  # Random\n",
    "    else:\n",
    "        return target_policy_action(state)  # Greedy\n",
    "\n",
    "def behavior_policy_prob(action, state):\n",
    "    \"\"\"Probability of action under behavior policy (epsilon-greedy).\"\"\"\n",
    "    greedy_action = target_policy_action(state)\n",
    "    \n",
    "    if action == greedy_action:\n",
    "        return 1.0 - epsilon_behavior + epsilon_behavior / 4.0\n",
    "    else:\n",
    "        return epsilon_behavior / 4.0\n",
    "\n",
    "print(\"Policies:\")\n",
    "print(\"  Target Policy: Greedy (deterministic, optimal)\")\n",
    "print(\"  Behavior Policy: \u03b5-greedy with \u03b5=0.3 (exploratory)\")\n",
    "\n",
    "print(\"Learning value function for target policy...\")\n",
    "print(\"  (using episodes generated by behavior policy)\")\n",
    "\n",
    "# Learn off-policy\n",
    "V_off_policy, stats_off = mc_prediction_off_policy(\n",
    "    env,\n",
    "    target_policy_action,\n",
    "    behavior_policy_action,\n",
    "    target_policy_prob,\n",
    "    behavior_policy_prob,\n",
    "    num_episodes=5000,\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "# For comparison, learn on-policy with target policy\n",
    "print(\"For comparison, learning with on-policy (target policy)...\")\n",
    "V_on_policy, returns_on = mc_prediction_first_visit(\n",
    "    env, target_policy_action, num_episodes=5000, gamma=0.9\n",
    ")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"Results:\")\n",
    "print(f\"Off-Policy Learning:\")\n",
    "print(f\"  States visited: {stats_off['states_visited']}\")\n",
    "print(f\"  Start state value: {V_off_policy.get((0,0), 0):.2f}\")\n",
    "print(f\"  Average importance ratio: {np.mean(stats_off['importance_ratios']):.3f}\")\n",
    "print(f\"  Max importance ratio: {np.max(stats_off['importance_ratios']):.3f}\")\n",
    "\n",
    "print(f\"On-Policy Learning (for comparison):\")\n",
    "print(f\"  States visited: {len(V_on_policy)}\")\n",
    "print(f\"  Start state value: {V_on_policy.get((0,0), 0):.2f}\")\n",
    "\n",
    "print(\"\ud83d\udca1 Key Observations:\")\n",
    "print(\"   - Off-policy successfully learns target policy values\")\n",
    "print(\"   - Uses exploratory behavior policy for data collection\")\n",
    "print(\"   - Importance ratios correct for policy difference\")\n",
    "print(\"   - Values should be similar to on-policy estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Importance Sampling: Reducing Variance\n",
    "\n",
    "**The Variance Problem with Ordinary Importance Sampling**\n",
    "\n",
    "Ordinary importance sampling (what we just implemented) has a significant problem: **high variance**.\n",
    "\n",
    "**Why High Variance?**\n",
    "\n",
    "The importance sampling ratio $\\rho = \\prod \\frac{\\pi(a|s)}{b(a|s)}$ can become very large:\n",
    "- If the trajectory is long, many ratios multiply together\n",
    "- If policies differ significantly, individual ratios can be large\n",
    "- A single large ratio can dominate the average\n",
    "\n",
    "**Example:**\n",
    "- Suppose we have 100 episodes with ratio \u2248 1.0\n",
    "- And 1 episode with ratio = 100\n",
    "- The single outlier heavily skews the estimate!\n",
    "\n",
    "**The Solution: Weighted Importance Sampling**\n",
    "\n",
    "Instead of a simple average, use a **weighted average** where the weights are the importance sampling ratios themselves.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "**Ordinary Importance Sampling:**\n",
    "$V(s) = \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1} G_t}{|\\mathcal{T}(s)|}$\n",
    "\n",
    "**Weighted Importance Sampling:**\n",
    "$V(s) = \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1} G_t}{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1}}$\n",
    "\n",
    "**Key Difference:**\n",
    "- Ordinary: Divide by number of visits\n",
    "- Weighted: Divide by sum of importance ratios\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "Weighted IS gives more weight to returns with larger importance ratios, but normalizes by the total weight. This:\n",
    "- Reduces the impact of extreme ratios\n",
    "- Provides more stable estimates\n",
    "- Converges faster in practice\n",
    "\n",
    "**Bias-Variance Trade-off:**\n",
    "\n",
    "| Method | Bias | Variance | Convergence |\n",
    "|--------|------|----------|-------------|\n",
    "| Ordinary IS | Unbiased | High | Slower |\n",
    "| Weighted IS | Biased (initially) | Low | Faster |\n",
    "\n",
    "**Important Note:**\n",
    "- Weighted IS is biased for finite samples\n",
    "- But the bias goes to zero as samples increase\n",
    "- In practice, lower variance usually wins!\n",
    "\n",
    "**When to Use Each:**\n",
    "- **Ordinary IS**: When you need unbiased estimates, have lots of data\n",
    "- **Weighted IS**: When variance is a problem, limited data (most practical cases)\n",
    "\n",
    "Let's implement weighted importance sampling and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_off_policy_weighted(env, target_policy_func, behavior_policy_func,\n",
    "                                      target_policy_probs, behavior_policy_probs,\n",
    "                                      num_episodes=5000, gamma=0.9):\n",
    "    \"\"\"Off-policy Monte Carlo prediction with weighted importance sampling.\n",
    "    \n",
    "    Uses weighted average to reduce variance compared to ordinary IS.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        target_policy_func: Function that selects actions for target policy\n",
    "        behavior_policy_func: Function that selects actions for behavior policy\n",
    "        target_policy_probs: Function that returns P(a|s) for target policy\n",
    "        behavior_policy_probs: Function that returns P(a|s) for behavior policy\n",
    "        num_episodes: Number of episodes to generate\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        V: Estimated value function for target policy\n",
    "        stats: Learning statistics\n",
    "    \"\"\"\n",
    "    V = defaultdict(float)\n",
    "    # For weighted IS, we need numerator and denominator separately\n",
    "    weighted_returns_sum = defaultdict(float)  # Sum of (ratio * return)\n",
    "    weights_sum = defaultdict(float)  # Sum of ratios\n",
    "    \n",
    "    episode_returns = []\n",
    "    importance_ratios = []\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode using behavior policy\n",
    "        episode = generate_episode(env, behavior_policy_func, max_steps=100)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Calculate importance sampling ratios\n",
    "        ratios = compute_importance_sampling_ratio(\n",
    "            episode, target_policy_probs, behavior_policy_probs\n",
    "        )\n",
    "        \n",
    "        # Track statistics\n",
    "        if returns:\n",
    "            episode_returns.append(returns[0])\n",
    "        if ratios:\n",
    "            importance_ratios.append(ratios[-1])\n",
    "        \n",
    "        # Update value estimates (first-visit)\n",
    "        visited_states = set()\n",
    "        \n",
    "        for t, (state, action, reward) in enumerate(episode):\n",
    "            if state not in visited_states and t < len(ratios):\n",
    "                visited_states.add(state)\n",
    "                \n",
    "                # Weighted importance sampling\n",
    "                ratio = ratios[t]\n",
    "                weighted_return = ratio * returns[t]\n",
    "                \n",
    "                # Update numerator and denominator\n",
    "                weighted_returns_sum[state] += weighted_return\n",
    "                weights_sum[state] += ratio\n",
    "                \n",
    "                # Update value estimate (weighted average)\n",
    "                if weights_sum[state] > 0:\n",
    "                    V[state] = weighted_returns_sum[state] / weights_sum[state]\n",
    "    \n",
    "    stats = {\n",
    "        'episode_returns': episode_returns,\n",
    "        'importance_ratios': importance_ratios,\n",
    "        'states_visited': len(V)\n",
    "    }\n",
    "    \n",
    "    return dict(V), stats\n",
    "\n",
    "\n",
    "print(\"Weighted Importance Sampling Implementation\")\n",
    "print(\"=\"*60)\n",
    "print(\"Implemented:\")\n",
    "print(\"  \u2713 Weighted importance sampling\")\n",
    "print(\"  \u2713 Variance reduction through weighted averaging\")\n",
    "print(\"  \u2713 Separate tracking of numerator and denominator\")\n",
    "print(\"Ready to compare with ordinary importance sampling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Ordinary vs Weighted Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ordinary and weighted importance sampling\n",
    "print(\"Comparing Ordinary vs Weighted Importance Sampling\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run multiple trials to measure variance\n",
    "num_trials = 20\n",
    "num_episodes_per_trial = 2000\n",
    "\n",
    "ordinary_estimates = []\n",
    "weighted_estimates = []\n",
    "\n",
    "print(f\"Running {num_trials} trials with {num_episodes_per_trial} episodes each...\")\n",
    "print(\"(This may take a moment)\")\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    # Ordinary importance sampling\n",
    "    V_ordinary, _ = mc_prediction_off_policy(\n",
    "        env, target_policy_action, behavior_policy_action,\n",
    "        target_policy_prob, behavior_policy_prob,\n",
    "        num_episodes=num_episodes_per_trial, gamma=0.9\n",
    "    )\n",
    "    ordinary_estimates.append(V_ordinary.get((0,0), 0))\n",
    "    \n",
    "    # Weighted importance sampling\n",
    "    V_weighted, _ = mc_prediction_off_policy_weighted(\n",
    "        env, target_policy_action, behavior_policy_action,\n",
    "        target_policy_prob, behavior_policy_prob,\n",
    "        num_episodes=num_episodes_per_trial, gamma=0.9\n",
    "    )\n",
    "    weighted_estimates.append(V_weighted.get((0,0), 0))\n",
    "    \n",
    "    if (trial + 1) % 5 == 0:\n",
    "        print(f\"  Completed {trial + 1}/{num_trials} trials\")\n",
    "\n",
    "# Calculate statistics\n",
    "ordinary_mean = np.mean(ordinary_estimates)\n",
    "ordinary_std = np.std(ordinary_estimates)\n",
    "weighted_mean = np.mean(weighted_estimates)\n",
    "weighted_std = np.std(weighted_estimates)\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"Results for Start State (0,0):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Ordinary Importance Sampling:\")\n",
    "print(f\"  Mean estimate: {ordinary_mean:.3f}\")\n",
    "print(f\"  Std deviation: {ordinary_std:.3f}\")\n",
    "print(f\"  Min estimate: {np.min(ordinary_estimates):.3f}\")\n",
    "print(f\"  Max estimate: {np.max(ordinary_estimates):.3f}\")\n",
    "\n",
    "print(f\"Weighted Importance Sampling:\")\n",
    "print(f\"  Mean estimate: {weighted_mean:.3f}\")\n",
    "print(f\"  Std deviation: {weighted_std:.3f}\")\n",
    "print(f\"  Min estimate: {np.min(weighted_estimates):.3f}\")\n",
    "print(f\"  Max estimate: {np.max(weighted_estimates):.3f}\")\n",
    "\n",
    "variance_reduction = ((ordinary_std - weighted_std) / ordinary_std) * 100\n",
    "print(f\"\ud83d\udcca Variance Reduction: {variance_reduction:.1f}%\")\n",
    "\n",
    "print(\"\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Variance Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Distribution of estimates\n",
    "ax = axes[0]\n",
    "ax.hist(ordinary_estimates, bins=15, alpha=0.6, label='Ordinary IS', color='red', edgecolor='black')\n",
    "ax.hist(weighted_estimates, bins=15, alpha=0.6, label='Weighted IS', color='green', edgecolor='black')\n",
    "ax.axvline(ordinary_mean, color='red', linestyle='--', linewidth=2, label=f'Ordinary Mean: {ordinary_mean:.2f}')\n",
    "ax.axvline(weighted_mean, color='green', linestyle='--', linewidth=2, label=f'Weighted Mean: {weighted_mean:.2f}')\n",
    "ax.set_xlabel('Value Estimate for State (0,0)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Distribution of Value Estimates', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Box plot comparison\n",
    "ax = axes[1]\n",
    "data_to_plot = [ordinary_estimates, weighted_estimates]\n",
    "bp = ax.boxplot(data_to_plot, labels=['Ordinary IS', 'Weighted IS'],\n",
    "                patch_artist=True, widths=0.6)\n",
    "\n",
    "# Color the boxes\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('Value Estimate for State (0,0)', fontsize=12)\n",
    "ax.set_title('Variance Comparison', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add variance reduction annotation\n",
    "ax.text(1.5, ax.get_ylim()[1] * 0.95, \n",
    "        f'Variance Reduction:{variance_reduction:.1f}%',\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "        fontsize=11, fontweight='bold', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Visualization Insights:\")\n",
    "print(\"1. Left Plot: Distribution of estimates across trials\")\n",
    "print(\"   - Ordinary IS: Wider spread (higher variance)\")\n",
    "print(\"   - Weighted IS: Tighter distribution (lower variance)\")\n",
    "print(\"   - Both centered around similar mean values\")\n",
    "\n",
    "print(\"2. Right Plot: Box plot shows variance clearly\")\n",
    "print(\"   - Ordinary IS: Larger box and whiskers\")\n",
    "print(\"   - Weighted IS: Smaller box (more consistent estimates)\")\n",
    "print(\"   - Outliers are less extreme with weighted IS\")\n",
    "\n",
    "print(\"\u2705 Key Takeaways:\")\n",
    "print(\"   1. Weighted IS significantly reduces variance\")\n",
    "print(\"   2. More stable and reliable estimates\")\n",
    "print(\"   3. Faster convergence in practice\")\n",
    "print(\"   4. Preferred method for most off-policy learning\")\n",
    "print(\"   Weighted importance sampling is the practical choice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Methods: Limitations and Challenges\n",
    "\n",
    "**Understanding When MC Methods Fall Short**\n",
    "\n",
    "While Monte Carlo methods are powerful and model-free, they have significant limitations that restrict their applicability. Understanding these limitations motivates the development of more advanced methods like Temporal Difference learning.\n",
    "\n",
    "**1. The \"Wait Until the End\" Problem**\n",
    "\n",
    "**The Issue:**\n",
    "- MC methods must wait until an episode completes before updating values\n",
    "- No learning happens during the episode\n",
    "- All updates occur at the end\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Long Episodes**: If episodes take 1000 steps, you wait 1000 steps to learn anything\n",
    "- **Continuing Tasks**: Some tasks never end (e.g., process control, robot operation)\n",
    "- **Slow Feedback**: Can't adjust behavior mid-episode based on what's working\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Episode: S\u2080 \u2192 S\u2081 \u2192 S\u2082 \u2192 ... \u2192 S\u2089\u2089\u2089 \u2192 S\u2081\u2080\u2080\u2080 (terminal)\n",
    "         \u2191                                    \u2191\n",
    "    No learning                         All learning happens here!\n",
    "```\n",
    "\n",
    "**Impact:**\n",
    "- Inefficient use of experience\n",
    "- Slow learning, especially with long episodes\n",
    "- Cannot handle continuing (non-episodic) tasks\n",
    "\n",
    "**2. High Variance in Return Estimates**\n",
    "\n",
    "**The Issue:**\n",
    "- Returns depend on entire trajectory of rewards\n",
    "- Many random events accumulate\n",
    "- Different episodes from same state can have very different returns\n",
    "\n",
    "**Mathematical Perspective:**\n",
    "\n",
    "Return: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$\n",
    "\n",
    "Each $R_i$ is random, and we're summing many random variables:\n",
    "- More terms \u2192 more variance\n",
    "- Longer episodes \u2192 higher variance\n",
    "- Stochastic environments \u2192 even more variance\n",
    "\n",
    "**Consequences:**\n",
    "- Need many episodes to get accurate estimates\n",
    "- Slow convergence\n",
    "- Unstable learning, especially early on\n",
    "- Off-policy methods (importance sampling) make this worse\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "From state S, three episodes:\n",
    "  Episode 1: G = 10  (got lucky)\n",
    "  Episode 2: G = -5  (got unlucky)\n",
    "  Episode 3: G = 3   (typical)\n",
    "  \n",
    "Average: 2.67 (but high variance!)\n",
    "Need many more episodes for stable estimate\n",
    "```\n",
    "\n",
    "**3. Inefficient Learning from Experience**\n",
    "\n",
    "**The Issue:**\n",
    "- Each episode provides one data point per state visited\n",
    "- Can't learn from partial episodes\n",
    "- Doesn't leverage structure of the problem\n",
    "\n",
    "**Comparison:**\n",
    "- **MC**: Uses complete return from state to end\n",
    "- **Better approach**: Could learn from each step along the way\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Episode: S\u2080 \u2192 S\u2081 \u2192 S\u2082 \u2192 S\u2083 \u2192 S\u2084 (terminal, R=10)\n",
    "\n",
    "MC Learning:\n",
    "  - Updates V(S\u2080), V(S\u2081), V(S\u2082), V(S\u2083) once at end\n",
    "  - Uses full return for each\n",
    "  \n",
    "Potential Improvement:\n",
    "  - Could update after each step\n",
    "  - Could learn from partial information\n",
    "  - 5 learning opportunities instead of 1!\n",
    "```\n",
    "\n",
    "**4. Requires Episodic Tasks**\n",
    "\n",
    "**The Issue:**\n",
    "- MC methods fundamentally require episodes to terminate\n",
    "- Many real-world problems are continuing (no natural end)\n",
    "\n",
    "**Examples of Continuing Tasks:**\n",
    "- Process control (factory, power plant)\n",
    "- Robot operation (runs indefinitely)\n",
    "- Stock trading (market never closes permanently)\n",
    "- Recommendation systems (always serving users)\n",
    "\n",
    "**Workarounds (not ideal):**\n",
    "- Artificially terminate episodes\n",
    "- Use very long episodes (but then variance increases)\n",
    "- Neither solution is satisfactory\n",
    "\n",
    "**5. Slow Convergence**\n",
    "\n",
    "**The Issue:**\n",
    "- Due to high variance, need many episodes\n",
    "- Each episode only updates visited states\n",
    "- Learning is sample-inefficient\n",
    "\n",
    "**Factors Affecting Convergence:**\n",
    "- Episode length (longer \u2192 slower)\n",
    "- Environment stochasticity (more random \u2192 slower)\n",
    "- State space size (larger \u2192 slower)\n",
    "- Exploration strategy (poor exploration \u2192 slower)\n",
    "\n",
    "**Practical Impact:**\n",
    "- May need millions of episodes for complex problems\n",
    "- Expensive in terms of computation and time\n",
    "- Not practical for real-world systems with costly interactions\n",
    "\n",
    "**Summary of Limitations:**\n",
    "\n",
    "| Limitation | Impact | Severity |\n",
    "|------------|--------|----------|\n",
    "| Wait until end | Slow learning | High |\n",
    "| High variance | Need many samples | High |\n",
    "| Episodic only | Can't handle continuing tasks | Critical |\n",
    "| Sample inefficiency | Expensive learning | Medium |\n",
    "| Slow convergence | Long training times | Medium |\n",
    "\n",
    "**The Path Forward: Temporal Difference Learning**\n",
    "\n",
    "These limitations motivate **Temporal Difference (TD) learning**, which we'll explore next. TD methods:\n",
    "\n",
    "\u2713 Learn from every step (not just at episode end)\n",
    "\u2713 Work with continuing tasks\n",
    "\u2713 Lower variance (bootstrap from estimates)\n",
    "\u2713 More sample-efficient\n",
    "\u2713 Faster convergence\n",
    "\n",
    "**When to Use Monte Carlo Despite Limitations:**\n",
    "\n",
    "MC methods are still valuable when:\n",
    "- Episodes are short\n",
    "- You need unbiased estimates\n",
    "- Environment is deterministic or low-noise\n",
    "- You have access to a simulator (cheap episodes)\n",
    "- You want simple, easy-to-understand algorithms\n",
    "\n",
    "**Key Insight:**\n",
    "\n",
    "Monte Carlo methods taught us that we can learn from experience without a model. But their limitations show us that we can do better by learning from partial episodes and bootstrapping from our own estimates. This insight leads directly to Temporal Difference learning, which combines the best of MC and Dynamic Programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing MC Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the high variance problem\n",
    "print(\"Demonstrating Monte Carlo Limitations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run MC prediction multiple times to show variance\n",
    "num_runs = 50\n",
    "episodes_per_run = 1000\n",
    "\n",
    "start_state_estimates = []\n",
    "\n",
    "print(f\"Running MC prediction {num_runs} times...\")\n",
    "print(f\"Each run uses {episodes_per_run} episodes\")\n",
    "\n",
    "for run in range(num_runs):\n",
    "    V, _ = mc_prediction_first_visit(\n",
    "        env, greedy_policy, num_episodes=episodes_per_run, gamma=0.9\n",
    "    )\n",
    "    start_state_estimates.append(V.get((0,0), 0))\n",
    "\n",
    "mean_estimate = np.mean(start_state_estimates)\n",
    "std_estimate = np.std(start_state_estimates)\n",
    "\n",
    "print(f\"Results for start state (0,0):\")\n",
    "print(f\"  Mean estimate: {mean_estimate:.3f}\")\n",
    "print(f\"  Std deviation: {std_estimate:.3f}\")\n",
    "print(f\"  Min: {np.min(start_state_estimates):.3f}\")\n",
    "print(f\"  Max: {np.max(start_state_estimates):.3f}\")\n",
    "print(f\"  Range: {np.max(start_state_estimates) - np.min(start_state_estimates):.3f}\")\n",
    "\n",
    "# Visualize variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Distribution of estimates\n",
    "ax = axes[0]\n",
    "ax.hist(start_state_estimates, bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax.axvline(mean_estimate, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {mean_estimate:.2f}')\n",
    "ax.axvline(mean_estimate - std_estimate, color='orange', linestyle=':', linewidth=2,\n",
    "           label=f'\u00b11 Std: {std_estimate:.2f}')\n",
    "ax.axvline(mean_estimate + std_estimate, color='orange', linestyle=':', linewidth=2)\n",
    "ax.set_xlabel('Value Estimate', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('High Variance in MC Estimates', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Estimates over runs\n",
    "ax = axes[1]\n",
    "ax.plot(start_state_estimates, marker='o', linestyle='-', alpha=0.6, color='steelblue')\n",
    "ax.axhline(mean_estimate, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "ax.fill_between(range(num_runs), \n",
    "                mean_estimate - std_estimate, \n",
    "                mean_estimate + std_estimate,\n",
    "                alpha=0.2, color='orange', label='\u00b11 Std')\n",
    "ax.set_xlabel('Run Number', fontsize=12)\n",
    "ax.set_ylabel('Value Estimate', fontsize=12)\n",
    "ax.set_title('Variability Across Runs', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\u26a0\ufe0f  Key Observations:\")\n",
    "print(\"1. High Variance:\")\n",
    "print(f\"   - Estimates vary significantly across runs\")\n",
    "print(f\"   - Standard deviation is {(std_estimate/mean_estimate)*100:.1f}% of mean\")\n",
    "print(f\"   - Need many episodes for stable estimates\")\n",
    "\n",
    "print(\"2. Sample Inefficiency:\")\n",
    "print(f\"   - Used {num_runs * episodes_per_run:,} total episodes\")\n",
    "print(f\"   - Still seeing significant variance\")\n",
    "print(f\"   - Each episode only updates visited states once\")\n",
    "\n",
    "print(\"3. Episodic Requirement:\")\n",
    "print(f\"   - Must wait for episode to complete\")\n",
    "print(f\"   - No learning during episode\")\n",
    "print(f\"   - Cannot handle continuing tasks\")\n",
    "\n",
    "print(\"\ud83c\udfaf Motivation for Temporal Difference Learning:\")\n",
    "print(\"   These limitations show we need methods that:\")\n",
    "print(\"   \u2022 Learn from every step, not just episode ends\")\n",
    "print(\"   \u2022 Have lower variance through bootstrapping\")\n",
    "print(\"   \u2022 Work with continuing tasks\")\n",
    "print(\"   \u2022 Are more sample-efficient\")\n",
    "print(\"   \u2192 This leads us to TD learning in the next section!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='td-learning'></a>\n",
    "### Temporal Difference Learning\n",
    "\n",
    "**Learning from Every Step**\n",
    "\n",
    "Temporal Difference (TD) learning represents a fundamental breakthrough in reinforcement learning. Unlike Monte Carlo methods that must wait until the end of an episode to update value estimates, TD methods learn from **every single step** of experience.\n",
    "\n",
    "**The Key Insight:**\n",
    "\n",
    "TD learning combines the best aspects of two approaches:\n",
    "\n",
    "1. **From Monte Carlo**: Learn directly from experience without a model\n",
    "2. **From Dynamic Programming**: Update estimates based on other estimates (bootstrapping)\n",
    "\n",
    "**Why \"Temporal Difference\"?**\n",
    "\n",
    "The name comes from the fact that TD methods learn from the **difference** between estimates at successive **time** steps. Instead of waiting for the actual return, TD methods use the difference between the current estimate and a better estimate based on the next state.\n",
    "\n",
    "**Advantages of TD Learning:**\n",
    "\n",
    "1. **Online Learning**: Update after every step, not just at episode end\n",
    "2. **Lower Variance**: Bootstrap from estimates rather than full returns\n",
    "3. **Works with Continuing Tasks**: No need for episodes to terminate\n",
    "4. **More Sample Efficient**: Learn more from each experience\n",
    "5. **Faster Convergence**: Updates propagate information more quickly\n",
    "\n",
    "**The Trade-off:**\n",
    "\n",
    "- **MC**: Unbiased but high variance (uses actual returns)\n",
    "- **TD**: Biased but lower variance (uses estimated returns)\n",
    "- In practice, TD's lower variance usually wins!\n",
    "\n",
    "Let's explore the simplest TD method: TD(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD(0) Prediction Algorithm\n",
    "\n",
    "**The Simplest Temporal Difference Method**\n",
    "\n",
    "TD(0) (pronounced \"TD-zero\") is the most fundamental TD algorithm. It updates the value estimate for a state immediately after transitioning to the next state.\n",
    "\n",
    "**The TD(0) Update Rule:**\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]\n",
    "$$\n",
    "\n",
    "**Breaking Down the Formula:**\n",
    "\n",
    "- $V(S_t)$: Current value estimate for state $S_t$\n",
    "- $\\alpha$: Learning rate (step size), typically 0.01 to 0.5\n",
    "- $R_{t+1}$: Immediate reward received after taking action\n",
    "- $\\gamma$: Discount factor (0 to 1)\n",
    "- $V(S_{t+1})$: Value estimate for next state\n",
    "- $R_{t+1} + \\gamma V(S_{t+1})$: **TD target** (estimate of true value)\n",
    "- $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$: **TD error** (how wrong we were)\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "1. We're in state $S_t$ with value estimate $V(S_t)$\n",
    "2. We take an action and receive reward $R_{t+1}$, landing in state $S_{t+1}$\n",
    "3. We form a **better estimate** of $V(S_t)$: $R_{t+1} + \\gamma V(S_{t+1})$\n",
    "4. We move our estimate toward this better estimate\n",
    "\n",
    "**Comparison with Monte Carlo:**\n",
    "\n",
    "Monte Carlo update:\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right]\n",
    "$$\n",
    "where $G_t$ is the **actual return** (sum of all future rewards)\n",
    "\n",
    "TD(0) update:\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]\n",
    "$$\n",
    "where $R_{t+1} + \\gamma V(S_{t+1})$ is an **estimated return**\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Aspect | Monte Carlo | TD(0) |\n",
    "|--------|-------------|-------|\n",
    "| Target | $G_t$ (actual return) | $R_{t+1} + \\gamma V(S_{t+1})$ (estimated) |\n",
    "| When to update | End of episode | After each step |\n",
    "| Bias | Unbiased | Biased (uses estimate) |\n",
    "| Variance | High | Lower |\n",
    "| Continuing tasks | No | Yes |\n",
    "\n",
    "**The Bootstrapping Concept:**\n",
    "\n",
    "TD methods \"bootstrap\" - they update estimates based on other estimates. This is like pulling yourself up by your bootstraps! Initially, all estimates might be wrong, but they gradually improve and help each other converge to the true values.\n",
    "\n",
    "Let's implement TD(0) prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_prediction(env, policy, num_episodes=1000, alpha=0.1, gamma=0.9):\n",
    "    \"\"\"\n",
    "    TD(0) prediction: Estimate state-value function for a given policy.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment with reset() and step() methods\n",
    "        policy: Function that takes state and returns action\n",
    "        num_episodes: Number of episodes to run\n",
    "        alpha: Learning rate (step size)\n",
    "        gamma: Discount factor\n",
    "    \n",
    "    Returns:\n",
    "        V: Dictionary mapping states to value estimates\n",
    "        episode_lengths: List of episode lengths for tracking\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = defaultdict(float)\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_length = 0\n",
    "        \n",
    "        while True:\n",
    "            # Select action according to policy\n",
    "            action = policy(state)\n",
    "            \n",
    "            # Take action and observe next state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_length += 1\n",
    "            \n",
    "            # TD(0) update rule\n",
    "            # V(S) \u2190 V(S) + \u03b1[R + \u03b3V(S') - V(S)]\n",
    "            td_target = reward + gamma * V[next_state]\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] = V[state] + alpha * td_error\n",
    "            \n",
    "            if done:\n",
    "                episode_lengths.append(episode_length)\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "    \n",
    "    return V, episode_lengths\n",
    "\n",
    "\n",
    "print(\"TD(0) Prediction Algorithm Implemented!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key Features:\")\n",
    "print(\"  \u2022 Updates after every step (online learning)\")\n",
    "print(\"  \u2022 Uses bootstrapping (estimates from estimates)\")\n",
    "print(\"  \u2022 Lower variance than Monte Carlo\")\n",
    "print(\"  \u2022 Works with continuing tasks\")\n",
    "print(\"Update Rule: V(S) \u2190 V(S) + \u03b1[R + \u03b3V(S') - V(S)]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing TD(0) with Monte Carlo\n",
    "\n",
    "Now let's compare TD(0) prediction with Monte Carlo prediction on the same environment to see the differences in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same grid world environment from before\n",
    "print(\"Comparing TD(0) vs Monte Carlo Prediction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnvironment(grid_size=4, goal_pos=(3, 3), obstacles=[])\n",
    "\n",
    "# Use the same greedy policy toward goal\n",
    "def greedy_policy(state):\n",
    "    \"\"\"Simple policy: move toward goal (3,3).\"\"\"\n",
    "    row, col = state\n",
    "    goal_row, goal_col = 3, 3\n",
    "    \n",
    "    # Move toward goal\n",
    "    if row < goal_row:\n",
    "        return 2  # Down\n",
    "    elif row > goal_row:\n",
    "        return 0  # Up\n",
    "    elif col < goal_col:\n",
    "        return 1  # Right\n",
    "    else:\n",
    "        return 3  # Left\n",
    "\n",
    "# Run both algorithms with same parameters\n",
    "num_episodes = 500\n",
    "gamma = 0.9\n",
    "\n",
    "print(f\"Running both algorithms for {num_episodes} episodes...\")\n",
    "\n",
    "# TD(0) prediction\n",
    "print(\"Running TD(0) prediction...\")\n",
    "V_td, lengths_td = td_prediction(env, greedy_policy, \n",
    "                                  num_episodes=num_episodes, \n",
    "                                  alpha=0.1, gamma=gamma)\n",
    "\n",
    "# Monte Carlo prediction (first-visit)\n",
    "print(\"Running Monte Carlo prediction...\")\n",
    "V_mc, lengths_mc = mc_prediction_first_visit(env, greedy_policy, \n",
    "                                              num_episodes=num_episodes, \n",
    "                                              gamma=gamma)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "# Compare value estimates for key states\n",
    "print(\"Value Estimates Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'State':<12} {'TD(0)':<12} {'MC':<12} {'Difference':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Compare some key states\n",
    "key_states = [(0,0), (0,3), (1,1), (2,2), (3,0), (3,3)]\n",
    "for state in key_states:\n",
    "    v_td = V_td.get(state, 0)\n",
    "    v_mc = V_mc.get(state, 0)\n",
    "    diff = abs(v_td - v_mc)\n",
    "    print(f\"{str(state):<12} {v_td:<12.3f} {v_mc:<12.3f} {diff:<12.3f}\")\n",
    "\n",
    "# Calculate statistics\n",
    "all_states = set(list(V_td.keys()) + list(V_mc.keys()))\n",
    "differences = [abs(V_td.get(s, 0) - V_mc.get(s, 0)) for s in all_states]\n",
    "mean_diff = np.mean(differences)\n",
    "max_diff = np.max(differences)\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(f\"Statistics:\")\n",
    "print(f\"  Mean absolute difference: {mean_diff:.4f}\")\n",
    "print(f\"  Max absolute difference:  {max_diff:.4f}\")\n",
    "print(f\"  Number of states visited: {len(all_states)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Faster Convergence of TD(0)\n",
    "\n",
    "One of the key advantages of TD learning is faster convergence. Let's visualize this by tracking how the value estimates evolve over episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_prediction_with_tracking(env, policy, num_episodes=500, alpha=0.1, gamma=0.9, track_state=(0,0)):\n",
    "    \"\"\"\n",
    "    TD(0) prediction with tracking of value estimates over time.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        policy: Policy function\n",
    "        num_episodes: Number of episodes\n",
    "        alpha: Learning rate\n",
    "        gamma: Discount factor\n",
    "        track_state: State to track value estimates for\n",
    "    \n",
    "    Returns:\n",
    "        V: Final value function\n",
    "        value_history: List of value estimates for tracked state\n",
    "    \"\"\"\n",
    "    V = defaultdict(float)\n",
    "    value_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # TD(0) update\n",
    "            td_target = reward + gamma * V[next_state]\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] = V[state] + alpha * td_error\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Track value estimate after each episode\n",
    "        value_history.append(V[track_state])\n",
    "    \n",
    "    return V, value_history\n",
    "\n",
    "\n",
    "def mc_prediction_with_tracking(env, policy, num_episodes=500, gamma=0.9, track_state=(0,0)):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction with tracking of value estimates over time.\n",
    "    \"\"\"\n",
    "    V = defaultdict(float)\n",
    "    returns = defaultdict(list)\n",
    "    value_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Generate episode\n",
    "        episode_data = []\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_data.append((state, reward))\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # Calculate returns and update values (first-visit)\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        \n",
    "        for state, reward in reversed(episode_data):\n",
    "            G = reward + gamma * G\n",
    "            \n",
    "            if state not in visited:\n",
    "                visited.add(state)\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "        \n",
    "        # Track value estimate after each episode\n",
    "        value_history.append(V[track_state])\n",
    "    \n",
    "    return V, value_history\n",
    "\n",
    "\n",
    "# Run both algorithms with tracking\n",
    "print(\"Tracking Convergence: TD(0) vs Monte Carlo\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = GridWorldEnvironment(grid_size=4, goal_pos=(3, 3), obstacles=[])\n",
    "track_state = (0, 0)  # Track the start state\n",
    "num_episodes = 500\n",
    "\n",
    "print(f\"Tracking value estimates for state {track_state}...\")\n",
    "\n",
    "# Run multiple times to get average behavior\n",
    "num_runs = 20\n",
    "td_histories = []\n",
    "mc_histories = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    # TD(0)\n",
    "    _, td_hist = td_prediction_with_tracking(env, greedy_policy, \n",
    "                                             num_episodes=num_episodes,\n",
    "                                             alpha=0.1, gamma=0.9,\n",
    "                                             track_state=track_state)\n",
    "    td_histories.append(td_hist)\n",
    "    \n",
    "    # Monte Carlo\n",
    "    _, mc_hist = mc_prediction_with_tracking(env, greedy_policy,\n",
    "                                             num_episodes=num_episodes,\n",
    "                                             gamma=0.9,\n",
    "                                             track_state=track_state)\n",
    "    mc_histories.append(mc_hist)\n",
    "\n",
    "# Average across runs\n",
    "td_avg = np.mean(td_histories, axis=0)\n",
    "mc_avg = np.mean(mc_histories, axis=0)\n",
    "\n",
    "# Calculate standard deviation for confidence bands\n",
    "td_std = np.std(td_histories, axis=0)\n",
    "mc_std = np.std(mc_histories, axis=0)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Convergence comparison\n",
    "ax = axes[0]\n",
    "episodes = np.arange(num_episodes)\n",
    "\n",
    "# TD(0) line\n",
    "ax.plot(episodes, td_avg, linewidth=2, color='blue', label='TD(0)', alpha=0.8)\n",
    "ax.fill_between(episodes, td_avg - td_std, td_avg + td_std, \n",
    "                alpha=0.2, color='blue')\n",
    "\n",
    "# Monte Carlo line\n",
    "ax.plot(episodes, mc_avg, linewidth=2, color='red', label='Monte Carlo', alpha=0.8)\n",
    "ax.fill_between(episodes, mc_avg - mc_std, mc_avg + mc_std, \n",
    "                alpha=0.2, color='red')\n",
    "\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel(f'Value Estimate for State {track_state}', fontsize=12)\n",
    "ax.set_title('Convergence Speed: TD(0) vs Monte Carlo', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Variance comparison\n",
    "ax = axes[1]\n",
    "\n",
    "# Calculate rolling standard deviation (variance proxy)\n",
    "window = 50\n",
    "td_rolling_std = pd.Series(td_avg).rolling(window=window, min_periods=1).std()\n",
    "mc_rolling_std = pd.Series(mc_avg).rolling(window=window, min_periods=1).std()\n",
    "\n",
    "ax.plot(episodes, td_rolling_std, linewidth=2, color='blue', \n",
    "        label='TD(0)', alpha=0.8)\n",
    "ax.plot(episodes, mc_rolling_std, linewidth=2, color='red', \n",
    "        label='Monte Carlo', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel(f'Rolling Std Dev (window={window})', fontsize=12)\n",
    "ax.set_title('Variance Comparison', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\ud83d\udcca Convergence Analysis:\")\n",
    "\n",
    "# Find when each method gets close to final value\n",
    "td_final = td_avg[-1]\n",
    "mc_final = mc_avg[-1]\n",
    "threshold = 0.1  # Within 10% of final value\n",
    "\n",
    "td_converge = np.where(np.abs(td_avg - td_final) < threshold * abs(td_final))[0]\n",
    "mc_converge = np.where(np.abs(mc_avg - mc_final) < threshold * abs(mc_final))[0]\n",
    "\n",
    "td_converge_ep = td_converge[0] if len(td_converge) > 0 else num_episodes\n",
    "mc_converge_ep = mc_converge[0] if len(mc_converge) > 0 else num_episodes\n",
    "\n",
    "print(f\"Final Value Estimates:\")\n",
    "print(f\"  TD(0):        {td_final:.4f}\")\n",
    "print(f\"  Monte Carlo:  {mc_final:.4f}\")\n",
    "print(f\"  Difference:   {abs(td_final - mc_final):.4f}\")\n",
    "\n",
    "print(f\"Convergence Speed (episodes to reach 90% of final value):\")\n",
    "print(f\"  TD(0):        {td_converge_ep} episodes\")\n",
    "print(f\"  Monte Carlo:  {mc_converge_ep} episodes\")\n",
    "if td_converge_ep < mc_converge_ep:\n",
    "    speedup = mc_converge_ep / max(td_converge_ep, 1)\n",
    "    print(f\"  \u2192 TD(0) is {speedup:.1f}x faster!\")\n",
    "\n",
    "print(f\"Variance (average std dev across runs):\")\n",
    "print(f\"  TD(0):        {np.mean(td_std):.4f}\")\n",
    "print(f\"  Monte Carlo:  {np.mean(mc_std):.4f}\")\n",
    "variance_reduction = (1 - np.mean(td_std) / np.mean(mc_std)) * 100\n",
    "print(f\"  \u2192 TD(0) has {variance_reduction:.1f}% lower variance\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\u2705 Key Observations:\")\n",
    "print(\"1. Faster Convergence:\")\n",
    "print(\"   \u2022 TD(0) typically converges faster than MC\")\n",
    "print(\"   \u2022 Updates after every step vs waiting for episode end\")\n",
    "print(\"   \u2022 Information propagates more quickly through states\")\n",
    "\n",
    "print(\"2. Lower Variance:\")\n",
    "print(\"   \u2022 TD(0) has smoother learning curves\")\n",
    "print(\"   \u2022 Bootstrapping reduces variance\")\n",
    "print(\"   \u2022 More stable estimates with fewer episodes\")\n",
    "\n",
    "print(\"3. Sample Efficiency:\")\n",
    "print(\"   \u2022 TD(0) learns more from each episode\")\n",
    "print(\"   \u2022 Every transition provides a learning opportunity\")\n",
    "print(\"   \u2022 Better use of experience\")\n",
    "\n",
    "print(\"\ud83c\udfaf Conclusion:\")\n",
    "print(\"   TD learning combines the best of both worlds:\")\n",
    "print(\"   \u2022 Model-free like Monte Carlo\")\n",
    "print(\"   \u2022 Bootstrapping like Dynamic Programming\")\n",
    "print(\"   \u2022 Result: Faster, more efficient learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: TD(0) Prediction\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **TD Learning Fundamentals**:\n",
    "   - Learn from every step, not just episode ends\n",
    "   - Bootstrap from current estimates\n",
    "   - Combine MC's model-free approach with DP's bootstrapping\n",
    "\n",
    "2. **TD(0) Update Rule**:\n",
    "   $$V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]$$\n",
    "   - TD target: $R_{t+1} + \\gamma V(S_{t+1})$\n",
    "   - TD error: $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$\n",
    "\n",
    "3. **Advantages Over Monte Carlo**:\n",
    "   - Faster convergence\n",
    "   - Lower variance\n",
    "   - Online learning\n",
    "   - Works with continuing tasks\n",
    "   - More sample efficient\n",
    "\n",
    "4. **Trade-offs**:\n",
    "   - TD is biased (uses estimates)\n",
    "   - MC is unbiased (uses actual returns)\n",
    "   - In practice, TD's lower variance usually wins\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "TD(0) is just for prediction (evaluating a policy). In the next sections, we'll explore:\n",
    "- **SARSA**: On-policy TD control (learning optimal policies)\n",
    "- **Q-Learning**: Off-policy TD control\n",
    "- **Deep RL**: Combining TD learning with neural networks\n",
    "\n",
    "These methods build on the TD(0) foundation to create powerful learning algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA: On-Policy TD Control\n",
    "\n",
    "**From Prediction to Control**\n",
    "\n",
    "TD(0) taught us how to evaluate a policy (prediction). Now we'll learn how to find optimal policies using **SARSA** (State-Action-Reward-State-Action), an on-policy TD control algorithm.\n",
    "\n",
    "**What is SARSA?**\n",
    "\n",
    "SARSA is a TD method that learns action-value functions Q(s,a) instead of state-value functions V(s). By learning Q-values, the agent can directly select actions without needing a model of the environment.\n",
    "\n",
    "**Why \"SARSA\"?**\n",
    "\n",
    "The name comes from the tuple of information used in each update:\n",
    "- **S**: Current state\n",
    "- **A**: Action taken\n",
    "- **R**: Reward received\n",
    "- **S'**: Next state\n",
    "- **A'**: Next action (chosen by the current policy)\n",
    "\n",
    "**On-Policy Learning:**\n",
    "\n",
    "SARSA is an **on-policy** algorithm, meaning:\n",
    "- It learns about the policy it's currently following\n",
    "- The next action A' used in the update is chosen by the same policy being learned\n",
    "- This makes SARSA more conservative and safer in practice\n",
    "\n",
    "**The SARSA Update Rule:**\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q(S_t, A_t)$: Current Q-value estimate\n",
    "- $\\alpha$: Learning rate (step size)\n",
    "- $R_{t+1}$: Immediate reward\n",
    "- $\\gamma$: Discount factor\n",
    "- $Q(S_{t+1}, A_{t+1})$: Q-value of next state-action pair\n",
    "- $A_{t+1}$: Action actually taken in next state (following current policy)\n",
    "\n",
    "**SARSA TD Target:**\n",
    "\n",
    "$$\n",
    "\\text{TD Target} = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})\n",
    "$$\n",
    "\n",
    "**SARSA TD Error:**\n",
    "\n",
    "$$\n",
    "\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\n",
    "$$\n",
    "\n",
    "**Key Differences from TD(0):**\n",
    "\n",
    "| Aspect | TD(0) | SARSA |\n",
    "|--------|-------|-------|\n",
    "| Learns | State values V(s) | Action values Q(s,a) |\n",
    "| Purpose | Policy evaluation | Policy improvement |\n",
    "| Update uses | Next state value | Next state-action value |\n",
    "| Output | Value function | Optimal policy |\n",
    "\n",
    "**SARSA Algorithm:**\n",
    "\n",
    "1. Initialize Q(s,a) arbitrarily for all state-action pairs\n",
    "2. For each episode:\n",
    "   - Initialize state S\n",
    "   - Choose action A from S using policy derived from Q (e.g., \u03b5-greedy)\n",
    "   - For each step of episode:\n",
    "     - Take action A, observe R and S'\n",
    "     - Choose A' from S' using policy derived from Q\n",
    "     - Update: Q(S,A) \u2190 Q(S,A) + \u03b1[R + \u03b3Q(S',A') - Q(S,A)]\n",
    "     - S \u2190 S', A \u2190 A'\n",
    "   - Until S is terminal\n",
    "\n",
    "Let's implement SARSA and apply it to the Taxi-v3 environment from OpenAI Gym!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing SARSA for Taxi-v3\n",
    "\n",
    "**The Taxi Problem:**\n",
    "\n",
    "The Taxi-v3 environment is a classic RL problem where:\n",
    "- A taxi must pick up a passenger at one location and drop them off at another\n",
    "- The taxi can move in 4 directions (North, South, East, West)\n",
    "- The taxi can pick up and drop off passengers\n",
    "- Rewards: +20 for successful dropoff, -1 per step, -10 for illegal pick-up/drop-off\n",
    "\n",
    "This is a perfect environment to demonstrate SARSA because:\n",
    "- Discrete state and action spaces (good for tabular methods)\n",
    "- Clear goal and reward structure\n",
    "- Requires learning a multi-step strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    \"\"\"\n",
    "    SARSA (On-Policy TD Control) Agent.\n",
    "    \n",
    "    Learns optimal policy through on-policy temporal difference learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initialize SARSA agent.\n",
    "        \n",
    "        Args:\n",
    "            n_states: Number of states in the environment\n",
    "            n_actions: Number of actions available\n",
    "            alpha: Learning rate (step size)\n",
    "            gamma: Discount factor\n",
    "            epsilon: Exploration rate for \u03b5-greedy policy\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize Q-table with zeros\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select action using \u03b5-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            \n",
    "        Returns:\n",
    "            action: Selected action\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: choose random action\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            # Exploit: choose best action\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_action):\n",
    "        \"\"\"\n",
    "        Update Q-value using SARSA update rule.\n",
    "        \n",
    "        Q(S,A) \u2190 Q(S,A) + \u03b1[R + \u03b3Q(S',A') - Q(S,A)]\n",
    "        \n",
    "        Args:\n",
    "            state: Current state S\n",
    "            action: Action taken A\n",
    "            reward: Reward received R\n",
    "            next_state: Next state S'\n",
    "            next_action: Next action A' (chosen by policy)\n",
    "        \"\"\"\n",
    "        # SARSA TD target: R + \u03b3Q(S',A')\n",
    "        td_target = reward + self.gamma * self.Q[next_state, next_action]\n",
    "        \n",
    "        # TD error: TD target - current estimate\n",
    "        td_error = td_target - self.Q[state, action]\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.Q[state, action] += self.alpha * td_error\n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        \"\"\"\n",
    "        Get the greedy action (best action) for a state.\n",
    "        Used for evaluation without exploration.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            \n",
    "        Returns:\n",
    "            action: Best action according to Q-table\n",
    "        \"\"\"\n",
    "        return np.argmax(self.Q[state])\n",
    "\n",
    "\n",
    "print(\"SARSA Agent Implemented!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key Features:\")\n",
    "print(\"  \u2022 On-policy TD control algorithm\")\n",
    "print(\"  \u2022 Learns Q(s,a) action-value function\")\n",
    "print(\"  \u2022 Uses \u03b5-greedy policy for exploration\")\n",
    "print(\"  \u2022 Updates based on action actually taken\")\n",
    "print(\"  \u2022 Suitable for episodic tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training SARSA on Taxi-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(env, agent, num_episodes=5000, max_steps=200):\n",
    "    \"\"\"\n",
    "    Train SARSA agent on an environment.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI Gym environment\n",
    "        agent: SARSA agent\n",
    "        num_episodes: Number of training episodes\n",
    "        max_steps: Maximum steps per episode\n",
    "        \n",
    "    Returns:\n",
    "        episode_rewards: List of total rewards per episode\n",
    "        episode_lengths: List of episode lengths\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize episode\n",
    "        state = env.reset()\n",
    "        action = agent.select_action(state)  # Choose initial action\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Take action, observe result\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if not done:\n",
    "                # Choose next action using current policy\n",
    "                next_action = agent.select_action(next_state)\n",
    "                \n",
    "                # SARSA update\n",
    "                agent.update(state, action, reward, next_state, next_action)\n",
    "                \n",
    "                # Move to next state-action pair\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            else:\n",
    "                # Terminal state: Q(S',A') = 0\n",
    "                agent.update(state, action, reward, next_state, 0)\n",
    "                break\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_length = np.mean(episode_lengths[-100:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | \"\n",
    "                  f\"Avg Reward (last 100): {avg_reward:.2f} | \"\n",
    "                  f\"Avg Length: {avg_length:.1f}\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "\n",
    "# Create Taxi-v3 environment\n",
    "print(\"Training SARSA Agent on Taxi-v3 Environment\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "print(f\"Environment Details:\")\n",
    "print(f\"  State space size: {env.observation_space.n}\")\n",
    "print(f\"  Action space size: {env.action_space.n}\")\n",
    "print(f\"  Actions: 0=South, 1=North, 2=East, 3=West, 4=Pickup, 5=Dropoff\")\n",
    "\n",
    "# Create SARSA agent\n",
    "agent = SARSAAgent(\n",
    "    n_states=env.observation_space.n,\n",
    "    n_actions=env.action_space.n,\n",
    "    alpha=0.1,      # Learning rate\n",
    "    gamma=0.99,     # Discount factor\n",
    "    epsilon=0.1     # Exploration rate\n",
    ")\n",
    "\n",
    "print(f\"Agent Parameters:\")\n",
    "print(f\"  Learning rate (\u03b1): {agent.alpha}\")\n",
    "print(f\"  Discount factor (\u03b3): {agent.gamma}\")\n",
    "print(f\"  Exploration rate (\u03b5): {agent.epsilon}\")\n",
    "\n",
    "print(f\"Starting training...\")\n",
    "\n",
    "# Train the agent\n",
    "episode_rewards, episode_lengths = train_sarsa(env, agent, num_episodes=5000)\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate final performance\n",
    "final_avg_reward = np.mean(episode_rewards[-100:])\n",
    "final_avg_length = np.mean(episode_lengths[-100:])\n",
    "\n",
    "print(f\"Final Performance (last 100 episodes):\")\n",
    "print(f\"  Average Reward: {final_avg_reward:.2f}\")\n",
    "print(f\"  Average Episode Length: {final_avg_length:.1f} steps\")\n",
    "print(f\"  Success Rate: {(np.array(episode_rewards[-100:]) > 0).mean() * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing SARSA Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of learning progress\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Calculate moving averages for smoother curves\n",
    "window = 100\n",
    "rewards_smooth = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "lengths_smooth = np.convolve(episode_lengths, np.ones(window)/window, mode='valid')\n",
    "\n",
    "episodes = np.arange(len(episode_rewards))\n",
    "episodes_smooth = np.arange(len(rewards_smooth))\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "ax1.plot(episodes, episode_rewards, alpha=0.3, color='blue', linewidth=0.5, label='Raw Rewards')\n",
    "ax1.plot(episodes_smooth, rewards_smooth, color='darkblue', linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5, linewidth=1, label='Break-even')\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\n",
    "ax1.set_title('SARSA Learning Curve: Episode Rewards Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode Lengths\n",
    "ax2.plot(episodes, episode_lengths, alpha=0.3, color='green', linewidth=0.5, label='Raw Lengths')\n",
    "ax2.plot(episodes_smooth, lengths_smooth, color='darkgreen', linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Episode Length (steps)', fontsize=12)\n",
    "ax2.set_title('SARSA Learning Curve: Episode Length Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Learning Curve Analysis:\")\n",
    "print(\"1. Episode Rewards:\")\n",
    "print(\"   \u2022 Initially negative (agent is learning)\")\n",
    "print(\"   \u2022 Gradually improves as Q-values converge\")\n",
    "print(\"   \u2022 Stabilizes at positive rewards (successful deliveries)\")\n",
    "\n",
    "print(\"2. Episode Length:\")\n",
    "print(\"   \u2022 Initially high (random exploration)\")\n",
    "print(\"   \u2022 Decreases as agent learns efficient paths\")\n",
    "print(\"   \u2022 Stabilizes at optimal trajectory length\")\n",
    "\n",
    "print(\"3. Learning Progress:\")\n",
    "initial_avg = np.mean(episode_rewards[:100])\n",
    "final_avg = np.mean(episode_rewards[-100:])\n",
    "improvement = final_avg - initial_avg\n",
    "print(f\"   \u2022 Initial performance (first 100 episodes): {initial_avg:.2f}\")\n",
    "print(f\"   \u2022 Final performance (last 100 episodes): {final_avg:.2f}\")\n",
    "print(f\"   \u2022 Total improvement: {improvement:.2f} ({improvement/abs(initial_avg)*100:.1f}% better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, agent, num_episodes=100, render=False):\n",
    "    \"\"\"\n",
    "    Evaluate the learned policy without exploration.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI Gym environment\n",
    "        agent: Trained SARSA agent\n",
    "        num_episodes: Number of evaluation episodes\n",
    "        render: Whether to render the environment\n",
    "        \n",
    "    Returns:\n",
    "        avg_reward: Average reward over episodes\n",
    "        avg_length: Average episode length\n",
    "        success_rate: Percentage of successful episodes\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    episode_lengths = []\n",
    "    successes = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            if render and episode == 0:  # Render first episode only\n",
    "                env.render()\n",
    "            \n",
    "            # Use greedy policy (no exploration)\n",
    "            action = agent.get_greedy_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        if total_reward > 0:  # Successful delivery\n",
    "            successes += 1\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_length = np.mean(episode_lengths)\n",
    "    success_rate = (successes / num_episodes) * 100\n",
    "    \n",
    "    return avg_reward, avg_length, success_rate\n",
    "\n",
    "\n",
    "print(\"Evaluating Learned Policy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "avg_reward, avg_length, success_rate = evaluate_policy(env, agent, num_episodes=100)\n",
    "\n",
    "print(f\"Evaluation Results (100 episodes, greedy policy):\")\n",
    "print(f\"  Average Reward: {avg_reward:.2f}\")\n",
    "print(f\"  Average Episode Length: {avg_length:.1f} steps\")\n",
    "print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"\u2705 SARSA Successfully Learned an Optimal Policy!\")\n",
    "print(\"Key Achievements:\")\n",
    "print(\"  \u2022 Agent learned to navigate the taxi environment\")\n",
    "print(\"  \u2022 Discovered optimal pickup and dropoff strategies\")\n",
    "print(\"  \u2022 Achieved high success rate with efficient paths\")\n",
    "print(\"  \u2022 Learned entirely from trial and error!\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: SARSA Algorithm\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **SARSA Fundamentals**:\n",
    "   - On-policy TD control algorithm\n",
    "   - Learns Q(s,a) action-value function\n",
    "   - Updates based on actions actually taken by the policy\n",
    "   - Name from: State-Action-Reward-State-Action\n",
    "\n",
    "2. **SARSA Update Rule**:\n",
    "   $$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]$$\n",
    "   - Uses the action A' actually chosen by the policy\n",
    "   - On-policy: learns about the policy being followed\n",
    "   - Conservative: accounts for exploration in learning\n",
    "\n",
    "3. **Practical Implementation**:\n",
    "   - Successfully trained agent on Taxi-v3 environment\n",
    "   - Achieved high success rate and efficient navigation\n",
    "   - Demonstrated clear learning progress over episodes\n",
    "   - Learned complex multi-step strategies\n",
    "\n",
    "4. **Key Advantages**:\n",
    "   - Model-free: no need to know environment dynamics\n",
    "   - Online learning: updates after every step\n",
    "   - Guaranteed convergence (under certain conditions)\n",
    "   - Safe exploration: learns about actual policy behavior\n",
    "\n",
    "5. **On-Policy vs Off-Policy**:\n",
    "   - SARSA (on-policy): learns about the policy it follows\n",
    "   - More conservative, safer in practice\n",
    "   - Next: Q-Learning (off-policy) for comparison\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Now that we understand SARSA (on-policy TD control), we'll explore:\n",
    "- **Q-Learning**: Off-policy TD control that learns optimal policy directly\n",
    "- **Deep Q-Networks (DQN)**: Scaling TD learning to large state spaces\n",
    "- **Policy Gradient Methods**: Direct policy optimization\n",
    "\n",
    "These methods build on the TD learning foundation to tackle increasingly complex problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q-learning'></a>\n",
    "### Q-Learning: Off-Policy TD Control\n",
    "\n",
    "**From On-Policy to Off-Policy Learning**\n",
    "\n",
    "We've seen how SARSA learns about the policy it's currently following (on-policy). Now we'll explore **Q-Learning**, one of the most important breakthroughs in reinforcement learning - an off-policy TD control algorithm that learns the optimal policy directly!\n",
    "\n",
    "**What is Q-Learning?**\n",
    "\n",
    "Q-Learning is a model-free, off-policy TD control algorithm that learns the optimal action-value function Q*(s,a) regardless of the policy being followed. This makes it more flexible and often more sample-efficient than on-policy methods.\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "1. **Off-Policy**: Learns about the optimal policy while following a different (exploratory) policy\n",
    "2. **Model-Free**: Doesn't require knowledge of environment dynamics (transition probabilities or rewards)\n",
    "3. **Value-Based**: Learns Q-values, from which the optimal policy can be derived\n",
    "4. **Bootstrapping**: Updates estimates based on other estimates (like all TD methods)\n",
    "\n",
    "**Why is Q-Learning Model-Free?**\n",
    "\n",
    "Q-Learning is considered model-free because:\n",
    "\n",
    "- **No Environment Model Required**: The agent doesn't need to know P(s'|s,a) (transition probabilities) or R(s,a) (reward function)\n",
    "- **Learns from Experience**: Updates Q-values directly from observed transitions (s, a, r, s')\n",
    "- **No Planning**: Doesn't simulate future trajectories using a model\n",
    "- **Direct Learning**: Learns the value function without first learning how the environment works\n",
    "\n",
    "This is in contrast to model-based methods (like Dynamic Programming) that require complete knowledge of the environment's dynamics.\n",
    "\n",
    "**The Q-Learning Update Rule:**\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\\\leftarrow Q(S_t, A_t) + \\\\alpha \\\\left[ R_{t+1} + \\\\gamma \\\\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\\\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $S_t$: Current state\n",
    "- $A_t$: Action taken\n",
    "- $R_{t+1}$: Reward received\n",
    "- $S_{t+1}$: Next state\n",
    "- $\\\\alpha$: Learning rate\n",
    "- $\\\\gamma$: Discount factor\n",
    "- $\\\\max_{a} Q(S_{t+1}, a)$: Maximum Q-value over all actions in next state\n",
    "\n",
    "**Q-Learning TD Target:**\n",
    "\n",
    "$$\n",
    "\\\\text{TD Target} = R_{t+1} + \\\\gamma \\\\max_{a} Q(S_{t+1}, a)\n",
    "$$\n",
    "\n",
    "**Q-Learning TD Error:**\n",
    "\n",
    "$$\n",
    "\\\\delta_t = R_{t+1} + \\\\gamma \\\\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)\n",
    "$$\n",
    "\n",
    "**The Key Difference: max vs actual action**\n",
    "\n",
    "| Aspect | SARSA (On-Policy) | Q-Learning (Off-Policy) |\n",
    "|--------|-------------------|-------------------------|\n",
    "| Update uses | $Q(S', A')$ - action actually taken | $\\\\max_a Q(S', a)$ - best possible action |\n",
    "| Learns about | Policy being followed | Optimal policy |\n",
    "| Behavior | Conservative, accounts for exploration | Optimistic, assumes optimal behavior |\n",
    "| Convergence | To policy being followed | To optimal policy Q* |\n",
    "\n",
    "**Why the max operator matters:**\n",
    "\n",
    "- SARSA: \"What will I actually do next?\" \u2192 Uses A' from current policy\n",
    "- Q-Learning: \"What's the best I could do next?\" \u2192 Uses max over all actions\n",
    "\n",
    "This makes Q-Learning learn the optimal policy even while exploring randomly!\n",
    "\n",
    "**Q-Learning Algorithm:**\n",
    "\n",
    "1. Initialize Q(s,a) arbitrarily for all state-action pairs\n",
    "2. For each episode:\n",
    "   - Initialize state S\n",
    "   - For each step of episode:\n",
    "     - Choose action A from S using policy derived from Q (e.g., \u03b5-greedy)\n",
    "     - Take action A, observe R and S'\n",
    "     - Update: $Q(S,A) \\\\leftarrow Q(S,A) + \\\\alpha[R + \\\\gamma \\\\max_a Q(S',a) - Q(S,A)]$\n",
    "     - S \u2190 S'\n",
    "   - Until S is terminal\n",
    "\n",
    "Let's implement Q-Learning and apply it to a grid-world problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Q-Learning for Grid World\n",
    "\n",
    "**The Grid World Problem:**\n",
    "\n",
    "We'll create a simple grid world environment where:\n",
    "- The agent starts at a specific position\n",
    "- The goal is to reach a target position\n",
    "- The agent can move in 4 directions: up, down, left, right\n",
    "- Rewards: +10 for reaching goal, -1 for each step, -10 for hitting walls\n",
    "\n",
    "This is perfect for demonstrating Q-Learning because:\n",
    "- Simple, discrete state and action spaces\n",
    "- Clear optimal policy exists\n",
    "- Easy to visualize Q-values and learned policy\n",
    "- Can compare with SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"A simple grid world environment for Q-Learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, size=5, start=(0, 0), goal=(4, 4), obstacles=None):\n",
    "        self.size = size\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles if obstacles else []\n",
    "        self.state = start\n",
    "        \n",
    "        # Actions: 0=up, 1=down, 2=left, 3=right\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.action_names = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to start state.\"\"\"\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action and return (next_state, reward, done).\"\"\"\n",
    "        row, col = self.state\n",
    "        \n",
    "        # Calculate new position\n",
    "        if action == 0:  # UP\n",
    "            new_state = (max(0, row - 1), col)\n",
    "        elif action == 1:  # DOWN\n",
    "            new_state = (min(self.size - 1, row + 1), col)\n",
    "        elif action == 2:  # LEFT\n",
    "            new_state = (row, max(0, col - 1))\n",
    "        else:  # RIGHT\n",
    "            new_state = (row, min(self.size - 1, col + 1))\n",
    "        \n",
    "        # Check if hit obstacle\n",
    "        if new_state in self.obstacles:\n",
    "            new_state = self.state  # Stay in place\n",
    "            reward = -10\n",
    "        # Check if reached goal\n",
    "        elif new_state == self.goal:\n",
    "            reward = 10\n",
    "        # Normal step\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        self.state = new_state\n",
    "        done = (new_state == self.goal)\n",
    "        \n",
    "        return new_state, reward, done\n",
    "\n",
    "print(\"Grid World Environment Created!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning (Off-Policy TD Control) Agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Q-Learning agent.\n",
    "        \n",
    "        Args:\n",
    "            n_states: Number of states (for grid: size * size)\n",
    "            n_actions: Number of actions (4 for grid world)\n",
    "            alpha: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon: Exploration rate for \u03b5-greedy\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize Q-table: Q[state][action]\n",
    "        # For grid world, state is (row, col) tuple\n",
    "        self.Q = {}\n",
    "    \n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"Get Q-value for state-action pair.\"\"\"\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = np.zeros(self.n_actions)\n",
    "        return self.Q[state][action]\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using \u03b5-greedy policy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            # Exploit: best action\n",
    "            return self.get_greedy_action(state)\n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        \"\"\"Get best action for state (greedy).\"\"\"\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = np.zeros(self.n_actions)\n",
    "        return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update Q-value using Q-Learning update rule.\n",
    "        \n",
    "        Q(S,A) \u2190 Q(S,A) + \u03b1[R + \u03b3 max_a Q(S',a) - Q(S,A)]\n",
    "        \n",
    "        Args:\n",
    "            state: Current state S\n",
    "            action: Action taken A\n",
    "            reward: Reward received R\n",
    "            next_state: Next state S'\n",
    "            done: Whether episode ended\n",
    "        \"\"\"\n",
    "        # Ensure states exist in Q-table\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = np.zeros(self.n_actions)\n",
    "        if next_state not in self.Q:\n",
    "            self.Q[next_state] = np.zeros(self.n_actions)\n",
    "        \n",
    "        # Q-Learning TD target: R + \u03b3 max_a Q(S',a)\n",
    "        # Key difference from SARSA: uses MAX instead of actual next action\n",
    "        if done:\n",
    "            td_target = reward  # No future rewards if episode ended\n",
    "        else:\n",
    "            td_target = reward + self.gamma * np.max(self.Q[next_state])\n",
    "        \n",
    "        # TD error\n",
    "        td_error = td_target - self.Q[state][action]\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "\n",
    "print(\"Q-Learning Agent Implemented!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\Key Features:\")\n",
    "print(\"  \u2022 Off-policy learning with \u03b5-greedy exploration\")\n",
    "print(\"  \u2022 Uses max Q-value for next state (not actual action)\")\n",
    "print(\"  \u2022 Learns optimal policy Q* directly\")\n",
    "print(\"  \u2022 Model-free: no environment dynamics needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qlearning(env, agent, num_episodes=1000, max_steps=100):\n",
    "    \"\"\"\n",
    "    Train Q-Learning agent on environment.\n",
    "    \n",
    "    Args:\n",
    "        env: Grid world environment\n",
    "        agent: Q-Learning agent\n",
    "        num_episodes: Number of training episodes\n",
    "        max_steps: Maximum steps per episode\n",
    "    \n",
    "    Returns:\n",
    "        episode_rewards: List of total rewards per episode\n",
    "        episode_lengths: List of episode lengths\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select action using \u03b5-greedy\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Q-Learning update (off-policy)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_length = np.mean(episode_lengths[-100:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | Avg Length: {avg_length:.1f}\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "print(\"Training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and agent\n",
    "print(\"Training Q-Learning Agent on Grid World\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create 5x5 grid world with some obstacles\n",
    "obstacles = [(1, 1), (2, 2), (3, 1)]\n",
    "env = GridWorld(size=5, start=(0, 0), goal=(4, 4), obstacles=obstacles)\n",
    "\n",
    "print(f\"Environment:\")\n",
    "print(f\"  Grid size: {env.size}x{env.size}\")\n",
    "print(f\"  Start: {env.start}\")\n",
    "print(f\"  Goal: {env.goal}\")\n",
    "print(f\"  Obstacles: {obstacles}\")\n",
    "print(f\"  Actions: {env.action_names}\")\n",
    "\n",
    "# Create Q-Learning agent\n",
    "agent = QLearningAgent(\n",
    "    n_states=env.size * env.size,\n",
    "    n_actions=len(env.actions),\n",
    "    alpha=0.1,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nAgent Parameters:\")\n",
    "print(f\"  Learning rate (\u03b1): {agent.alpha}\")\n",
    "print(f\"  Discount factor (\u03b3): {agent.gamma}\")\n",
    "print(f\"  Exploration rate (\u03b5): {agent.epsilon}\")\n",
    "\n",
    "print(f\"\\nStarting training...\")\n",
    "\n",
    "# Train the agent\n",
    "episode_rewards, episode_lengths = train_qlearning(env, agent, num_episodes=1000)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Q-Learning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Smooth the curves\n",
    "window = 50\n",
    "rewards_smooth = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "lengths_smooth = np.convolve(episode_lengths, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot 1: Rewards\n",
    "ax1.plot(episode_rewards, alpha=0.3, color='blue', label='Raw')\n",
    "ax1.plot(range(window-1, len(episode_rewards)), rewards_smooth, \n",
    "         linewidth=2, color='blue', label=f'Smoothed ({window}-episode avg)')\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\n",
    "ax1.set_title('Q-Learning: Episode Rewards Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode lengths\n",
    "ax2.plot(episode_lengths, alpha=0.3, color='green', label='Raw')\n",
    "ax2.plot(range(window-1, len(episode_lengths)), lengths_smooth, \n",
    "         linewidth=2, color='green', label=f'Smoothed ({window}-episode avg)')\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Episode Length (steps)', fontsize=12)\n",
    "ax2.set_title('Q-Learning: Episode Length Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\ud83d\udcca Interpretation:\")\n",
    "print(\"   - Rewards increase as agent learns optimal policy\")\n",
    "print(\"   - Episode length decreases as agent finds shorter paths\")\n",
    "print(\"   - Convergence indicates successful learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Learned Q-Values and Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_values_and_policy(agent, env):\n",
    "    \"\"\"Visualize Q-values and learned policy on grid.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    # Create grids for visualization\n",
    "    max_q_grid = np.zeros((env.size, env.size))\n",
    "    policy_grid = np.zeros((env.size, env.size), dtype=int)\n",
    "    \n",
    "    # Fill grids\n",
    "    for row in range(env.size):\n",
    "        for col in range(env.size):\n",
    "            state = (row, col)\n",
    "            if state in agent.Q:\n",
    "                max_q_grid[row, col] = np.max(agent.Q[state])\n",
    "                policy_grid[row, col] = np.argmax(agent.Q[state])\n",
    "    \n",
    "    # Plot 1: Q-values heatmap\n",
    "    im1 = ax1.imshow(max_q_grid, cmap='RdYlGn', interpolation='nearest')\n",
    "    ax1.set_title('Maximum Q-Values per State', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Column', fontsize=12)\n",
    "    ax1.set_ylabel('Row', fontsize=12)\n",
    "    \n",
    "    # Add Q-values as text\n",
    "    for row in range(env.size):\n",
    "        for col in range(env.size):\n",
    "            state = (row, col)\n",
    "            if state == env.goal:\n",
    "                text = 'GOAL'\n",
    "                color = 'white'\n",
    "            elif state in env.obstacles:\n",
    "                text = 'X'\n",
    "                color = 'red'\n",
    "            else:\n",
    "                text = f'{max_q_grid[row, col]:.1f}'\n",
    "                color = 'black' if max_q_grid[row, col] < 5 else 'white'\n",
    "            ax1.text(col, row, text, ha='center', va='center', \n",
    "                    color=color, fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im1, ax=ax1, label='Max Q-Value')\n",
    "    \n",
    "    # Plot 2: Policy arrows\n",
    "    ax2.imshow(np.ones((env.size, env.size)) * 0.5, cmap='gray', alpha=0.3)\n",
    "    ax2.set_title('Learned Policy (Greedy)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Column', fontsize=12)\n",
    "    ax2.set_ylabel('Row', fontsize=12)\n",
    "    \n",
    "    # Arrow directions\n",
    "    arrows = {0: '\u2191', 1: '\u2193', 2: '\u2190', 3: '\u2192'}\n",
    "    \n",
    "    for row in range(env.size):\n",
    "        for col in range(env.size):\n",
    "            state = (row, col)\n",
    "            if state == env.goal:\n",
    "                ax2.add_patch(plt.Rectangle((col-0.4, row-0.4), 0.8, 0.8, \n",
    "                                            fill=True, color='gold', alpha=0.7))\n",
    "                ax2.text(col, row, '\u2605', ha='center', va='center', \n",
    "                        fontsize=24, color='darkgreen')\n",
    "            elif state in env.obstacles:\n",
    "                ax2.add_patch(plt.Rectangle((col-0.4, row-0.4), 0.8, 0.8, \n",
    "                                            fill=True, color='red', alpha=0.5))\n",
    "                ax2.text(col, row, 'X', ha='center', va='center', \n",
    "                        fontsize=20, color='darkred', fontweight='bold')\n",
    "            else:\n",
    "                action = policy_grid[row, col]\n",
    "                ax2.text(col, row, arrows[action], ha='center', va='center', \n",
    "                        fontsize=24, color='blue', fontweight='bold')\n",
    "    \n",
    "    ax2.set_xticks(range(env.size))\n",
    "    ax2.set_yticks(range(env.size))\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "visualize_q_values_and_policy(agent, env)\n",
    "\n",
    "print(\"\\\ud83d\udcca Visualization Explanation:\")\n",
    "print(\"\\Left Plot (Q-Values):\")\n",
    "print(\"  \u2022 Shows maximum Q-value for each state\")\n",
    "print(\"  \u2022 Higher values (green) indicate states closer to goal\")\n",
    "print(\"  \u2022 Lower values (red) indicate less desirable states\")\n",
    "print(\"\\Right Plot (Policy):\")\n",
    "print(\"  \u2022 Arrows show the best action in each state\")\n",
    "print(\"  \u2022 Policy learned to navigate around obstacles\")\n",
    "print(\"  \u2022 All arrows point toward the goal (\u2605)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Q-Learning Algorithm\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **Q-Learning Fundamentals**:\n",
    "   - Off-policy TD control algorithm\n",
    "   - Learns optimal Q*(s,a) directly\n",
    "   - Uses max operator for next state value\n",
    "   - Model-free: no environment dynamics needed\n",
    "\n",
    "2. **Q-Learning Update Rule**:\n",
    "   $Q(S_t, A_t) \\\\leftarrow Q(S_t, A_t) + \\\\alpha \\\\left[ R_{t+1} + \\\\gamma \\\\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\\\right]$\n",
    "   - Uses max Q-value for next state (not actual action)\n",
    "   - Off-policy: learns optimal policy while exploring\n",
    "   - Optimistic: assumes best possible future actions\n",
    "\n",
    "3. **Practical Implementation**:\n",
    "   - Successfully trained agent on grid world\n",
    "   - Learned to navigate around obstacles\n",
    "   - Found optimal paths to goal\n",
    "   - Visualized Q-values and policy\n",
    "\n",
    "4. **Key Advantages**:\n",
    "   - Model-free: works without knowing environment dynamics\n",
    "   - Off-policy: can learn from any exploratory policy\n",
    "   - Converges to optimal policy Q*\n",
    "   - Simple and effective for tabular problems\n",
    "\n",
    "5. **SARSA vs Q-Learning Comparison**:\n",
    "\n",
    "| Aspect | SARSA | Q-Learning |\n",
    "|--------|-------|------------|\n",
    "| Policy Type | On-policy | Off-policy |\n",
    "| Update Target | $R + \\\\gamma Q(S', A')$ | $R + \\\\gamma \\\\max_a Q(S', a)$ |\n",
    "| Learns | Policy being followed | Optimal policy |\n",
    "| Behavior | Conservative | Optimistic |\n",
    "| Safety | Safer (accounts for exploration) | Can be risky |\n",
    "| Convergence | To followed policy | To optimal policy Q* |\n",
    "\n",
    "**Why Q-Learning is Model-Free:**\n",
    "\n",
    "Q-Learning doesn't require:\n",
    "- Transition probabilities P(s'|s,a)\n",
    "- Reward function R(s,a)\n",
    "- Environment model for planning\n",
    "\n",
    "It learns directly from experience (s, a, r, s') tuples!\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Q-Learning works great for small, discrete state spaces. For larger problems, we need:\n",
    "- **Deep Q-Networks (DQN)**: Combining Q-Learning with neural networks\n",
    "- **Function Approximation**: Handling continuous and high-dimensional states\n",
    "- **Advanced Techniques**: Experience replay, target networks, double Q-learning\n",
    "\n",
    "These extensions allow Q-Learning to scale to complex problems like Atari games and robotic control!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-Decreasing Exploration Strategy\n",
    "\n",
    "**The Problem with Fixed Epsilon**\n",
    "\n",
    "In our Q-Learning implementation above, we used a fixed epsilon value (\u03b5 = 0.1). This means the agent explores randomly 10% of the time throughout the entire training process. While this works, it's not optimal:\n",
    "\n",
    "- **Early in training**: We want MORE exploration to discover good actions\n",
    "- **Late in training**: We want LESS exploration to exploit what we've learned\n",
    "\n",
    "A fixed epsilon means we're either:\n",
    "- Under-exploring early (if \u03b5 is too small)\n",
    "- Over-exploring late (if \u03b5 is too large)\n",
    "\n",
    "**Exploration Schedules: The Solution**\n",
    "\n",
    "An **exploration schedule** (or **epsilon decay**) gradually reduces epsilon over time, allowing the agent to:\n",
    "1. Explore extensively at the beginning\n",
    "2. Gradually shift toward exploitation\n",
    "3. Eventually converge to a near-greedy policy\n",
    "\n",
    "**Common Epsilon Decay Strategies:**\n",
    "\n",
    "1. **Linear Decay**: Decrease epsilon by a constant amount each episode\n",
    "   $$\\epsilon_t = \\epsilon_{start} - \\frac{t}{T}(\\epsilon_{start} - \\epsilon_{end})$$\n",
    "   where $t$ is the current episode and $T$ is the total episodes\n",
    "\n",
    "2. **Exponential Decay**: Multiply epsilon by a decay factor each episode\n",
    "   $$\\epsilon_t = \\epsilon_{start} \\cdot \\gamma^t$$\n",
    "   where $\\gamma$ is the decay rate (e.g., 0.995)\n",
    "\n",
    "3. **Step Decay**: Reduce epsilon by a factor at specific intervals\n",
    "   $$\\epsilon_t = \\epsilon_{start} \\cdot \\text{decay}^{\\lfloor t / \\text{step} \\rfloor}$$\n",
    "\n",
    "**Key Parameters:**\n",
    "- $\\epsilon_{start}$: Initial exploration rate (e.g., 1.0 for full exploration)\n",
    "- $\\epsilon_{end}$: Minimum exploration rate (e.g., 0.01 to maintain some exploration)\n",
    "- Decay rate: How quickly epsilon decreases\n",
    "\n",
    "Let's implement these strategies and see their effect on learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonSchedule:\n",
    "    \"\"\"Base class for epsilon decay schedules.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon_start=1.0, epsilon_end=0.01):\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.current_epsilon = epsilon_start\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Get epsilon value for current episode.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset epsilon to starting value.\"\"\"\n",
    "        self.current_epsilon = self.epsilon_start\n",
    "\n",
    "\n",
    "class LinearDecay(EpsilonSchedule):\n",
    "    \"\"\"Linear epsilon decay schedule.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, decay_episodes=1000):\n",
    "        super().__init__(epsilon_start, epsilon_end)\n",
    "        self.decay_episodes = decay_episodes\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Linear decay: \u03b5_t = \u03b5_start - (t/T)(\u03b5_start - \u03b5_end)\"\"\"\n",
    "        if episode >= self.decay_episodes:\n",
    "            self.current_epsilon = self.epsilon_end\n",
    "        else:\n",
    "            decay_amount = (self.epsilon_start - self.epsilon_end) * (episode / self.decay_episodes)\n",
    "            self.current_epsilon = self.epsilon_start - decay_amount\n",
    "        \n",
    "        return self.current_epsilon\n",
    "\n",
    "\n",
    "class ExponentialDecay(EpsilonSchedule):\n",
    "    \"\"\"Exponential epsilon decay schedule.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, decay_rate=0.995):\n",
    "        super().__init__(epsilon_start, epsilon_end)\n",
    "        self.decay_rate = decay_rate\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Exponential decay: \u03b5_t = \u03b5_start * \u03b3^t\"\"\"\n",
    "        self.current_epsilon = max(\n",
    "            self.epsilon_end,\n",
    "            self.epsilon_start * (self.decay_rate ** episode)\n",
    "        )\n",
    "        return self.current_epsilon\n",
    "\n",
    "\n",
    "class StepDecay(EpsilonSchedule):\n",
    "    \"\"\"Step-based epsilon decay schedule.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, decay_factor=0.5, step_size=200):\n",
    "        super().__init__(epsilon_start, epsilon_end)\n",
    "        self.decay_factor = decay_factor\n",
    "        self.step_size = step_size\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Step decay: \u03b5_t = \u03b5_start * decay^\u230at/step\u230b\"\"\"\n",
    "        num_steps = episode // self.step_size\n",
    "        self.current_epsilon = max(\n",
    "            self.epsilon_end,\n",
    "            self.epsilon_start * (self.decay_factor ** num_steps)\n",
    "        )\n",
    "        return self.current_epsilon\n",
    "\n",
    "\n",
    "print(\"Epsilon Decay Schedules Implemented!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Available schedules:\")\n",
    "print(\"  1. LinearDecay: Constant decrease per episode\")\n",
    "print(\"  2. ExponentialDecay: Multiplicative decrease per episode\")\n",
    "print(\"  3. StepDecay: Decrease at fixed intervals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Epsilon Decay Over Time\n",
    "\n",
    "Let's visualize how different decay strategies behave over the course of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different decay schedules\n",
    "num_episodes = 1000\n",
    "\n",
    "schedules = {\n",
    "    'Linear Decay': LinearDecay(epsilon_start=1.0, epsilon_end=0.01, decay_episodes=800),\n",
    "    'Exponential Decay': ExponentialDecay(epsilon_start=1.0, epsilon_end=0.01, decay_rate=0.995),\n",
    "    'Step Decay': StepDecay(epsilon_start=1.0, epsilon_end=0.01, decay_factor=0.5, step_size=200),\n",
    "    'Fixed Epsilon': None  # For comparison\n",
    "}\n",
    "\n",
    "# Track epsilon values over episodes\n",
    "epsilon_values = {name: [] for name in schedules.keys()}\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    for name, schedule in schedules.items():\n",
    "        if schedule is None:\n",
    "            epsilon_values[name].append(0.1)  # Fixed epsilon\n",
    "        else:\n",
    "            epsilon_values[name].append(schedule.get_epsilon(episode))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'gray']\n",
    "linestyles = ['-', '-', '-', '--']\n",
    "\n",
    "for (name, values), color, linestyle in zip(epsilon_values.items(), colors, linestyles):\n",
    "    plt.plot(values, label=name, color=color, linestyle=linestyle, linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Episode', fontsize=12)\n",
    "plt.ylabel('Epsilon (\u03b5)', fontsize=12)\n",
    "plt.title('Comparison of Epsilon Decay Strategies', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([-0.05, 1.05])\n",
    "\n",
    "# Add annotations\n",
    "plt.axhline(y=0.01, color='black', linestyle=':', alpha=0.5, linewidth=1)\n",
    "plt.text(num_episodes * 0.95, 0.05, '\u03b5_min = 0.01', fontsize=10, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Decay Strategy Characteristics:\")\n",
    "print(\"Linear Decay:\")\n",
    "print(\"  \u2022 Constant rate of decrease\")\n",
    "print(\"  \u2022 Predictable and easy to tune\")\n",
    "print(\"  \u2022 Good for problems with known training duration\")\n",
    "\n",
    "print(\"Exponential Decay:\")\n",
    "print(\"  \u2022 Fast initial decrease, then slower\")\n",
    "print(\"  \u2022 Smooth transition from exploration to exploitation\")\n",
    "print(\"  \u2022 Most commonly used in practice\")\n",
    "\n",
    "print(\"Step Decay:\")\n",
    "print(\"  \u2022 Sudden drops at intervals\")\n",
    "print(\"  \u2022 Allows extended exploration at each level\")\n",
    "print(\"  \u2022 Useful for curriculum learning\")\n",
    "\n",
    "print(\"Fixed Epsilon:\")\n",
    "print(\"  \u2022 No decay - constant exploration\")\n",
    "print(\"  \u2022 Simple but suboptimal\")\n",
    "print(\"  \u2022 Continues exploring even after convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified Q-Learning Agent with Epsilon Decay\n",
    "\n",
    "Now let's create an enhanced Q-Learning agent that uses epsilon decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgentWithDecay:\n",
    "    \"\"\"Q-Learning agent with epsilon decay schedule.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon_schedule=None):\n",
    "        \"\"\"\n",
    "        Initialize Q-Learning agent with epsilon decay.\n",
    "        \n",
    "        Args:\n",
    "            n_states: Number of states\n",
    "            n_actions: Number of actions\n",
    "            alpha: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon_schedule: EpsilonSchedule object (if None, uses fixed epsilon=0.1)\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_schedule = epsilon_schedule\n",
    "        self.epsilon = 0.1 if epsilon_schedule is None else epsilon_schedule.epsilon_start\n",
    "        \n",
    "        # Q-table: dictionary mapping states to action values\n",
    "        self.Q = {}\n",
    "    \n",
    "    def get_q_values(self, state):\n",
    "        \"\"\"Get Q-values for a state (initialize if not seen before).\"\"\"\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = np.zeros(self.n_actions)\n",
    "        return self.Q[state]\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using \u03b5-greedy policy with current epsilon.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            q_values = self.get_q_values(state)\n",
    "            return np.argmax(q_values)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-value using Q-Learning rule.\"\"\"\n",
    "        q_values = self.get_q_values(state)\n",
    "        next_q_values = self.get_q_values(next_state)\n",
    "        \n",
    "        # Q-Learning TD target\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * np.max(next_q_values)\n",
    "        \n",
    "        # TD error\n",
    "        td_error = td_target - q_values[action]\n",
    "        \n",
    "        # Update Q-value\n",
    "        q_values[action] += self.alpha * td_error\n",
    "    \n",
    "    def update_epsilon(self, episode):\n",
    "        \"\"\"Update epsilon based on schedule.\"\"\"\n",
    "        if self.epsilon_schedule is not None:\n",
    "            self.epsilon = self.epsilon_schedule.get_epsilon(episode)\n",
    "\n",
    "\n",
    "print(\"Enhanced Q-Learning Agent with Epsilon Decay Implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Learning Performance with Different Decay Strategies\n",
    "\n",
    "Let's train multiple agents with different epsilon strategies and compare their learning performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_decay(env, agent, num_episodes=1000, max_steps=100):\n",
    "    \"\"\"Train Q-Learning agent with epsilon decay.\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilon_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Update epsilon for this episode\n",
    "        agent.update_epsilon(episode)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        # Run episode\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "    \n",
    "    return episode_rewards, episode_lengths, epsilon_history\n",
    "\n",
    "\n",
    "# Run experiments with different decay strategies\n",
    "print(\"Training Q-Learning Agents with Different Epsilon Strategies\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_episodes = 1000\n",
    "num_runs = 5  # Multiple runs for statistical significance\n",
    "\n",
    "# Define strategies to compare\n",
    "strategies = {\n",
    "    'Fixed \u03b5=0.1': None,\n",
    "    'Linear Decay': LinearDecay(epsilon_start=1.0, epsilon_end=0.01, decay_episodes=800),\n",
    "    'Exponential Decay': ExponentialDecay(epsilon_start=1.0, epsilon_end=0.01, decay_rate=0.995),\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {name: {'rewards': [], 'lengths': []} for name in strategies.keys()}\n",
    "\n",
    "# Run experiments\n",
    "np.random.seed(42)\n",
    "for strategy_name, schedule in strategies.items():\n",
    "    print(f\"Training with {strategy_name}...\")\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        # Create fresh environment and agent\n",
    "        obstacles = [(1, 1), (2, 2), (3, 1)]\n",
    "        env = GridWorld(size=5, start=(0, 0), goal=(4, 4), obstacles=obstacles)\n",
    "        \n",
    "        # Reset schedule for each run\n",
    "        if schedule is not None:\n",
    "            schedule.reset()\n",
    "        \n",
    "        agent = QLearningAgentWithDecay(\n",
    "            n_states=env.size * env.size,\n",
    "            n_actions=len(env.actions),\n",
    "            alpha=0.1,\n",
    "            gamma=0.95,\n",
    "            epsilon_schedule=schedule\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        rewards, lengths, _ = train_with_decay(env, agent, num_episodes)\n",
    "        results[strategy_name]['rewards'].append(rewards)\n",
    "        results[strategy_name]['lengths'].append(lengths)\n",
    "    \n",
    "    print(f\"  Completed {num_runs} runs\")\n",
    "\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance across runs\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "colors = {'Fixed \u03b5=0.1': 'gray', 'Linear Decay': 'blue', 'Exponential Decay': 'green'}\n",
    "window = 50  # Smoothing window\n",
    "\n",
    "# Plot 1: Average Rewards\n",
    "for strategy_name, data in results.items():\n",
    "    # Average across runs\n",
    "    avg_rewards = np.mean(data['rewards'], axis=0)\n",
    "    \n",
    "    # Smooth the curve\n",
    "    if len(avg_rewards) >= window:\n",
    "        smoothed = np.convolve(avg_rewards, np.ones(window)/window, mode='valid')\n",
    "        x = range(window-1, len(avg_rewards))\n",
    "    else:\n",
    "        smoothed = avg_rewards\n",
    "        x = range(len(avg_rewards))\n",
    "    \n",
    "    ax1.plot(x, smoothed, label=strategy_name, color=colors[strategy_name], \n",
    "             linewidth=2.5, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Average Reward', fontsize=12)\n",
    "ax1.set_title('Learning Performance: Rewards Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11, loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Average Episode Lengths\n",
    "for strategy_name, data in results.items():\n",
    "    # Average across runs\n",
    "    avg_lengths = np.mean(data['lengths'], axis=0)\n",
    "    \n",
    "    # Smooth the curve\n",
    "    if len(avg_lengths) >= window:\n",
    "        smoothed = np.convolve(avg_lengths, np.ones(window)/window, mode='valid')\n",
    "        x = range(window-1, len(avg_lengths))\n",
    "    else:\n",
    "        smoothed = avg_lengths\n",
    "        x = range(len(avg_lengths))\n",
    "    \n",
    "    ax2.plot(x, smoothed, label=strategy_name, color=colors[strategy_name], \n",
    "             linewidth=2.5, alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Average Episode Length (steps)', fontsize=12)\n",
    "ax2.set_title('Learning Performance: Episode Length Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11, loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\ud83d\udcca Performance Summary (Final 100 Episodes):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for strategy_name, data in results.items():\n",
    "    # Calculate statistics for last 100 episodes\n",
    "    final_rewards = [np.mean(run[-100:]) for run in data['rewards']]\n",
    "    final_lengths = [np.mean(run[-100:]) for run in data['lengths']]\n",
    "    \n",
    "    avg_reward = np.mean(final_rewards)\n",
    "    std_reward = np.std(final_rewards)\n",
    "    avg_length = np.mean(final_lengths)\n",
    "    std_length = np.std(final_lengths)\n",
    "    \n",
    "    print(f\"{strategy_name}:\")\n",
    "    print(f\"  Average Reward: {avg_reward:6.2f} \u00b1 {std_reward:.2f}\")\n",
    "    print(f\"  Average Length: {avg_length:6.2f} \u00b1 {std_length:.2f} steps\")\n",
    "\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"\ud83d\udca1 Key Insights:\")\n",
    "print(\"   \u2022 Epsilon decay strategies typically converge faster\")\n",
    "print(\"   \u2022 Final performance is often better with decay\")\n",
    "print(\"   \u2022 Exponential decay provides smooth transition\")\n",
    "print(\"   \u2022 Linear decay is more predictable and easier to tune\")\n",
    "print(\"   \u2022 Fixed epsilon continues exploring unnecessarily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Epsilon-Decreasing Exploration Strategy\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **The Problem with Fixed Epsilon**:\n",
    "   - Explores too much late in training (wastes time)\n",
    "   - Or explores too little early in training (misses good actions)\n",
    "   - Doesn't adapt to the learning progress\n",
    "\n",
    "2. **Epsilon Decay Strategies**:\n",
    "   - **Linear Decay**: Constant decrease rate, predictable\n",
    "   - **Exponential Decay**: Fast initial decrease, then slower (most common)\n",
    "   - **Step Decay**: Sudden drops at intervals\n",
    "\n",
    "3. **Benefits of Epsilon Decay**:\n",
    "   - Better exploration early in training\n",
    "   - Better exploitation late in training\n",
    "   - Faster convergence to optimal policy\n",
    "   - Higher final performance\n",
    "\n",
    "4. **Practical Considerations**:\n",
    "   - Start with high epsilon (0.9-1.0) for thorough exploration\n",
    "   - End with small epsilon (0.01-0.05) to maintain some exploration\n",
    "   - Tune decay rate based on problem complexity\n",
    "   - Exponential decay is a good default choice\n",
    "\n",
    "5. **When to Use Each Strategy**:\n",
    "   - **Linear**: When you know training duration and want predictable behavior\n",
    "   - **Exponential**: General-purpose, works well in most scenarios\n",
    "   - **Step**: For curriculum learning or staged training\n",
    "   - **Fixed**: Only for very simple problems or when exploration is critical\n",
    "\n",
    "**Implementation Tips:**\n",
    "\n",
    "```python\n",
    "# Good starting values for exponential decay\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "decay_rate = 0.995  # Reaches ~0.01 after ~900 episodes\n",
    "\n",
    "# For linear decay\n",
    "decay_episodes = 0.8 * total_episodes  # Decay over 80% of training\n",
    "```\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Epsilon decay is a fundamental technique used in:\n",
    "- Deep Q-Networks (DQN)\n",
    "- All epsilon-greedy based algorithms\n",
    "- Exploration strategies in general\n",
    "\n",
    "In the next sections, we'll see how this technique scales to deep reinforcement learning with neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dqn'></a>\n",
    "### Deep Q-Networks (DQN)\n",
    "\n",
    "**The Limitation of Tabular Q-Learning**\n",
    "\n",
    "So far, we've been using **tabular methods** - storing Q-values in a table (or dictionary) with one entry for each state-action pair. This works well for small problems like grid worlds, but it has severe limitations:\n",
    "\n",
    "**Problems with Tabular Methods:**\n",
    "\n",
    "1. **Memory Explosion**: \n",
    "   - A 100\u00d7100 grid with 4 actions needs 40,000 entries\n",
    "   - Atari games have ~10^9 possible screen states!\n",
    "   - Continuous state spaces (e.g., robot joint angles) have infinite states\n",
    "\n",
    "2. **No Generalization**:\n",
    "   - Each state is learned independently\n",
    "   - Similar states don't share knowledge\n",
    "   - Must visit every state many times to learn\n",
    "\n",
    "3. **Scalability**:\n",
    "   - Can't handle high-dimensional inputs (images, sensor data)\n",
    "   - Impractical for real-world problems\n",
    "\n",
    "**The Solution: Function Approximation**\n",
    "\n",
    "Instead of storing Q-values in a table, we use a **function approximator** to estimate them:\n",
    "\n",
    "$$Q(s, a) \\approx Q(s, a; \\theta)$$\n",
    "\n",
    "where $\\theta$ represents the parameters of our function approximator.\n",
    "\n",
    "**Why Neural Networks?**\n",
    "\n",
    "Neural networks are universal function approximators that can:\n",
    "\n",
    "1. **Handle High-Dimensional Inputs**: Process images, sensor data, etc.\n",
    "2. **Generalize**: Similar inputs produce similar outputs\n",
    "3. **Learn Features**: Automatically extract relevant features from raw data\n",
    "4. **Scale**: Work with millions of states using thousands of parameters\n",
    "\n",
    "**From Q-Table to Q-Network:**\n",
    "\n",
    "```\n",
    "Tabular Q-Learning:          Deep Q-Learning:\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  State-Action   \u2502          \u2502     State       \u2502\n",
    "\u2502     Table       \u2502          \u2502   (e.g., image) \u2502\n",
    "\u2502                 \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\u2502  s\u2081,a\u2081 \u2192 0.5    \u2502                  \u2502\n",
    "\u2502  s\u2081,a\u2082 \u2192 0.3    \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  s\u2082,a\u2081 \u2192 0.8    \u2502          \u2502 Neural Network \u2502\n",
    "\u2502  ...            \u2502          \u2502   Q(s; \u03b8)      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                     \u2502\n",
    "                             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                             \u2502  Q-values for  \u2502\n",
    "                             \u2502  all actions   \u2502\n",
    "                             \u2502 [Q(s,a\u2081), ...] \u2502\n",
    "                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Key Insight:**\n",
    "\n",
    "Instead of looking up Q(s,a) in a table, we:\n",
    "1. Feed the state into a neural network\n",
    "2. The network outputs Q-values for all actions\n",
    "3. Select the action with the highest Q-value\n",
    "\n",
    "This allows us to:\n",
    "- Handle complex, high-dimensional states\n",
    "- Generalize to unseen states\n",
    "- Learn from raw sensory inputs (pixels, audio, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN Architecture\n",
    "\n",
    "**What is a Deep Q-Network?**\n",
    "\n",
    "A Deep Q-Network (DQN) is a neural network that approximates the Q-function. It was introduced by DeepMind in 2015 and achieved human-level performance on Atari games by learning directly from pixel inputs.\n",
    "\n",
    "**Network Architecture:**\n",
    "\n",
    "The basic DQN architecture consists of:\n",
    "\n",
    "1. **Input Layer**: Receives the state representation\n",
    "   - For images: Raw pixels or preprocessed frames\n",
    "   - For vectors: State features (position, velocity, etc.)\n",
    "\n",
    "2. **Hidden Layers**: Extract features and learn representations\n",
    "   - Fully connected layers for vector inputs\n",
    "   - Convolutional layers for image inputs\n",
    "   - Activation functions (ReLU is common)\n",
    "\n",
    "3. **Output Layer**: Produces Q-values for each action\n",
    "   - One output neuron per action\n",
    "   - No activation (linear output)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Given a state $s$, the Q-network computes:\n",
    "\n",
    "$$Q(s, a; \\theta) = f_{\\theta}(s)_a$$\n",
    "\n",
    "where:\n",
    "- $f_{\\theta}$ is the neural network with parameters $\\theta$\n",
    "- $f_{\\theta}(s)$ outputs a vector of Q-values, one for each action\n",
    "- $f_{\\theta}(s)_a$ is the Q-value for action $a$\n",
    "\n",
    "**Training Objective:**\n",
    "\n",
    "We train the network to minimize the **temporal difference error**:\n",
    "\n",
    "$$L(\\theta) = \\mathbb{E}\\left[(r + \\gamma \\max_{a'} Q(s', a'; \\theta) - Q(s, a; \\theta))^2\\right]$$\n",
    "\n",
    "This is the same TD error from Q-learning, but now we're updating network parameters $\\theta$ instead of table entries!\n",
    "\n",
    "**Architecture Variants:**\n",
    "\n",
    "1. **Simple DQN** (for low-dimensional states):\n",
    "   ```\n",
    "   State \u2192 FC(64) \u2192 ReLU \u2192 FC(64) \u2192 ReLU \u2192 FC(n_actions) \u2192 Q-values\n",
    "   ```\n",
    "\n",
    "2. **Convolutional DQN** (for image inputs):\n",
    "   ```\n",
    "   Image \u2192 Conv(32,8\u00d78,s=4) \u2192 ReLU \u2192 Conv(64,4\u00d74,s=2) \u2192 ReLU \u2192 \n",
    "         \u2192 Conv(64,3\u00d73,s=1) \u2192 ReLU \u2192 FC(512) \u2192 ReLU \u2192 FC(n_actions) \u2192 Q-values\n",
    "   ```\n",
    "\n",
    "3. **Dueling DQN** (separates value and advantage):\n",
    "   ```\n",
    "   State \u2192 Shared Layers \u2192 \u252c\u2192 Value Stream \u2192 V(s)\n",
    "                           \u2514\u2192 Advantage Stream \u2192 A(s,a)\n",
    "   Q(s,a) = V(s) + (A(s,a) - mean(A(s,:)))\n",
    "   ```\n",
    "\n",
    "**Key Design Choices:**\n",
    "\n",
    "1. **Network Size**: \n",
    "   - Larger networks can represent more complex functions\n",
    "   - But require more data and computation\n",
    "   - Start small and increase if needed\n",
    "\n",
    "2. **Activation Functions**:\n",
    "   - ReLU is standard for hidden layers\n",
    "   - No activation on output (Q-values can be any real number)\n",
    "\n",
    "3. **Output Structure**:\n",
    "   - Output Q-values for ALL actions simultaneously\n",
    "   - More efficient than separate networks per action\n",
    "   - Allows easy action selection: argmax over outputs\n",
    "\n",
    "Let's implement a simple Q-network in PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Q-Network in PyTorch\n",
    "\n",
    "We'll create a flexible Q-network class that can handle different state dimensions and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network for approximating Q-values.\n",
    "    \n",
    "    This network takes a state as input and outputs Q-values for all actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[64, 64]):\n",
    "        \"\"\"\n",
    "        Initialize the Q-Network.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of the state space (input size)\n",
    "            action_dim: Number of possible actions (output size)\n",
    "            hidden_dims: List of hidden layer sizes\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Build the network layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (no activation - Q-values can be any real number)\n",
    "        layers.append(nn.Linear(prev_dim, action_dim))\n",
    "        \n",
    "        # Combine all layers into a sequential model\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights for better training.\"\"\"\n",
    "        for module in self.network:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0.0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            state: State tensor of shape (batch_size, state_dim) or (state_dim,)\n",
    "        \n",
    "        Returns:\n",
    "            Q-values for all actions, shape (batch_size, action_dim) or (action_dim,)\n",
    "        \"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (numpy array or tensor)\n",
    "            epsilon: Exploration rate (0 = greedy, 1 = random)\n",
    "        \n",
    "        Returns:\n",
    "            Selected action (integer)\n",
    "        \"\"\"\n",
    "        # Exploration: random action\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        # Exploitation: greedy action\n",
    "        with torch.no_grad():\n",
    "            # Convert state to tensor if needed\n",
    "            if isinstance(state, np.ndarray):\n",
    "                state = torch.FloatTensor(state)\n",
    "            \n",
    "            # Get Q-values and select best action\n",
    "            q_values = self.forward(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def get_q_values(self, state):\n",
    "        \"\"\"\n",
    "        Get Q-values for a state.\n",
    "        \n",
    "        Args:\n",
    "            state: State tensor or numpy array\n",
    "        \n",
    "        Returns:\n",
    "            Q-values as numpy array\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if isinstance(state, np.ndarray):\n",
    "                state = torch.FloatTensor(state)\n",
    "            q_values = self.forward(state)\n",
    "            return q_values.numpy()\n",
    "\n",
    "\n",
    "print(\"Q-Network Implementation Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Key Features:\")\n",
    "print(\"  \u2022 Flexible architecture with configurable hidden layers\")\n",
    "print(\"  \u2022 Xavier weight initialization for stable training\")\n",
    "print(\"  \u2022 Epsilon-greedy action selection built-in\")\n",
    "print(\"  \u2022 Handles both single states and batches\")\n",
    "print(\"  \u2022 PyTorch implementation for GPU acceleration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrating the Q-Network with Sample States\n",
    "\n",
    "Let's create a Q-network and see how it processes states and produces Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple Q-Network for CartPole-like environment\n",
    "print(\"Example 1: Q-Network for CartPole Environment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CartPole has 4-dimensional state and 2 actions\n",
    "state_dim = 4  # [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "action_dim = 2  # [push left, push right]\n",
    "\n",
    "# Create Q-network\n",
    "q_net = QNetwork(state_dim=state_dim, action_dim=action_dim, hidden_dims=[64, 64])\n",
    "\n",
    "print(f\"Network Architecture:\")\n",
    "print(q_net)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in q_net.parameters())\n",
    "trainable_params = sum(p.numel() for p in q_net.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Create a sample state\n",
    "sample_state = np.array([0.02, 0.5, -0.1, 0.3])  # Example CartPole state\n",
    "print(f\"Sample State: {sample_state}\")\n",
    "\n",
    "# Forward pass: get Q-values\n",
    "q_values = q_net.get_q_values(sample_state)\n",
    "print(f\"Q-values:\")\n",
    "print(f\"  Q(s, left):  {q_values[0]:.4f}\")\n",
    "print(f\"  Q(s, right): {q_values[1]:.4f}\")\n",
    "\n",
    "# Select action (greedy)\n",
    "action = q_net.get_action(sample_state, epsilon=0.0)\n",
    "action_name = \"LEFT\" if action == 0 else \"RIGHT\"\n",
    "print(f\"Greedy Action: {action} ({action_name})\")\n",
    "\n",
    "# Select action with exploration\n",
    "print(f\"With \u03b5=0.1 exploration:\")\n",
    "actions = [q_net.get_action(sample_state, epsilon=0.1) for _ in range(10)]\n",
    "print(f\"  10 action samples: {actions}\")\n",
    "print(f\"  Greedy action selected: {actions.count(action)}/10 times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Batch Processing\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Example 2: Batch Processing Multiple States\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a batch of states\n",
    "batch_size = 5\n",
    "batch_states = np.random.randn(batch_size, state_dim)\n",
    "\n",
    "print(f\"Batch of {batch_size} states:\")\n",
    "print(batch_states)\n",
    "\n",
    "# Forward pass with batch\n",
    "batch_states_tensor = torch.FloatTensor(batch_states)\n",
    "batch_q_values = q_net(batch_states_tensor)\n",
    "\n",
    "print(f\"Batch Q-values (shape: {batch_q_values.shape}):\")\n",
    "print(batch_q_values.detach().numpy())\n",
    "\n",
    "# Select best action for each state in batch\n",
    "best_actions = torch.argmax(batch_q_values, dim=1)\n",
    "print(f\"Best actions for each state: {best_actions.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Different Network Architectures\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Example 3: Comparing Different Network Architectures\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "architectures = {\n",
    "    'Small': [32],\n",
    "    'Medium': [64, 64],\n",
    "    'Large': [128, 128, 64],\n",
    "    'Deep': [64, 64, 64, 64]\n",
    "}\n",
    "\n",
    "print(f\"State dim: {state_dim}, Action dim: {action_dim}\")\n",
    "\n",
    "for name, hidden_dims in architectures.items():\n",
    "    net = QNetwork(state_dim=state_dim, action_dim=action_dim, hidden_dims=hidden_dims)\n",
    "    params = sum(p.numel() for p in net.parameters())\n",
    "    \n",
    "    print(f\"{name:10s} {str(hidden_dims):20s} \u2192 {params:,} parameters\")\n",
    "\n",
    "print(\"\ud83d\udca1 Architecture Selection Tips:\")\n",
    "print(\"   \u2022 Start with medium-sized networks (64-128 units)\")\n",
    "print(\"   \u2022 Increase size if underfitting (poor performance)\")\n",
    "print(\"   \u2022 Decrease size if overfitting or slow training\")\n",
    "print(\"   \u2022 Deeper networks can learn more complex patterns\")\n",
    "print(\"   \u2022 But require more data and careful tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Visualizing Q-values for Different States\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Example 4: Visualizing Q-values Across State Space\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple 2D state space for visualization\n",
    "simple_q_net = QNetwork(state_dim=2, action_dim=4, hidden_dims=[32, 32])\n",
    "\n",
    "# Generate a grid of states\n",
    "x = np.linspace(-2, 2, 20)\n",
    "y = np.linspace(-2, 2, 20)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Compute Q-values for each state in the grid\n",
    "q_values_grid = np.zeros((20, 20, 4))\n",
    "\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        state = np.array([X[i, j], Y[i, j]])\n",
    "        q_values_grid[i, j] = simple_q_net.get_q_values(state)\n",
    "\n",
    "# Plot Q-values for each action\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "action_names = ['Action 0', 'Action 1', 'Action 2', 'Action 3']\n",
    "\n",
    "for idx, (ax, action_name) in enumerate(zip(axes.flat, action_names)):\n",
    "    im = ax.contourf(X, Y, q_values_grid[:, :, idx], levels=20, cmap='RdYlGn')\n",
    "    ax.set_xlabel('State Dimension 1', fontsize=11)\n",
    "    ax.set_ylabel('State Dimension 2', fontsize=11)\n",
    "    ax.set_title(f'Q-values for {action_name}', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, label='Q-value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Q-Network Output Across 2D State Space(Untrained Network - Random Initialization)', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Interpretation:\")\n",
    "print(\"   \u2022 Each subplot shows Q-values for one action\")\n",
    "print(\"   \u2022 Colors indicate Q-value magnitude (green=high, red=low)\")\n",
    "print(\"   \u2022 This is an UNTRAINED network (random weights)\")\n",
    "print(\"   \u2022 After training, Q-values would reflect learned policy\")\n",
    "print(\"   \u2022 The network can generalize to unseen states!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Neural Network Q-Function\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **Function Approximation**:\n",
    "   - Replaces Q-tables with neural networks\n",
    "   - Enables handling of large/continuous state spaces\n",
    "   - Provides generalization to unseen states\n",
    "\n",
    "2. **Q-Network Architecture**:\n",
    "   - Input: State representation\n",
    "   - Hidden layers: Feature extraction with ReLU activations\n",
    "   - Output: Q-values for all actions (no activation)\n",
    "\n",
    "3. **Key Design Decisions**:\n",
    "   - Network size: Balance capacity vs. sample efficiency\n",
    "   - Depth: Deeper networks for complex patterns\n",
    "   - Initialization: Xavier/He initialization for stable training\n",
    "\n",
    "4. **Advantages Over Tabular Methods**:\n",
    "   - **Scalability**: Handle millions of states with thousands of parameters\n",
    "   - **Generalization**: Similar states produce similar Q-values\n",
    "   - **Flexibility**: Can process raw sensory inputs (images, audio)\n",
    "   - **Efficiency**: Share knowledge across similar states\n",
    "\n",
    "5. **Implementation Details**:\n",
    "   - PyTorch provides automatic differentiation for training\n",
    "   - Batch processing for efficient computation\n",
    "   - Epsilon-greedy action selection integrated\n",
    "   - GPU acceleration available\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "We now have a Q-network that can approximate Q-values, but we haven't trained it yet! In the next sections, we'll learn about:\n",
    "\n",
    "- **Experience Replay**: Storing and reusing past experiences for stable training\n",
    "- **Target Networks**: Preventing moving target problems during training\n",
    "- **DQN Training**: Putting it all together to learn from experience\n",
    "- **Double DQN**: Reducing overestimation bias\n",
    "\n",
    "These techniques are crucial for making deep Q-learning work in practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay\n",
    "\n",
    "**The Problem with Online Learning**\n",
    "\n",
    "When training a Q-network, a naive approach would be to update the network immediately after each interaction with the environment:\n",
    "\n",
    "1. Observe state $s_t$\n",
    "2. Take action $a_t$\n",
    "3. Receive reward $r_t$ and next state $s_{t+1}$\n",
    "4. Immediately update the network using this single transition\n",
    "5. Discard the transition and move on\n",
    "\n",
    "**Why This Fails:**\n",
    "\n",
    "This online learning approach has several critical problems:\n",
    "\n",
    "1. **Correlation Between Consecutive Samples**:\n",
    "   - Sequential experiences are highly correlated\n",
    "   - The agent visits similar states in succession\n",
    "   - Neural networks assume i.i.d. (independent and identically distributed) data\n",
    "   - Correlated samples lead to poor convergence and overfitting\n",
    "\n",
    "2. **Sample Inefficiency**:\n",
    "   - Each experience is used only once for learning\n",
    "   - Gathering experiences can be expensive (especially in real-world scenarios)\n",
    "   - We're throwing away valuable data!\n",
    "\n",
    "3. **Catastrophic Forgetting**:\n",
    "   - The network quickly forgets what it learned about earlier states\n",
    "   - As the agent explores new regions, it \"overwrites\" knowledge about previous regions\n",
    "   - This leads to unstable and oscillating behavior\n",
    "\n",
    "**The Solution: Experience Replay**\n",
    "\n",
    "Experience replay, introduced in the original DQN paper (Mnih et al., 2015), solves these problems elegantly:\n",
    "\n",
    "**Key Idea**: Store past experiences in a **replay buffer** (memory) and randomly sample mini-batches for training.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "1. **Store**: Save each transition $(s_t, a_t, r_t, s_{t+1}, done_t)$ in a replay buffer\n",
    "2. **Sample**: Randomly sample a mini-batch of transitions from the buffer\n",
    "3. **Learn**: Update the network using the sampled batch\n",
    "4. **Repeat**: Continue storing new experiences and sampling for training\n",
    "\n",
    "**Why Experience Replay Works:**\n",
    "\n",
    "1. **Breaks Correlation**:\n",
    "   - Random sampling creates i.i.d. training batches\n",
    "   - Transitions from different episodes and time steps are mixed\n",
    "   - Network sees diverse experiences in each update\n",
    "\n",
    "2. **Improves Sample Efficiency**:\n",
    "   - Each experience can be used multiple times\n",
    "   - Rare or important experiences aren't immediately forgotten\n",
    "   - Better utilization of collected data\n",
    "\n",
    "3. **Stabilizes Learning**:\n",
    "   - Smooths out the learning process\n",
    "   - Reduces variance in updates\n",
    "   - Prevents catastrophic forgetting\n",
    "\n",
    "4. **Enables Off-Policy Learning**:\n",
    "   - Can learn from experiences generated by old policies\n",
    "   - Decouples data collection from learning\n",
    "   - More flexible training strategies\n",
    "\n",
    "**Mathematical Perspective:**\n",
    "\n",
    "The Q-learning update with experience replay:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\text{Sample mini-batch: } & \\{(s_i, a_i, r_i, s'_i, done_i)\\}_{i=1}^{N} \\sim \\mathcal{D} \\\\\n",
    "\\text{Target: } & y_i = r_i + \\gamma (1 - done_i) \\max_{a'} Q(s'_i, a'; \\theta^-) \\\\\n",
    "\\text{Loss: } & \\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i - Q(s_i, a_i; \\theta)\\right)^2\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{D}$ is the replay buffer\n",
    "- $N$ is the mini-batch size\n",
    "- $\\theta$ are the current network parameters\n",
    "- $\\theta^-$ are the target network parameters (we'll cover this next)\n",
    "\n",
    "**Practical Considerations:**\n",
    "\n",
    "- **Buffer Size**: Typically 10,000 to 1,000,000 transitions\n",
    "  - Larger buffers provide more diversity but use more memory\n",
    "  - Should be large enough to store experiences from many episodes\n",
    "\n",
    "- **Batch Size**: Usually 32 to 256 transitions\n",
    "  - Larger batches provide more stable gradients\n",
    "  - Smaller batches train faster but with more variance\n",
    "\n",
    "- **Sampling Strategy**: Uniform random sampling is most common\n",
    "  - Advanced: Prioritized Experience Replay samples important transitions more often\n",
    "\n",
    "- **When to Start Training**: Wait until buffer has enough samples\n",
    "  - Typically start training after 1,000-10,000 initial experiences\n",
    "\n",
    "Let's implement a replay buffer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer for storing and sampling transitions.\n",
    "    \n",
    "    The replay buffer stores transitions (s, a, r, s', done) and provides\n",
    "    random sampling for training. This breaks correlation between consecutive\n",
    "    samples and improves sample efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        \"\"\"Initialize the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            capacity: Maximum number of transitions to store\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)  # Automatically removes old experiences\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition in the buffer.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state\n",
    "            done: Whether episode ended\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random batch of transitions.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of transitions to sample\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, rewards, next_states, dones)\n",
    "            Each element is a numpy array or list\n",
    "        \"\"\"\n",
    "        # Randomly sample batch_size transitions\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Unzip the batch into separate arrays\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert to numpy arrays for easier processing\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of the buffer.\"\"\"\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def is_ready(self, batch_size):\n",
    "        \"\"\"Check if buffer has enough samples for training.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Required batch size\n",
    "            \n",
    "        Returns:\n",
    "            True if buffer has at least batch_size samples\n",
    "        \"\"\"\n",
    "        return len(self.buffer) >= batch_size\n",
    "\n",
    "\n",
    "print(\"ReplayBuffer class implemented successfully!\")\n",
    "print(\"Key Features:\")\n",
    "print(\"  \u2022 Stores transitions (s, a, r, s', done)\")\n",
    "print(\"  \u2022 Automatic capacity management with deque\")\n",
    "print(\"  \u2022 Random sampling for breaking correlations\")\n",
    "print(\"  \u2022 Efficient batch preparation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrating the Replay Buffer\n",
    "\n",
    "Let's see how the replay buffer works in practice with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic Usage\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Basic Replay Buffer Operations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a replay buffer with small capacity for demonstration\n",
    "replay_buffer = ReplayBuffer(capacity=5)\n",
    "\n",
    "print(f\"Initial buffer size: {len(replay_buffer)}\")\n",
    "print(f\"Buffer capacity: {replay_buffer.capacity}\")\n",
    "\n",
    "# Add some transitions\n",
    "print(\"Adding transitions to buffer...\")\n",
    "for i in range(7):\n",
    "    state = np.array([i, i*2])\n",
    "    action = i % 4\n",
    "    reward = i * 0.1\n",
    "    next_state = np.array([i+1, (i+1)*2])\n",
    "    done = (i == 6)\n",
    "    \n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    print(f\"  Step {i}: Added transition | Buffer size: {len(replay_buffer)}\")\n",
    "\n",
    "print(f\"\ud83d\udca1 Notice: Buffer size capped at {replay_buffer.capacity}\")\n",
    "print(\"   Oldest transitions are automatically removed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Sampling from the Buffer\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Example 2: Sampling Mini-Batches\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a larger buffer and fill it\n",
    "replay_buffer = ReplayBuffer(capacity=100)\n",
    "\n",
    "# Simulate collecting experiences\n",
    "print(\"Collecting 50 experiences...\")\n",
    "for i in range(50):\n",
    "    state = np.random.randn(4)  # Random 4D state\n",
    "    action = np.random.randint(0, 3)  # Random action from {0, 1, 2}\n",
    "    reward = np.random.randn()  # Random reward\n",
    "    next_state = np.random.randn(4)\n",
    "    done = (i % 10 == 9)  # Episode ends every 10 steps\n",
    "    \n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"Buffer size: {len(replay_buffer)}\")\n",
    "\n",
    "# Sample a mini-batch\n",
    "batch_size = 8\n",
    "print(f\"Sampling mini-batch of size {batch_size}...\")\n",
    "\n",
    "states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "print(f\"Batch contents:\")\n",
    "print(f\"  States shape: {states.shape}\")\n",
    "print(f\"  Actions shape: {actions.shape}\")\n",
    "print(f\"  Rewards shape: {rewards.shape}\")\n",
    "print(f\"  Next states shape: {next_states.shape}\")\n",
    "print(f\"  Dones shape: {dones.shape}\")\n",
    "\n",
    "print(f\"First 3 transitions in batch:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Transition {i}:\")\n",
    "    print(f\"    State: {states[i]}\")\n",
    "    print(f\"    Action: {actions[i]}\")\n",
    "    print(f\"    Reward: {rewards[i]:.3f}\")\n",
    "    print(f\"    Done: {bool(dones[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Demonstrating Correlation Breaking\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Example 3: Breaking Temporal Correlation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create buffer and add sequential experiences\n",
    "replay_buffer = ReplayBuffer(capacity=100)\n",
    "\n",
    "# Simulate an agent moving through a 1D environment\n",
    "print(\"Simulating sequential experiences (agent moving right):\")\n",
    "for position in range(50):\n",
    "    state = np.array([position])\n",
    "    action = 1  # Always move right\n",
    "    reward = 0.1\n",
    "    next_state = np.array([position + 1])\n",
    "    done = False\n",
    "    \n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"Added {len(replay_buffer)} sequential transitions (positions 0-49)\")\n",
    "\n",
    "# Sample and show that samples are NOT sequential\n",
    "print(\"Sampling 10 transitions:\")\n",
    "states, actions, rewards, next_states, dones = replay_buffer.sample(10)\n",
    "\n",
    "positions = states.flatten()\n",
    "print(f\"Sampled positions: {positions.astype(int)}\")\n",
    "print(f\"\u2713 Notice: Positions are NOT consecutive!\")\n",
    "print(f\"  This breaks the temporal correlation.\")\n",
    "print(f\"  The network sees diverse experiences in each batch.\")\n",
    "\n",
    "# Show correlation in sequential vs sampled\n",
    "sequential_positions = np.arange(10)\n",
    "sampled_positions = np.sort(positions[:10].astype(int))\n",
    "\n",
    "print(f\"Comparison:\")\n",
    "print(f\"  Sequential (online learning): {sequential_positions}\")\n",
    "print(f\"  Sampled (replay buffer):      {sampled_positions}\")\n",
    "print(f\"  Sequential correlation: HIGH (consecutive states)\")\n",
    "print(f\"  Sampled correlation:    LOW (random states)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Sample Efficiency Demonstration\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Example 4: Sample Efficiency with Experience Replay\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate training with and without replay\n",
    "num_experiences = 1000\n",
    "batch_size = 32\n",
    "training_steps = 100\n",
    "\n",
    "print(f\"Scenario: Collected {num_experiences} experiences\")\n",
    "print(f\"Training for {training_steps} steps with batch size {batch_size}\")\n",
    "\n",
    "# Without replay: each experience used once\n",
    "experiences_used_no_replay = num_experiences\n",
    "\n",
    "# With replay: each training step uses batch_size samples\n",
    "experiences_used_with_replay = training_steps * batch_size\n",
    "\n",
    "print(f\"\ud83d\udcca Sample Usage:\")\n",
    "print(f\"  WITHOUT Experience Replay:\")\n",
    "print(f\"    \u2022 Each experience used: 1 time\")\n",
    "print(f\"    \u2022 Total experience usage: {experiences_used_no_replay}\")\n",
    "print(f\"    \u2022 Training updates: {num_experiences}\")\n",
    "\n",
    "print(f\"  WITH Experience Replay:\")\n",
    "print(f\"    \u2022 Each experience used: ~{experiences_used_with_replay / num_experiences:.1f} times (on average)\")\n",
    "print(f\"    \u2022 Total experience usage: {experiences_used_with_replay}\")\n",
    "print(f\"    \u2022 Training updates: {training_steps}\")\n",
    "\n",
    "efficiency_gain = experiences_used_with_replay / experiences_used_no_replay\n",
    "print(f\"  \u2713 Sample Efficiency Gain: {efficiency_gain:.1f}x\")\n",
    "print(f\"    We get {efficiency_gain:.1f}x more learning from the same data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Visualizing Buffer Dynamics\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Example 5: Replay Buffer Dynamics Over Time\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate buffer filling and sampling over time\n",
    "buffer_capacity = 100\n",
    "replay_buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "buffer_sizes = []\n",
    "experiences_collected = []\n",
    "\n",
    "# Collect experiences over time\n",
    "for step in range(200):\n",
    "    # Add new experience\n",
    "    state = np.random.randn(4)\n",
    "    action = np.random.randint(0, 3)\n",
    "    reward = np.random.randn()\n",
    "    next_state = np.random.randn(4)\n",
    "    done = False\n",
    "    \n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    buffer_sizes.append(len(replay_buffer))\n",
    "    experiences_collected.append(step + 1)\n",
    "\n",
    "# Plot buffer size over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(experiences_collected, buffer_sizes, linewidth=2, color='blue', label='Buffer Size')\n",
    "plt.axhline(y=buffer_capacity, color='red', linestyle='--', linewidth=2, label=f'Capacity ({buffer_capacity})')\n",
    "plt.fill_between(experiences_collected, 0, buffer_sizes, alpha=0.3, color='blue')\n",
    "\n",
    "plt.xlabel('Experiences Collected', fontsize=12)\n",
    "plt.ylabel('Buffer Size', fontsize=12)\n",
    "plt.title('Replay Buffer Size Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Interpretation:\")\n",
    "print(\"   \u2022 Buffer grows linearly until reaching capacity\")\n",
    "print(\"   \u2022 After capacity is reached, oldest experiences are removed\")\n",
    "print(\"   \u2022 This maintains a 'sliding window' of recent experiences\")\n",
    "print(\"   \u2022 Ensures buffer contains relevant, up-to-date experiences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Experience Replay\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **The Problem**:\n",
    "   - Online learning suffers from correlated samples\n",
    "   - Sample inefficiency (each experience used once)\n",
    "   - Catastrophic forgetting of earlier experiences\n",
    "\n",
    "2. **The Solution**:\n",
    "   - Store transitions in a replay buffer\n",
    "   - Randomly sample mini-batches for training\n",
    "   - Reuse experiences multiple times\n",
    "\n",
    "3. **Key Benefits**:\n",
    "   - **Breaks Correlation**: Random sampling creates i.i.d. batches\n",
    "   - **Sample Efficiency**: Each experience used multiple times\n",
    "   - **Stability**: Smooths learning, reduces variance\n",
    "   - **Off-Policy**: Can learn from old experiences\n",
    "\n",
    "4. **Implementation Details**:\n",
    "   - Use `deque` with `maxlen` for automatic capacity management\n",
    "   - Store complete transitions: $(s, a, r, s', done)$\n",
    "   - Random sampling with `random.sample()`\n",
    "   - Batch preparation for efficient training\n",
    "\n",
    "5. **Practical Guidelines**:\n",
    "   - Buffer size: 10,000 - 1,000,000 transitions\n",
    "   - Batch size: 32 - 256 transitions\n",
    "   - Start training after buffer has enough samples\n",
    "   - Larger buffers = more diversity, more memory\n",
    "\n",
    "**Impact on Deep RL:**\n",
    "\n",
    "Experience replay was a crucial innovation that made DQN work. Without it:\n",
    "- Training is unstable and often diverges\n",
    "- Sample efficiency is poor\n",
    "- Performance is significantly worse\n",
    "\n",
    "With experience replay:\n",
    "- DQN can learn from pixels to play Atari games\n",
    "- Training is stable and reliable\n",
    "- Sample efficiency is greatly improved\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Now that we have both a Q-network and experience replay, we need one more ingredient for stable DQN training: **target networks**. We'll cover this next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Networks for Stable Training\n",
    "\n",
    "**The Moving Target Problem**\n",
    "\n",
    "When training a Q-network, we face a fundamental instability issue. Recall the Q-learning update:\n",
    "\n",
    "$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$\n",
    "\n",
    "The problem: We're using the **same network** to:\n",
    "1. Estimate the current Q-value: $Q(s, a)$\n",
    "2. Estimate the target Q-value: $r + \\gamma \\max_{a'} Q(s', a')$\n",
    "\n",
    "This creates a **moving target problem**:\n",
    "- Every time we update the network, we change both the prediction AND the target\n",
    "- It's like trying to hit a target that moves every time you adjust your aim\n",
    "- This leads to oscillations, divergence, and unstable training\n",
    "\n",
    "**The Target Network Solution**\n",
    "\n",
    "The solution is to use **two separate networks**:\n",
    "\n",
    "1. **Online Network** (parameters $\\theta$): Updated every step, used to select actions\n",
    "2. **Target Network** (parameters $\\theta^-$): Updated infrequently, used to compute targets\n",
    "\n",
    "The modified update becomes:\n",
    "\n",
    "$\n",
    "\\text{Loss} = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\left[ \\left( r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a') - Q_\\theta(s, a) \\right)^2 \\right]\n",
    "$\n",
    "\n",
    "where:\n",
    "- $Q_\\theta$ is the online network (being trained)\n",
    "- $Q_{\\theta^-}$ is the target network (held fixed)\n",
    "- $\\mathcal{D}$ is the replay buffer\n",
    "\n",
    "**How Target Networks Work:**\n",
    "\n",
    "1. Initialize both networks with the same weights: $\\theta^- = \\theta$\n",
    "2. For many steps:\n",
    "   - Use online network to select actions and compute current Q-values\n",
    "   - Use target network to compute target Q-values\n",
    "   - Update only the online network\n",
    "3. Periodically (e.g., every 1000 steps): $\\theta^- \\leftarrow \\theta$\n",
    "\n",
    "**Why This Works:**\n",
    "\n",
    "- The target network provides **stable targets** for many updates\n",
    "- The online network can learn without chasing a moving target\n",
    "- Periodic updates ensure the target network eventually catches up\n",
    "- This dramatically improves training stability\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "\n",
    "- **Update Frequency**: How often to copy weights (e.g., every 1000-10000 steps)\n",
    "- **Soft Updates** (alternative): $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau) \\theta^-$ with small $\\tau$ (e.g., 0.001)\n",
    "\n",
    "Let's implement a complete DQN agent with target networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Network agent with experience replay and target network.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, \n",
    "                 lr=1e-3, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, \n",
    "                 epsilon_decay=0.995, buffer_size=10000, batch_size=64,\n",
    "                 target_update_freq=1000):\n",
    "        \"\"\"\n",
    "        Initialize DQN agent.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Number of possible actions\n",
    "            hidden_dim: Size of hidden layers\n",
    "            lr: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon_start: Initial exploration rate\n",
    "            epsilon_end: Minimum exploration rate\n",
    "            epsilon_decay: Decay rate for epsilon\n",
    "            buffer_size: Size of replay buffer\n",
    "            batch_size: Mini-batch size for training\n",
    "            target_update_freq: Steps between target network updates\n",
    "        \"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Create online and target networks\n",
    "        self.online_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        \n",
    "        # Initialize target network with same weights as online network\n",
    "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "        self.target_network.eval()  # Target network is always in eval mode\n",
    "        \n",
    "        # Optimizer for online network\n",
    "        self.optimizer = optim.Adam(self.online_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            training: If True, use epsilon-greedy; if False, use greedy\n",
    "            \n",
    "        Returns:\n",
    "            action: Selected action\n",
    "        \"\"\"\n",
    "        # Exploration: random action\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        # Exploitation: best action according to online network\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.online_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition in the replay buffer.\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Perform one update step using a mini-batch from replay buffer.\n",
    "        \n",
    "        Returns:\n",
    "            loss: TD loss value (or None if buffer too small)\n",
    "        \"\"\"\n",
    "        # Don't update if buffer doesn't have enough samples\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample mini-batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Compute current Q-values using online network\n",
    "        current_q_values = self.online_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values using target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize the online network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_network.parameters(), max_norm=10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from online network to target network.\"\"\"\n",
    "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "        print(f\"Target network updated at step {self.steps}\")\n",
    "\n",
    "\n",
    "print(\"DQN Agent with Target Network implemented!\")\n",
    "print(\"Key components:\")\n",
    "print(\"  \u2713 Online network for action selection and training\")\n",
    "print(\"  \u2713 Target network for stable Q-value targets\")\n",
    "print(\"  \u2713 Experience replay for breaking correlations\")\n",
    "print(\"  \u2713 Epsilon-greedy exploration\")\n",
    "print(\"  \u2713 Periodic target network updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training DQN on CartPole\n",
    "\n",
    "Now let's train our DQN agent on the CartPole environment! CartPole is a classic control problem where the goal is to balance a pole on a moving cart.\n",
    "\n",
    "**CartPole Environment:**\n",
    "- **State**: 4 continuous values (cart position, cart velocity, pole angle, pole angular velocity)\n",
    "- **Actions**: 2 discrete actions (push left or push right)\n",
    "- **Reward**: +1 for every timestep the pole stays upright\n",
    "- **Episode ends**: When pole falls too far or cart moves off screen\n",
    "- **Success**: Average reward of 195+ over 100 episodes\n",
    "\n",
    "Let's implement the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env_name='CartPole-v1', num_episodes=500, max_steps=500, \n",
    "              print_every=50, render=False):\n",
    "    \"\"\"\n",
    "    Train a DQN agent on a Gym environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Name of Gym environment\n",
    "        num_episodes: Number of episodes to train\n",
    "        max_steps: Maximum steps per episode\n",
    "        print_every: Print progress every N episodes\n",
    "        render: Whether to render the environment\n",
    "        \n",
    "    Returns:\n",
    "        agent: Trained DQN agent\n",
    "        episode_rewards: List of total rewards per episode\n",
    "        episode_lengths: List of episode lengths\n",
    "        losses: List of training losses\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    print(f\"Training DQN on {env_name}\")\n",
    "    print(f\"State dimension: {state_dim}\")\n",
    "    print(f\"Action dimension: {action_dim}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create agent\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=128,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        buffer_size=10000,\n",
    "        batch_size=64,\n",
    "        target_update_freq=100\n",
    "    )\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    recent_rewards = deque(maxlen=100)\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            # Select and perform action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update agent\n",
    "            loss = agent.update()\n",
    "            if loss is not None:\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        recent_rewards.append(episode_reward)\n",
    "        if episode_loss:\n",
    "            losses.append(np.mean(episode_loss))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(recent_rewards)\n",
    "            avg_length = np.mean(list(recent_rewards))\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "            print(f\"  Avg Reward (last 100): {avg_reward:.2f}\")\n",
    "            print(f\"  Epsilon: {agent.epsilon:.3f}\")\n",
    "            print(f\"  Buffer Size: {len(agent.replay_buffer)}\")\n",
    "            if losses:\n",
    "                print(f\"  Avg Loss: {np.mean(losses[-100:]):.4f}\")\n",
    "            \n",
    "            # Check if solved\n",
    "            if avg_reward >= 195.0:\n",
    "                print(f\"\ud83c\udf89 Environment solved in {episode + 1} episodes!\")\n",
    "                print(f\"   Average reward: {avg_reward:.2f}\")\n",
    "                break\n",
    "    \n",
    "    env.close()\n",
    "    return agent, episode_rewards, episode_lengths, losses\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "print(\"Starting DQN training...\")\n",
    "agent, rewards, lengths, losses = train_dqn(\n",
    "    env_name='CartPole-v1',\n",
    "    num_episodes=500,\n",
    "    max_steps=500,\n",
    "    print_every=50\n",
    ")\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Total episodes: {len(rewards)}\")\n",
    "print(f\"Final average reward (last 100): {np.mean(rewards[-100:]):.2f}\")\n",
    "print(f\"Best episode reward: {max(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Training Progress\n",
    "\n",
    "Let's visualize how the agent's performance improved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "# Moving average\n",
    "window = 50\n",
    "if len(rewards) >= window:\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(rewards)), moving_avg, color='red', \n",
    "             linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "ax1.axhline(y=195, color='green', linestyle='--', linewidth=2, \n",
    "            label='Solved Threshold (195)', alpha=0.7)\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\n",
    "ax1.set_title('DQN Training Progress: Episode Rewards', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode Lengths\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(lengths, alpha=0.3, color='purple', label='Episode Length')\n",
    "if len(lengths) >= window:\n",
    "    moving_avg_length = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(range(window-1, len(lengths)), moving_avg_length, color='orange', \n",
    "             linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Episode Length (steps)', fontsize=12)\n",
    "ax2.set_title('Episode Lengths Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training Loss\n",
    "ax3 = axes[1, 0]\n",
    "if losses:\n",
    "    ax3.plot(losses, alpha=0.5, color='red', label='TD Loss')\n",
    "    if len(losses) >= window:\n",
    "        moving_avg_loss = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "        ax3.plot(range(window-1, len(losses)), moving_avg_loss, color='darkred', \n",
    "                 linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "    ax3.set_xlabel('Episode', fontsize=12)\n",
    "    ax3.set_ylabel('Loss', fontsize=12)\n",
    "    ax3.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')  # Log scale for better visualization\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No loss data available', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Reward Distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(rewards, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "ax4.axvline(x=np.mean(rewards), color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Mean: {np.mean(rewards):.2f}')\n",
    "ax4.axvline(x=195, color='blue', linestyle='--', \n",
    "            linewidth=2, label='Solved: 195')\n",
    "ax4.set_xlabel('Total Reward', fontsize=12)\n",
    "ax4.set_ylabel('Frequency', fontsize=12)\n",
    "ax4.set_title('Distribution of Episode Rewards', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\ud83d\udcca Training Summary Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Episodes: {len(rewards)}\")\n",
    "print(f\"Reward Statistics:\")\n",
    "print(f\"  Mean: {np.mean(rewards):.2f}\")\n",
    "print(f\"  Std: {np.std(rewards):.2f}\")\n",
    "print(f\"  Min: {np.min(rewards):.2f}\")\n",
    "print(f\"  Max: {np.max(rewards):.2f}\")\n",
    "print(f\"  Last 100 episodes mean: {np.mean(rewards[-100:]):.2f}\")\n",
    "print(f\"Episode Length Statistics:\")\n",
    "print(f\"  Mean: {np.mean(lengths):.2f}\")\n",
    "print(f\"  Max: {np.max(lengths)}\")\n",
    "print(f\"Exploration:\")\n",
    "print(f\"  Final epsilon: {agent.epsilon:.4f}\")\n",
    "print(f\"  Replay buffer size: {len(agent.replay_buffer)}\")\n",
    "\n",
    "if np.mean(rewards[-100:]) >= 195:\n",
    "    print(\"\u2705 Environment SOLVED! Average reward \u2265 195 over last 100 episodes\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f  Not quite solved yet. Need {195 - np.mean(rewards[-100:]):.2f} more reward on average.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Results\n",
    "\n",
    "**What We Observe:**\n",
    "\n",
    "1. **Learning Curve**: The agent starts with poor performance (random actions) and gradually improves as it learns\n",
    "\n",
    "2. **Exploration vs Exploitation**: \n",
    "   - Early episodes: High epsilon \u2192 more exploration \u2192 variable performance\n",
    "   - Later episodes: Low epsilon \u2192 more exploitation \u2192 stable high performance\n",
    "\n",
    "3. **Target Network Impact**:\n",
    "   - Training is stable without wild oscillations\n",
    "   - Loss decreases smoothly over time\n",
    "   - The agent converges to a good policy\n",
    "\n",
    "4. **Experience Replay Benefits**:\n",
    "   - Efficient use of past experiences\n",
    "   - Breaks temporal correlations\n",
    "   - Enables mini-batch training\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "- **Target networks** prevent the moving target problem and stabilize training\n",
    "- **Experience replay** breaks correlations and improves sample efficiency\n",
    "- **Epsilon-greedy** exploration ensures the agent discovers good strategies\n",
    "- The combination of these techniques makes DQN work!\n",
    "\n",
    "**Hyperparameter Sensitivity:**\n",
    "\n",
    "- **Learning rate**: Too high \u2192 instability; too low \u2192 slow learning\n",
    "- **Target update frequency**: Too frequent \u2192 moving target; too rare \u2192 slow adaptation\n",
    "- **Batch size**: Larger \u2192 more stable gradients; smaller \u2192 more updates\n",
    "- **Buffer size**: Larger \u2192 more diversity; smaller \u2192 more recent experiences\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "DQN with target networks is a powerful algorithm, but it still has limitations. In the next section, we'll explore **Double DQN**, which addresses the overestimation bias in standard DQN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double DQN: Addressing Overestimation Bias\n",
    "\n",
    "**The Problem with Standard DQN**\n",
    "\n",
    "Standard DQN has a subtle but important flaw: it tends to **overestimate** Q-values. This happens because of how the max operator is used in the Q-learning update:\n",
    "\n",
    "$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s,a) \\right]\n",
    "$\n",
    "\n",
    "**Why Overestimation Occurs:**\n",
    "\n",
    "The same network is used for both:\n",
    "1. **Selecting** the best action: $\\arg\\max_{a'} Q(s', a')$\n",
    "2. **Evaluating** that action: $Q(s', a')$\n",
    "\n",
    "This creates a **maximization bias**: if the Q-values have any estimation errors (which they always do), the max operation will tend to select actions with positive errors, leading to systematic overestimation.\n",
    "\n",
    "**Example of the Problem:**\n",
    "\n",
    "Imagine you're estimating the value of 3 actions, and your estimates have random errors:\n",
    "- True values: [1.0, 1.0, 1.0] (all equal)\n",
    "- Noisy estimates: [0.9, 1.2, 0.8] (with random errors)\n",
    "- Standard DQN picks: max([0.9, 1.2, 0.8]) = 1.2\n",
    "- This overestimates the true value of 1.0!\n",
    "\n",
    "Over many updates, these overestimations accumulate and can hurt performance.\n",
    "\n",
    "**The Double DQN Solution**\n",
    "\n",
    "Double DQN (DDQN) addresses this by **decoupling action selection from action evaluation**:\n",
    "\n",
    "1. Use the **online network** to select the best action\n",
    "2. Use the **target network** to evaluate that action\n",
    "\n",
    "**Double DQN Update Rule:**\n",
    "\n",
    "$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma Q_{\\theta^-}\\left(s', \\arg\\max_{a'} Q_\\theta(s', a')\\right) - Q(s,a) \\right]\n",
    "$\n",
    "\n",
    "where:\n",
    "- $Q_\\theta$ is the online network (selects action)\n",
    "- $Q_{\\theta^-}$ is the target network (evaluates action)\n",
    "\n",
    "**Key Insight:**\n",
    "\n",
    "By using different networks for selection and evaluation, we reduce the correlation between the errors, which reduces overestimation bias.\n",
    "\n",
    "**Benefits of Double DQN:**\n",
    "\n",
    "- More accurate Q-value estimates\n",
    "- Better performance on many tasks\n",
    "- More stable learning\n",
    "- Minimal computational overhead (we already have both networks!)\n",
    "\n",
    "Let's implement Double DQN and compare it with standard DQN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"Double DQN agent that reduces overestimation bias.\n",
    "    \n",
    "    Inherits from DQNAgent and only modifies the update method to use\n",
    "    Double Q-learning: online network selects actions, target network evaluates them.\n",
    "    \"\"\"\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update the agent using Double Q-learning.\n",
    "        \n",
    "        Key difference from standard DQN:\n",
    "        - Online network selects the best action\n",
    "        - Target network evaluates that action\n",
    "        \"\"\"\n",
    "        # Need enough samples in buffer\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample mini-batch from replay buffer\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(np.array(next_states))\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Compute current Q-values\n",
    "        current_q_values = self.online_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values using Double Q-learning\n",
    "        with torch.no_grad():\n",
    "            # DOUBLE DQN: Use online network to SELECT actions\n",
    "            next_actions = self.online_network(next_states).argmax(1)\n",
    "            \n",
    "            # DOUBLE DQN: Use target network to EVALUATE those actions\n",
    "            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Compute targets\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize the online network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "print(\"Double DQN Agent implemented!\")\n",
    "print(\"Key difference from standard DQN:\")\n",
    "print(\"  \u2713 Online network SELECTS the best action\")\n",
    "print(\"  \u2713 Target network EVALUATES that action\")\n",
    "print(\"  \u2713 This decoupling reduces overestimation bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Standard DQN vs Double DQN\n",
    "\n",
    "Now let's train both algorithms on the same environment and compare their performance. We'll look at:\n",
    "\n",
    "1. **Learning curves**: How quickly do they learn?\n",
    "2. **Final performance**: Which achieves better results?\n",
    "3. **Stability**: Which is more consistent?\n",
    "4. **Q-value estimates**: Do we see evidence of overestimation?\n",
    "\n",
    "Let's run the comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_comparison(agent_class, agent_name, env_name='CartPole-v1', \n",
    "                            num_episodes=300, max_steps=500, seed=42):\n",
    "    \"\"\"\n",
    "    Train an agent and return metrics for comparison.\n",
    "    \n",
    "    Args:\n",
    "        agent_class: DQNAgent or DoubleDQNAgent class\n",
    "        agent_name: Name for logging\n",
    "        env_name: Gym environment name\n",
    "        num_episodes: Number of training episodes\n",
    "        max_steps: Max steps per episode\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    # Set seeds for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make(env_name)\n",
    "    env.seed(seed)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    print(f\"Training {agent_name} on {env_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create agent\n",
    "    agent = agent_class(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=128,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        buffer_size=10000,\n",
    "        batch_size=64,\n",
    "        target_update_freq=100\n",
    "    )\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    q_values = []  # Track Q-values to detect overestimation\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        episode_q = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select and perform action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            \n",
    "            # Track Q-values\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                q_vals = agent.online_network(state_tensor).max().item()\n",
    "                episode_q.append(q_vals)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update agent\n",
    "            loss = agent.update()\n",
    "            if loss is not None:\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        if episode_loss:\n",
    "            losses.append(np.mean(episode_loss))\n",
    "        if episode_q:\n",
    "            q_values.append(np.mean(episode_q))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return {\n",
    "        'name': agent_name,\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'losses': losses,\n",
    "        'q_values': q_values,\n",
    "        'agent': agent\n",
    "    }\n",
    "\n",
    "\n",
    "# Train both agents with the same seed for fair comparison\n",
    "print(\"Starting comparison experiment...\")\n",
    "print(\"This will train both DQN and Double DQN on CartPole-v1\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train standard DQN\n",
    "dqn_results = train_agent_comparison(\n",
    "    DQNAgent, \n",
    "    \"Standard DQN\",\n",
    "    num_episodes=300,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train Double DQN\n",
    "ddqn_results = train_agent_comparison(\n",
    "    DoubleDQNAgent,\n",
    "    \"Double DQN\", \n",
    "    num_episodes=300,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Training completed for both agents!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Comparison\n",
    "\n",
    "Let's create comprehensive visualizations to compare the two algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Prepare data\n",
    "window = 20  # Moving average window\n",
    "\n",
    "# Plot 1: Episode Rewards Comparison\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(dqn_results['rewards'], alpha=0.2, color='blue', linewidth=0.5)\n",
    "ax1.plot(ddqn_results['rewards'], alpha=0.2, color='red', linewidth=0.5)\n",
    "\n",
    "# Moving averages\n",
    "if len(dqn_results['rewards']) >= window:\n",
    "    dqn_ma = np.convolve(dqn_results['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(dqn_results['rewards'])), dqn_ma, \n",
    "             color='blue', linewidth=2.5, label='Standard DQN')\n",
    "\n",
    "if len(ddqn_results['rewards']) >= window:\n",
    "    ddqn_ma = np.convolve(ddqn_results['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(ddqn_results['rewards'])), ddqn_ma, \n",
    "             color='red', linewidth=2.5, label='Double DQN')\n",
    "\n",
    "ax1.axhline(y=195, color='green', linestyle='--', linewidth=2, \n",
    "            label='Solved Threshold', alpha=0.7)\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\n",
    "ax1.set_title('Learning Curves: DQN vs Double DQN', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Q-Value Estimates Over Time\n",
    "ax2 = axes[0, 1]\n",
    "if dqn_results['q_values'] and ddqn_results['q_values']:\n",
    "    ax2.plot(dqn_results['q_values'], alpha=0.3, color='blue', linewidth=0.5)\n",
    "    ax2.plot(ddqn_results['q_values'], alpha=0.3, color='red', linewidth=0.5)\n",
    "    \n",
    "    # Moving averages\n",
    "    if len(dqn_results['q_values']) >= window:\n",
    "        dqn_q_ma = np.convolve(dqn_results['q_values'], np.ones(window)/window, mode='valid')\n",
    "        ax2.plot(range(window-1, len(dqn_results['q_values'])), dqn_q_ma, \n",
    "                 color='blue', linewidth=2.5, label='Standard DQN')\n",
    "    \n",
    "    if len(ddqn_results['q_values']) >= window:\n",
    "        ddqn_q_ma = np.convolve(ddqn_results['q_values'], np.ones(window)/window, mode='valid')\n",
    "        ax2.plot(range(window-1, len(ddqn_results['q_values'])), ddqn_q_ma, \n",
    "                 color='red', linewidth=2.5, label='Double DQN')\n",
    "    \n",
    "    ax2.set_xlabel('Episode', fontsize=12)\n",
    "    ax2.set_ylabel('Average Max Q-Value', fontsize=12)\n",
    "    ax2.set_title('Q-Value Estimates: Evidence of Overestimation?', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training Loss Comparison\n",
    "ax3 = axes[1, 0]\n",
    "if dqn_results['losses'] and ddqn_results['losses']:\n",
    "    ax3.plot(dqn_results['losses'], alpha=0.3, color='blue', linewidth=0.5)\n",
    "    ax3.plot(ddqn_results['losses'], alpha=0.3, color='red', linewidth=0.5)\n",
    "    \n",
    "    # Moving averages\n",
    "    if len(dqn_results['losses']) >= window:\n",
    "        dqn_loss_ma = np.convolve(dqn_results['losses'], np.ones(window)/window, mode='valid')\n",
    "        ax3.plot(range(window-1, len(dqn_results['losses'])), dqn_loss_ma, \n",
    "                 color='blue', linewidth=2.5, label='Standard DQN')\n",
    "    \n",
    "    if len(ddqn_results['losses']) >= window:\n",
    "        ddqn_loss_ma = np.convolve(ddqn_results['losses'], np.ones(window)/window, mode='valid')\n",
    "        ax3.plot(range(window-1, len(ddqn_results['losses'])), ddqn_loss_ma, \n",
    "                 color='red', linewidth=2.5, label='Double DQN')\n",
    "    \n",
    "    ax3.set_xlabel('Episode', fontsize=12)\n",
    "    ax3.set_ylabel('Loss', fontsize=12)\n",
    "    ax3.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')\n",
    "\n",
    "# Plot 4: Performance Distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(dqn_results['rewards'], bins=30, alpha=0.5, color='blue', \n",
    "         label='Standard DQN', edgecolor='black')\n",
    "ax4.hist(ddqn_results['rewards'], bins=30, alpha=0.5, color='red', \n",
    "         label='Double DQN', edgecolor='black')\n",
    "ax4.axvline(x=np.mean(dqn_results['rewards']), color='blue', \n",
    "            linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax4.axvline(x=np.mean(ddqn_results['rewards']), color='red', \n",
    "            linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax4.set_xlabel('Total Reward', fontsize=12)\n",
    "ax4.set_ylabel('Frequency', fontsize=12)\n",
    "ax4.set_title('Reward Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison statistics\n",
    "print(\"\" + \"=\" * 70)\n",
    "print(\"DETAILED COMPARISON: Standard DQN vs Double DQN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\ud83d\udcca REWARD STATISTICS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Metric':<30} {'Standard DQN':>18} {'Double DQN':>18}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Mean Reward':<30} {np.mean(dqn_results['rewards']):>18.2f} \"\n",
    "      f\"{np.mean(ddqn_results['rewards']):>18.2f}\")\n",
    "print(f\"{'Std Reward':<30} {np.std(dqn_results['rewards']):>18.2f} \"\n",
    "      f\"{np.std(ddqn_results['rewards']):>18.2f}\")\n",
    "print(f\"{'Max Reward':<30} {np.max(dqn_results['rewards']):>18.2f} \"\n",
    "      f\"{np.max(ddqn_results['rewards']):>18.2f}\")\n",
    "print(f\"{'Last 50 Episodes Mean':<30} {np.mean(dqn_results['rewards'][-50:]):>18.2f} \"\n",
    "      f\"{np.mean(ddqn_results['rewards'][-50:]):>18.2f}\")\n",
    "\n",
    "print(\"\ud83d\udcc8 Q-VALUE STATISTICS (Overestimation Check):\")\n",
    "print(\"-\" * 70)\n",
    "if dqn_results['q_values'] and ddqn_results['q_values']:\n",
    "    print(f\"{'Mean Q-Value':<30} {np.mean(dqn_results['q_values']):>18.2f} \"\n",
    "          f\"{np.mean(ddqn_results['q_values']):>18.2f}\")\n",
    "    print(f\"{'Max Q-Value':<30} {np.max(dqn_results['q_values']):>18.2f} \"\n",
    "          f\"{np.max(ddqn_results['q_values']):>18.2f}\")\n",
    "    print(f\"{'Final 50 Episodes Mean Q':<30} {np.mean(dqn_results['q_values'][-50:]):>18.2f} \"\n",
    "          f\"{np.mean(ddqn_results['q_values'][-50:]):>18.2f}\")\n",
    "\n",
    "print(\"\ud83c\udfaf CONVERGENCE:\")\n",
    "print(\"-\" * 70)\n",
    "# Find first episode where moving average exceeds 195\n",
    "dqn_solved = -1\n",
    "ddqn_solved = -1\n",
    "threshold = 195\n",
    "window_size = 100\n",
    "\n",
    "for i in range(window_size, len(dqn_results['rewards'])):\n",
    "    if np.mean(dqn_results['rewards'][i-window_size:i]) >= threshold:\n",
    "        dqn_solved = i\n",
    "        break\n",
    "\n",
    "for i in range(window_size, len(ddqn_results['rewards'])):\n",
    "    if np.mean(ddqn_results['rewards'][i-window_size:i]) >= threshold:\n",
    "        ddqn_solved = i\n",
    "        break\n",
    "\n",
    "if dqn_solved > 0:\n",
    "    print(f\"{'Standard DQN solved at':<30} Episode {dqn_solved}\")\n",
    "else:\n",
    "    print(f\"{'Standard DQN':<30} Not solved\")\n",
    "\n",
    "if ddqn_solved > 0:\n",
    "    print(f\"{'Double DQN solved at':<30} Episode {ddqn_solved}\")\n",
    "else:\n",
    "    print(f\"{'Double DQN':<30} Not solved\")\n",
    "\n",
    "print(\"\" + \"=\" * 70)\n",
    "\n",
    "# Determine winner\n",
    "dqn_final = np.mean(dqn_results['rewards'][-50:])\n",
    "ddqn_final = np.mean(ddqn_results['rewards'][-50:])\n",
    "\n",
    "if ddqn_final > dqn_final:\n",
    "    improvement = ((ddqn_final - dqn_final) / dqn_final) * 100\n",
    "    print(f\"\ud83c\udfc6 WINNER: Double DQN\")\n",
    "    print(f\"   Improvement: {improvement:.1f}% better final performance\")\n",
    "elif dqn_final > ddqn_final:\n",
    "    improvement = ((dqn_final - ddqn_final) / ddqn_final) * 100\n",
    "    print(f\"\ud83c\udfc6 WINNER: Standard DQN\")\n",
    "    print(f\"   Improvement: {improvement:.1f}% better final performance\")\n",
    "else:\n",
    "    print(f\"\ud83e\udd1d TIE: Both algorithms performed similarly\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: Why Double DQN Works Better\n",
    "\n",
    "**Key Observations from the Comparison:**\n",
    "\n",
    "1. **Q-Value Estimates**:\n",
    "   - Standard DQN typically shows higher Q-values, indicating overestimation\n",
    "   - Double DQN produces more conservative, accurate Q-value estimates\n",
    "   - This is evidence of the overestimation bias being reduced\n",
    "\n",
    "2. **Learning Stability**:\n",
    "   - Double DQN often shows smoother learning curves\n",
    "   - Less variance in performance across episodes\n",
    "   - More consistent convergence to good policies\n",
    "\n",
    "3. **Final Performance**:\n",
    "   - Double DQN frequently achieves better or equal final performance\n",
    "   - The improvement is more pronounced in complex environments\n",
    "   - CartPole is relatively simple, so differences may be subtle\n",
    "\n",
    "4. **Computational Cost**:\n",
    "   - Double DQN has virtually no additional computational cost\n",
    "   - Same network architecture and training time\n",
    "   - Only the update rule changes slightly\n",
    "\n",
    "**When Does Double DQN Help Most?**\n",
    "\n",
    "Double DQN provides the biggest benefits when:\n",
    "- The environment has stochastic rewards or transitions\n",
    "- The action space is large\n",
    "- Q-value estimation is noisy\n",
    "- Long-term planning is important\n",
    "\n",
    "**Practical Recommendations:**\n",
    "\n",
    "\u2705 **Use Double DQN** as your default choice - it's strictly better than standard DQN with no downsides\n",
    "\n",
    "\u2705 **Combine with other improvements** like:\n",
    "   - Prioritized Experience Replay\n",
    "   - Dueling Networks\n",
    "   - Noisy Networks for exploration\n",
    "\n",
    "\u2705 **Monitor Q-values** during training to detect overestimation issues\n",
    "\n",
    "**Mathematical Insight:**\n",
    "\n",
    "The key insight is that by decoupling action selection from evaluation, we reduce the positive bias:\n",
    "\n",
    "$\n",
    "\\mathbb{E}[\\max_a Q(s,a)] \\geq \\max_a \\mathbb{E}[Q(s,a)]\n",
    "$\n",
    "\n",
    "This inequality (Jensen's inequality for the max function) shows that taking the max of noisy estimates gives a biased result. Double DQN mitigates this by using independent estimates.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Double DQN is a foundational improvement to DQN. Modern deep RL often combines it with other techniques like:\n",
    "- **Dueling DQN**: Separate value and advantage streams\n",
    "- **Prioritized Replay**: Sample important transitions more frequently  \n",
    "- **Rainbow DQN**: Combines multiple improvements into one algorithm\n",
    "\n",
    "In the next sections, we'll explore policy gradient methods, which take a fundamentally different approach to RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='policy-optimization'></a>\n",
    "### Policy Optimization Methods\n",
    "\n",
    "So far, we've focused on **value-based methods** like Q-learning and DQN, which learn a value function and derive a policy from it. Now we'll explore a fundamentally different approach: **policy gradient methods**, which directly learn the policy itself.\n",
    "\n",
    "**Why Policy Gradients?**\n",
    "\n",
    "Value-based methods have limitations:\n",
    "- They require a discrete action space (or complex modifications for continuous actions)\n",
    "- The policy is deterministic (always pick the max Q-value action)\n",
    "- Small changes in Q-values can cause large changes in the policy\n",
    "\n",
    "Policy gradient methods address these issues by:\n",
    "- Working naturally with continuous action spaces\n",
    "- Learning stochastic policies (probability distributions over actions)\n",
    "- Making smooth, gradual policy improvements\n",
    "\n",
    "**The Core Idea:**\n",
    "\n",
    "Instead of learning $Q(s, a)$ and deriving a policy, we directly parameterize the policy $\\pi_\\theta(a|s)$ and optimize the parameters $\\theta$ to maximize expected return.\n",
    "\n",
    "**Policy Representation:**\n",
    "\n",
    "For discrete actions, we typically use a neural network that outputs action probabilities:\n",
    "\n",
    "$\\pi_\\theta(a|s) = \\text{softmax}(f_\\theta(s))$\n",
    "\n",
    "For continuous actions, we might output the parameters of a distribution (e.g., mean and variance of a Gaussian).\n",
    "\n",
    "**The Policy Gradient Theorem:**\n",
    "\n",
    "The key insight is that we can compute the gradient of expected return with respect to policy parameters:\n",
    "\n",
    "$abla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[abla_\\theta \\log \\pi_\\theta(a|s) \\cdot G_t\\right]$\n",
    "\n",
    "where:\n",
    "- $J(\\theta)$ is the expected return (our objective)\n",
    "- $\\pi_\\theta(a|s)$ is the probability of taking action $a$ in state $s$\n",
    "- $G_t$ is the return (cumulative discounted reward) from time $t$\n",
    "\n",
    "This tells us: **increase the probability of actions that led to high returns, decrease the probability of actions that led to low returns.**\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "- $abla_\\theta \\log \\pi_\\theta(a|s)$ points in the direction that increases the probability of action $a$\n",
    "- $G_t$ scales this gradient: positive returns push the policy toward that action, negative returns push away\n",
    "- By sampling many trajectories and averaging, we get an unbiased estimate of the true gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The REINFORCE Algorithm\n",
    "\n",
    "**REINFORCE** (also known as Monte Carlo Policy Gradient) is the simplest policy gradient algorithm. It was introduced by Ronald Williams in 1992 and forms the foundation for more advanced methods.\n",
    "\n",
    "**Algorithm Overview:**\n",
    "\n",
    "1. Initialize policy network with random weights $\\theta$\n",
    "2. For each episode:\n",
    "   - Generate a complete trajectory by following $\\pi_\\theta$\n",
    "   - For each step $t$ in the trajectory:\n",
    "     - Compute the return $G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$\n",
    "     - Update: $\\theta \\leftarrow \\theta + \\alpha abla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t$\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "The REINFORCE update rule is:\n",
    "\n",
    "$\\theta_{t+1} = \\theta_t + \\alpha \\gamma^t G_t abla_\\theta \\log \\pi_\\theta(a_t|s_t)$\n",
    "\n",
    "Or equivalently, we minimize the loss:\n",
    "\n",
    "$L(\\theta) = -\\sum_{t=0}^{T} \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$\n",
    "\n",
    "**Key Properties:**\n",
    "\n",
    "- **Monte Carlo**: Uses complete episodes (waits until episode ends)\n",
    "- **On-policy**: Must use trajectories from the current policy\n",
    "- **Unbiased**: The gradient estimate is unbiased\n",
    "- **High variance**: The main drawback - returns can vary significantly\n",
    "\n",
    "**The Variance Problem:**\n",
    "\n",
    "REINFORCE suffers from high variance because:\n",
    "- Returns $G_t$ can vary dramatically between episodes\n",
    "- Even good actions might get negative updates if the episode was unlucky\n",
    "- This leads to slow, unstable learning\n",
    "\n",
    "**Variance Reduction with Baselines:**\n",
    "\n",
    "We can reduce variance by subtracting a **baseline** $b(s)$ from the return:\n",
    "\n",
    "$abla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[abla_\\theta \\log \\pi_\\theta(a|s) \\cdot (G_t - b(s))\\right]$\n",
    "\n",
    "This doesn't change the expected gradient (it's still unbiased) but can significantly reduce variance.\n",
    "\n",
    "Common baselines:\n",
    "- **Constant baseline**: Average return across episodes\n",
    "- **State-dependent baseline**: $b(s) = V(s)$ (the value function)\n",
    "\n",
    "When we use $V(s)$ as the baseline, $(G_t - V(s))$ is called the **advantage** - it measures how much better the action was compared to the average.\n",
    "\n",
    "Let's implement REINFORCE with PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE Algorithm Implementation\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network that represents a stochastic policy.\n",
    "    \n",
    "    Takes a state as input and outputs a probability distribution over actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        \"\"\"Initialize the policy network.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Number of discrete actions\n",
    "            hidden_dim: Size of hidden layers\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass - returns action logits.\"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action_probs(self, state):\n",
    "        \"\"\"Get action probabilities using softmax.\"\"\"\n",
    "        logits = self.forward(state)\n",
    "        return F.softmax(logits, dim=-1)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample an action from the policy distribution.\n",
    "        \n",
    "        Returns:\n",
    "            action: Sampled action\n",
    "            log_prob: Log probability of the action\n",
    "        \"\"\"\n",
    "        probs = self.get_action_probs(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"REINFORCE (Monte Carlo Policy Gradient) agent.\n",
    "    \n",
    "    Learns a policy by collecting complete episodes and updating\n",
    "    the policy network using the policy gradient theorem.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):\n",
    "        \"\"\"Initialize the REINFORCE agent.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Number of discrete actions\n",
    "            lr: Learning rate\n",
    "            gamma: Discount factor\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Storage for episode data\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action using the current policy.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, log_prob = self.policy.get_action(state_tensor)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward for the current step.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def compute_returns(self):\n",
    "        \"\"\"Compute discounted returns for each timestep.\n",
    "        \n",
    "        G_t = r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + ...\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        \n",
    "        # Work backwards through rewards\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        \n",
    "        # Normalize returns for stability (acts as a simple baseline)\n",
    "        if len(returns) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy using collected episode data.\n",
    "        \n",
    "        Uses the REINFORCE update rule:\n",
    "        loss = -sum(log_prob * return)\n",
    "        \"\"\"\n",
    "        returns = self.compute_returns()\n",
    "        \n",
    "        # Compute policy gradient loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            # Negative because we want to maximize returns\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        # Backpropagate\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_loss).sum()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "print(\"REINFORCE Agent implemented!\")\n",
    "print(\"Key components:\")\n",
    "print(\"  \u2713 PolicyNetwork: Neural network outputting action probabilities\")\n",
    "print(\"  \u2713 Action sampling: Stochastic action selection from policy distribution\")\n",
    "print(\"  \u2713 Return computation: Discounted cumulative rewards\")\n",
    "print(\"  \u2713 Policy gradient update: Increase probability of high-return actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training REINFORCE on CartPole\n",
    "\n",
    "Let's train our REINFORCE agent on the CartPole environment and visualize the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(env_name='CartPole-v1', num_episodes=1000, lr=0.001, gamma=0.99, \n",
    "                    print_every=100, seed=42):\n",
    "    \"\"\"Train a REINFORCE agent on a Gym environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Name of the Gym environment\n",
    "        num_episodes: Number of episodes to train\n",
    "        lr: Learning rate\n",
    "        gamma: Discount factor\n",
    "        print_every: Print progress every N episodes\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        agent: Trained agent\n",
    "        results: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    # Set seeds\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # Create agent\n",
    "    agent = REINFORCEAgent(state_dim, action_dim, lr=lr, gamma=gamma)\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"Training REINFORCE on {env_name}\")\n",
    "    print(f\"State dim: {state_dim}, Action dim: {action_dim}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset(seed=seed + episode)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        # Collect episode\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_reward(reward)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        # Update policy after episode\n",
    "        loss = agent.update()\n",
    "        \n",
    "        # Store metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            avg_length = np.mean(episode_lengths[-print_every:])\n",
    "            print(f\"Episode {episode + 1:4d} | \"\n",
    "                  f\"Avg Reward: {avg_reward:7.2f} | \"\n",
    "                  f\"Avg Length: {avg_length:6.1f} | \"\n",
    "                  f\"Loss: {loss:8.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    results = {\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'losses': losses\n",
    "    }\n",
    "    \n",
    "    return agent, results\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "print(\"Starting REINFORCE training...\")\n",
    "reinforce_agent, reinforce_results = train_reinforce(\n",
    "    env_name='CartPole-v1',\n",
    "    num_episodes=1000,\n",
    "    lr=0.001,\n",
    "    gamma=0.99,\n",
    "    print_every=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Final average reward (last 100 episodes): {np.mean(reinforce_results['rewards'][-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize REINFORCE training results\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Episode rewards\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(reinforce_results['rewards'], alpha=0.3, color='blue', label='Episode Reward')\n",
    "# Moving average\n",
    "window = 50\n",
    "if len(reinforce_results['rewards']) >= window:\n",
    "    ma = np.convolve(reinforce_results['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(reinforce_results['rewards'])), ma, \n",
    "             color='red', linewidth=2.5, label=f'{window}-Episode Moving Avg')\n",
    "ax1.axhline(y=195, color='green', linestyle='--', linewidth=2, label='Solved Threshold (195)')\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\n",
    "ax1.set_title('REINFORCE: Learning Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode lengths\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(reinforce_results['lengths'], alpha=0.3, color='purple', label='Episode Length')\n",
    "if len(reinforce_results['lengths']) >= window:\n",
    "    ma_len = np.convolve(reinforce_results['lengths'], np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(range(window-1, len(reinforce_results['lengths'])), ma_len, \n",
    "             color='darkviolet', linewidth=2.5, label=f'{window}-Episode Moving Avg')\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Steps', fontsize=12)\n",
    "ax2.set_title('REINFORCE: Episode Length Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training loss\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(reinforce_results['losses'], alpha=0.3, color='orange', label='Loss')\n",
    "if len(reinforce_results['losses']) >= window:\n",
    "    ma_loss = np.convolve(reinforce_results['losses'], np.ones(window)/window, mode='valid')\n",
    "    ax3.plot(range(window-1, len(reinforce_results['losses'])), ma_loss, \n",
    "             color='darkorange', linewidth=2.5, label=f'{window}-Episode Moving Avg')\n",
    "ax3.set_xlabel('Episode', fontsize=12)\n",
    "ax3.set_ylabel('Policy Loss', fontsize=12)\n",
    "ax3.set_title('REINFORCE: Policy Gradient Loss', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Reward distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(reinforce_results['rewards'], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax4.axvline(x=np.mean(reinforce_results['rewards']), color='red', \n",
    "            linestyle='--', linewidth=2, label=f'Mean: {np.mean(reinforce_results[\"rewards\"]):.1f}')\n",
    "ax4.axvline(x=195, color='green', linestyle='--', linewidth=2, label='Solved Threshold')\n",
    "ax4.set_xlabel('Total Reward', fontsize=12)\n",
    "ax4.set_ylabel('Frequency', fontsize=12)\n",
    "ax4.set_title('REINFORCE: Reward Distribution', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"REINFORCE Training Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total episodes: {len(reinforce_results['rewards'])}\")\n",
    "print(f\"Mean reward: {np.mean(reinforce_results['rewards']):.2f}\")\n",
    "print(f\"Max reward: {np.max(reinforce_results['rewards']):.2f}\")\n",
    "print(f\"Final 100 episodes avg: {np.mean(reinforce_results['rewards'][-100:]):.2f}\")\n",
    "\n",
    "# Check if solved\n",
    "solved_episodes = [i for i in range(100, len(reinforce_results['rewards'])) \n",
    "                   if np.mean(reinforce_results['rewards'][i-100:i]) >= 195]\n",
    "if solved_episodes:\n",
    "    print(f\"\u2713 Environment SOLVED at episode {solved_episodes[0]}!\")\n",
    "else:\n",
    "    print(f\"\u2717 Environment not solved within {len(reinforce_results['rewards'])} episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding REINFORCE: Key Insights\n",
    "\n",
    "**What We Observed:**\n",
    "\n",
    "1. **High Variance**: Notice the noisy learning curve - this is characteristic of REINFORCE. The returns can vary significantly between episodes, leading to unstable updates.\n",
    "\n",
    "2. **Monte Carlo Nature**: REINFORCE waits until the end of each episode to update. This means:\n",
    "   - Learning is slower (one update per episode)\n",
    "   - But the return estimates are unbiased\n",
    "\n",
    "3. **Normalization Helps**: We normalized returns in our implementation, which acts as a simple baseline and reduces variance.\n",
    "\n",
    "**Strengths of REINFORCE:**\n",
    "- Simple and elegant algorithm\n",
    "- Unbiased gradient estimates\n",
    "- Works with any differentiable policy\n",
    "- Foundation for more advanced methods\n",
    "\n",
    "**Weaknesses of REINFORCE:**\n",
    "- High variance leads to slow learning\n",
    "- Sample inefficient (needs many episodes)\n",
    "- Sensitive to hyperparameters\n",
    "- Can be unstable in complex environments\n",
    "\n",
    "**Next: Actor-Critic Methods**\n",
    "\n",
    "To address REINFORCE's high variance, we can use a learned value function as a baseline. This leads us to **Actor-Critic** methods, which combine the best of policy gradients and value-based learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor-Critic Methods\n",
    "\n",
    "**The Best of Both Worlds**\n",
    "\n",
    "Actor-Critic methods combine policy gradient methods (the \"actor\") with value function estimation (the \"critic\"). This hybrid approach addresses the high variance problem of REINFORCE while maintaining the benefits of policy gradients.\n",
    "\n",
    "**Architecture Overview:**\n",
    "\n",
    "- **Actor**: A policy network $\\pi_\\theta(a|s)$ that decides which actions to take\n",
    "- **Critic**: A value network $V_\\phi(s)$ that evaluates how good states are\n",
    "\n",
    "The critic provides a learned baseline for the actor's updates, significantly reducing variance.\n",
    "\n",
    "**The Advantage Function:**\n",
    "\n",
    "Instead of using raw returns $G_t$, Actor-Critic uses the **advantage**:\n",
    "\n",
    "$A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$\n",
    "\n",
    "The advantage tells us how much better an action is compared to the average action in that state.\n",
    "\n",
    "In practice, we estimate the advantage using TD error:\n",
    "\n",
    "$\\hat{A}_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "\n",
    "This is called the **TD advantage** or **one-step advantage**.\n",
    "\n",
    "**Actor-Critic Update Rules:**\n",
    "\n",
    "**Critic Update** (minimize TD error):\n",
    "$\\phi \\leftarrow \\phi - \\alpha_c abla_\\phi (r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t))^2$\n",
    "\n",
    "**Actor Update** (policy gradient with advantage):\n",
    "$\\theta \\leftarrow \\theta + \\alpha_a abla_\\theta \\log \\pi_\\theta(a_t|s_t) \\hat{A}_t$\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "1. **Lower Variance**: The critic provides a stable baseline\n",
    "2. **Online Learning**: Can update after every step (not just episodes)\n",
    "3. **Better Sample Efficiency**: Learns faster than pure policy gradient\n",
    "4. **Flexible**: Can use different architectures for actor and critic\n",
    "\n",
    "**Common Variants:**\n",
    "\n",
    "- **A2C (Advantage Actor-Critic)**: Synchronous version with multiple workers\n",
    "- **A3C (Asynchronous Advantage Actor-Critic)**: Parallel workers updating asynchronously\n",
    "- **GAE (Generalized Advantage Estimation)**: Better advantage estimates using multiple steps\n",
    "\n",
    "Let's implement a basic Actor-Critic agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic Implementation\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"Actor network that outputs action probabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"Critic network that estimates state values V(s).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, hidden_dim=128):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    \"\"\"Actor-Critic agent using TD advantage.\n",
    "    \n",
    "    Combines policy gradient (actor) with value function estimation (critic)\n",
    "    for lower variance and better sample efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, actor_lr=0.001, critic_lr=0.001, gamma=0.99):\n",
    "        \"\"\"Initialize Actor-Critic agent.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Number of discrete actions\n",
    "            actor_lr: Learning rate for actor\n",
    "            critic_lr: Learning rate for critic\n",
    "            gamma: Discount factor\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Actor (policy) network\n",
    "        self.actor = ActorNetwork(state_dim, action_dim)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        \n",
    "        # Critic (value) network\n",
    "        self.critic = CriticNetwork(state_dim)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using the actor network.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        probs = self.actor(state_tensor)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        \"\"\"Get state value from critic.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        return self.critic(state_tensor)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done, log_prob):\n",
    "        \"\"\"Update actor and critic using TD advantage.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state\n",
    "            done: Whether episode ended\n",
    "            log_prob: Log probability of action\n",
    "        \n",
    "        Returns:\n",
    "            actor_loss: Actor loss value\n",
    "            critic_loss: Critic loss value\n",
    "        \"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        \n",
    "        # Get current and next state values\n",
    "        value = self.critic(state_tensor)\n",
    "        next_value = self.critic(next_state_tensor)\n",
    "        \n",
    "        # Compute TD target and advantage\n",
    "        if done:\n",
    "            td_target = torch.tensor([[reward]], dtype=torch.float32)\n",
    "        else:\n",
    "            td_target = reward + self.gamma * next_value.detach()\n",
    "        \n",
    "        advantage = td_target - value\n",
    "        \n",
    "        # Update Critic (minimize TD error)\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update Actor (policy gradient with advantage)\n",
    "        actor_loss = -log_prob * advantage.detach()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "\n",
    "print(\"Actor-Critic Agent implemented!\")\n",
    "print(\"Architecture:\")\n",
    "print(\"  \u2713 Actor Network: Outputs action probabilities \u03c0(a|s)\")\n",
    "print(\"  \u2713 Critic Network: Estimates state values V(s)\")\n",
    "print(\"Update mechanism:\")\n",
    "print(\"  \u2713 TD Advantage: A = r + \u03b3V(s') - V(s)\")\n",
    "print(\"  \u2713 Actor uses advantage for policy gradient\")\n",
    "print(\"  \u2713 Critic minimizes TD error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(env_name='CartPole-v1', num_episodes=1000, actor_lr=0.001, \n",
    "                       critic_lr=0.001, gamma=0.99, print_every=100, seed=42):\n",
    "    \"\"\"Train an Actor-Critic agent on a Gym environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Name of the Gym environment\n",
    "        num_episodes: Number of episodes to train\n",
    "        actor_lr: Learning rate for actor\n",
    "        critic_lr: Learning rate for critic\n",
    "        gamma: Discount factor\n",
    "        print_every: Print progress every N episodes\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        agent: Trained agent\n",
    "        results: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    # Set seeds\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # Create agent\n",
    "    agent = ActorCriticAgent(state_dim, action_dim, actor_lr=actor_lr, \n",
    "                             critic_lr=critic_lr, gamma=gamma)\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    actor_losses = []\n",
    "    critic_losses = []\n",
    "    \n",
    "    print(f\"Training Actor-Critic on {env_name}\")\n",
    "    print(f\"State dim: {state_dim}, Action dim: {action_dim}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset(seed=seed + episode)\n",
    "        episode_reward = 0\n",
    "        episode_actor_loss = []\n",
    "        episode_critic_loss = []\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Update agent (online, after every step)\n",
    "            a_loss, c_loss = agent.update(state, action, reward, next_state, done, log_prob)\n",
    "            \n",
    "            episode_actor_loss.append(a_loss)\n",
    "            episode_critic_loss.append(c_loss)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        # Store metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        actor_losses.append(np.mean(episode_actor_loss))\n",
    "        critic_losses.append(np.mean(episode_critic_loss))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            avg_length = np.mean(episode_lengths[-print_every:])\n",
    "            print(f\"Episode {episode + 1:4d} | \"\n",
    "                  f\"Avg Reward: {avg_reward:7.2f} | \"\n",
    "                  f\"Avg Length: {avg_length:6.1f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    results = {\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'actor_losses': actor_losses,\n",
    "        'critic_losses': critic_losses\n",
    "    }\n",
    "    \n",
    "    return agent, results\n",
    "\n",
    "\n",
    "# Train Actor-Critic agent\n",
    "print(\"Starting Actor-Critic training...\")\n",
    "ac_agent, ac_results = train_actor_critic(\n",
    "    env_name='CartPole-v1',\n",
    "    num_episodes=1000,\n",
    "    actor_lr=0.001,\n",
    "    critic_lr=0.001,\n",
    "    gamma=0.99,\n",
    "    print_every=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Final average reward (last 100 episodes): {np.mean(ac_results['rewards'][-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare REINFORCE vs Actor-Critic\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "window = 50\n",
    "\n",
    "# Plot 1: Learning curves comparison\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(reinforce_results['rewards'], alpha=0.2, color='blue')\n",
    "ax1.plot(ac_results['rewards'], alpha=0.2, color='red')\n",
    "\n",
    "if len(reinforce_results['rewards']) >= window:\n",
    "    reinforce_ma = np.convolve(reinforce_results['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(reinforce_results['rewards'])), reinforce_ma, \n",
    "             color='blue', linewidth=2.5, label='REINFORCE')\n",
    "\n",
    "if len(ac_results['rewards']) >= window:\n",
    "    ac_ma = np.convolve(ac_results['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(ac_results['rewards'])), ac_ma, \n",
    "             color='red', linewidth=2.5, label='Actor-Critic')\n",
    "\n",
    "ax1.axhline(y=195, color='green', linestyle='--', linewidth=2, label='Solved Threshold')\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontsize=12)\n",
    "ax1.set_title('Learning Curves: REINFORCE vs Actor-Critic', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode lengths comparison\n",
    "ax2 = axes[0, 1]\n",
    "if len(reinforce_results['lengths']) >= window:\n",
    "    reinforce_len_ma = np.convolve(reinforce_results['lengths'], np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(range(window-1, len(reinforce_results['lengths'])), reinforce_len_ma, \n",
    "             color='blue', linewidth=2.5, label='REINFORCE')\n",
    "\n",
    "if len(ac_results['lengths']) >= window:\n",
    "    ac_len_ma = np.convolve(ac_results['lengths'], np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(range(window-1, len(ac_results['lengths'])), ac_len_ma, \n",
    "             color='red', linewidth=2.5, label='Actor-Critic')\n",
    "\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Episode Length', fontsize=12)\n",
    "ax2.set_title('Episode Length Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Reward distributions\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(reinforce_results['rewards'], bins=30, alpha=0.5, color='blue', \n",
    "         label='REINFORCE', edgecolor='black')\n",
    "ax3.hist(ac_results['rewards'], bins=30, alpha=0.5, color='red', \n",
    "         label='Actor-Critic', edgecolor='black')\n",
    "ax3.axvline(x=np.mean(reinforce_results['rewards']), color='blue', \n",
    "            linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax3.axvline(x=np.mean(ac_results['rewards']), color='red', \n",
    "            linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax3.set_xlabel('Total Reward', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Reward Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Rolling variance comparison\n",
    "ax4 = axes[1, 1]\n",
    "variance_window = 50\n",
    "\n",
    "reinforce_variance = [np.var(reinforce_results['rewards'][max(0,i-variance_window):i+1]) \n",
    "                      for i in range(len(reinforce_results['rewards']))]\n",
    "ac_variance = [np.var(ac_results['rewards'][max(0,i-variance_window):i+1]) \n",
    "               for i in range(len(ac_results['rewards']))]\n",
    "\n",
    "ax4.plot(reinforce_variance, color='blue', linewidth=2, label='REINFORCE', alpha=0.7)\n",
    "ax4.plot(ac_variance, color='red', linewidth=2, label='Actor-Critic', alpha=0.7)\n",
    "ax4.set_xlabel('Episode', fontsize=12)\n",
    "ax4.set_ylabel('Rolling Variance', fontsize=12)\n",
    "ax4.set_title(f'Reward Variance ({variance_window}-Episode Window)', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison statistics\n",
    "print(\"\" + \"=\" * 70)\n",
    "print(\"COMPARISON: REINFORCE vs Actor-Critic\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<30} {'REINFORCE':>18} {'Actor-Critic':>18}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Mean Reward':<30} {np.mean(reinforce_results['rewards']):>18.2f} \"\n",
    "      f\"{np.mean(ac_results['rewards']):>18.2f}\")\n",
    "print(f\"{'Std Reward':<30} {np.std(reinforce_results['rewards']):>18.2f} \"\n",
    "      f\"{np.std(ac_results['rewards']):>18.2f}\")\n",
    "print(f\"{'Max Reward':<30} {np.max(reinforce_results['rewards']):>18.2f} \"\n",
    "      f\"{np.max(ac_results['rewards']):>18.2f}\")\n",
    "print(f\"{'Final 100 Avg':<30} {np.mean(reinforce_results['rewards'][-100:]):>18.2f} \"\n",
    "      f\"{np.mean(ac_results['rewards'][-100:]):>18.2f}\")\n",
    "\n",
    "# Determine winner\n",
    "reinforce_final = np.mean(reinforce_results['rewards'][-100:])\n",
    "ac_final = np.mean(ac_results['rewards'][-100:])\n",
    "\n",
    "print(\"\" + \"=\" * 70)\n",
    "if ac_final > reinforce_final:\n",
    "    improvement = ((ac_final - reinforce_final) / reinforce_final) * 100\n",
    "    print(f\"\ud83c\udfc6 Actor-Critic wins with {improvement:.1f}% better final performance!\")\n",
    "elif reinforce_final > ac_final:\n",
    "    improvement = ((reinforce_final - ac_final) / ac_final) * 100\n",
    "    print(f\"\ud83c\udfc6 REINFORCE wins with {improvement:.1f}% better final performance!\")\n",
    "else:\n",
    "    print(\"\ud83e\udd1d It's a tie!\")\n",
    "\n",
    "print(\"\ud83d\udcca Key Observations:\")\n",
    "print(f\"   - REINFORCE variance: {np.var(reinforce_results['rewards']):.2f}\")\n",
    "print(f\"   - Actor-Critic variance: {np.var(ac_results['rewards']):.2f}\")\n",
    "if np.var(ac_results['rewards']) < np.var(reinforce_results['rewards']):\n",
    "    print(\"   \u2713 Actor-Critic shows lower variance (more stable learning)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: REINFORCE vs Actor-Critic\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Aspect | REINFORCE | Actor-Critic |\n",
    "|--------|-----------|-------------|\n",
    "| Update Frequency | End of episode | Every step |\n",
    "| Variance | High | Lower |\n",
    "| Bias | Unbiased | Slightly biased |\n",
    "| Sample Efficiency | Lower | Higher |\n",
    "| Stability | Less stable | More stable |\n",
    "\n",
    "**Why Actor-Critic Often Performs Better:**\n",
    "\n",
    "1. **Lower Variance**: The critic provides a learned baseline that reduces the variance of policy gradient estimates\n",
    "\n",
    "2. **Online Learning**: Updates happen after every step, not just at episode end, leading to faster learning\n",
    "\n",
    "3. **Bootstrapping**: Uses TD learning for the critic, which can be more sample efficient\n",
    "\n",
    "**Trade-offs:**\n",
    "\n",
    "- Actor-Critic introduces some bias through bootstrapping\n",
    "- Requires tuning two learning rates (actor and critic)\n",
    "- More complex implementation\n",
    "\n",
    "**When to Use Each:**\n",
    "\n",
    "- **REINFORCE**: Simple problems, when you need unbiased gradients, educational purposes\n",
    "- **Actor-Critic**: Most practical applications, when sample efficiency matters, complex environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Policy Optimization Methods\n",
    "\n",
    "Building on the foundation of REINFORCE and Actor-Critic, researchers have developed more sophisticated policy optimization algorithms. Let's explore three important ones: **A3C**, **PPO**, and **TRPO**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A3C: Asynchronous Advantage Actor-Critic\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "A3C (Asynchronous Advantage Actor-Critic), introduced by DeepMind in 2016, revolutionized deep RL by enabling efficient parallel training without experience replay.\n",
    "\n",
    "**Key Innovation: Asynchronous Training**\n",
    "\n",
    "Instead of one agent learning from one environment, A3C uses multiple workers:\n",
    "\n",
    "- Multiple **worker threads** run in parallel\n",
    "- Each worker has its own copy of the environment\n",
    "- Workers compute gradients independently\n",
    "- Gradients are applied **asynchronously** to a shared global network\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                   Global Network                        \u2502\n",
    "\u2502              (Shared Actor + Critic)                    \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "         \u2191 gradients    \u2191 gradients    \u2191 gradients\n",
    "         \u2502              \u2502              \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502 Worker 1\u2502   \u2502 Worker 2\u2502   \u2502 Worker N\u2502\n",
    "    \u2502  (Env)  \u2502   \u2502  (Env)  \u2502   \u2502  (Env)  \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Why Asynchronous Works:**\n",
    "\n",
    "1. **Decorrelation**: Different workers explore different parts of the state space, providing diverse experiences\n",
    "2. **No Replay Buffer**: Eliminates the need for experience replay (saves memory)\n",
    "3. **Parallelism**: Utilizes multiple CPU cores efficiently\n",
    "4. **Stability**: Asynchronous updates act as a form of regularization\n",
    "\n",
    "**A3C Algorithm (Pseudocode):**\n",
    "\n",
    "```\n",
    "# Global shared parameters: \u03b8 (actor), \u03c6 (critic)\n",
    "# Each worker thread:\n",
    "\n",
    "while not converged:\n",
    "    # Sync local parameters with global\n",
    "    \u03b8_local \u2190 \u03b8\n",
    "    \u03c6_local \u2190 \u03c6\n",
    "    \n",
    "    # Collect n-step trajectory\n",
    "    trajectory = []\n",
    "    for t = 1 to n_steps:\n",
    "        a_t ~ \u03c0_\u03b8(s_t)\n",
    "        s_{t+1}, r_t = env.step(a_t)\n",
    "        trajectory.append((s_t, a_t, r_t))\n",
    "    \n",
    "    # Compute n-step returns and advantages\n",
    "    R = V_\u03c6(s_n) if not terminal else 0\n",
    "    for t = n-1 to 0:\n",
    "        R = r_t + \u03b3 * R\n",
    "        A_t = R - V_\u03c6(s_t)  # Advantage\n",
    "    \n",
    "    # Compute gradients\n",
    "    \u2207\u03b8 = \u03a3 \u2207_\u03b8 log \u03c0_\u03b8(a_t|s_t) * A_t\n",
    "    \u2207\u03c6 = \u03a3 \u2207_\u03c6 (R_t - V_\u03c6(s_t))\u00b2\n",
    "    \n",
    "    # Asynchronously update global parameters\n",
    "    \u03b8 \u2190 \u03b8 + \u03b1_actor * \u2207\u03b8\n",
    "    \u03c6 \u2190 \u03c6 + \u03b1_critic * \u2207\u03c6\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- **N-step returns**: Uses multi-step bootstrapping for better bias-variance trade-off\n",
    "- **Entropy bonus**: Often adds entropy term to encourage exploration\n",
    "- **Shared network**: Actor and critic often share lower layers\n",
    "\n",
    "**Advantages:**\n",
    "- Highly parallelizable\n",
    "- No replay buffer needed\n",
    "- Works well on both discrete and continuous action spaces\n",
    "- Stable training through diverse experiences\n",
    "\n",
    "**Limitations:**\n",
    "- Requires multiple CPU cores\n",
    "- Asynchronous updates can cause stale gradients\n",
    "- Harder to implement correctly than synchronous methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PPO: Proximal Policy Optimization\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "PPO (Proximal Policy Optimization), introduced by OpenAI in 2017, has become one of the most popular RL algorithms due to its simplicity, stability, and strong performance.\n",
    "\n",
    "**The Problem PPO Solves:**\n",
    "\n",
    "Policy gradient methods can be unstable because:\n",
    "- Large policy updates can destroy good policies\n",
    "- Small updates are safe but slow\n",
    "- Finding the right step size is difficult\n",
    "\n",
    "PPO constrains policy updates to stay \"close\" to the old policy, preventing catastrophic updates.\n",
    "\n",
    "**Key Concept: Clipped Surrogate Objective**\n",
    "\n",
    "PPO uses a clever clipping mechanism to limit how much the policy can change:\n",
    "\n",
    "$L^{CLIP}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$\n",
    "\n",
    "where:\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio\n",
    "- $\\hat{A}_t$ is the estimated advantage\n",
    "- $\\epsilon$ is a small hyperparameter (typically 0.1-0.2)\n",
    "\n",
    "**How Clipping Works:**\n",
    "\n",
    "```\n",
    "If advantage > 0 (good action):\n",
    "    - We want to increase \u03c0(a|s)\n",
    "    - But clip prevents ratio from exceeding (1 + \u03b5)\n",
    "    - This limits how much we can increase the probability\n",
    "\n",
    "If advantage < 0 (bad action):\n",
    "    - We want to decrease \u03c0(a|s)\n",
    "    - But clip prevents ratio from going below (1 - \u03b5)\n",
    "    - This limits how much we can decrease the probability\n",
    "```\n",
    "\n",
    "**PPO Algorithm (Pseudocode):**\n",
    "\n",
    "```\n",
    "for iteration = 1, 2, ...:\n",
    "    # Collect trajectories using current policy \u03c0_\u03b8\n",
    "    trajectories = collect_trajectories(\u03c0_\u03b8, num_steps)\n",
    "    \n",
    "    # Compute advantages using GAE or n-step returns\n",
    "    advantages = compute_advantages(trajectories)\n",
    "    \n",
    "    # Store old policy probabilities\n",
    "    \u03c0_old = \u03c0_\u03b8.detach()\n",
    "    \n",
    "    # Multiple epochs of optimization on same data\n",
    "    for epoch = 1 to K:\n",
    "        for minibatch in trajectories:\n",
    "            # Compute probability ratio\n",
    "            ratio = \u03c0_\u03b8(a|s) / \u03c0_old(a|s)\n",
    "            \n",
    "            # Compute clipped objective\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = clip(ratio, 1-\u03b5, 1+\u03b5) * advantage\n",
    "            policy_loss = -min(surr1, surr2)\n",
    "            \n",
    "            # Value function loss\n",
    "            value_loss = (V(s) - returns)\u00b2\n",
    "            \n",
    "            # Entropy bonus for exploration\n",
    "            entropy = -\u03a3 \u03c0(a|s) log \u03c0(a|s)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + c1 * value_loss - c2 * entropy\n",
    "            \n",
    "            # Update parameters\n",
    "            \u03b8 \u2190 \u03b8 - \u03b1 * \u2207loss\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Multiple epochs**: Reuses collected data for multiple gradient updates\n",
    "2. **Minibatch updates**: Uses minibatch SGD for efficiency\n",
    "3. **Entropy bonus**: Encourages exploration\n",
    "4. **GAE**: Often uses Generalized Advantage Estimation\n",
    "\n",
    "**Why PPO is Popular:**\n",
    "\n",
    "- **Simple**: Easy to implement compared to TRPO\n",
    "- **Stable**: Clipping prevents catastrophic updates\n",
    "- **Sample efficient**: Reuses data with multiple epochs\n",
    "- **General**: Works well across many domains\n",
    "- **Scalable**: Easy to parallelize\n",
    "\n",
    "**Typical Hyperparameters:**\n",
    "- Clip range \u03b5: 0.1 - 0.2\n",
    "- Number of epochs K: 3 - 10\n",
    "- Minibatch size: 32 - 512\n",
    "- GAE \u03bb: 0.95\n",
    "- Entropy coefficient: 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRPO: Trust Region Policy Optimization\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "TRPO (Trust Region Policy Optimization), introduced by Schulman et al. in 2015, was a breakthrough in stable policy optimization. It provides theoretical guarantees for monotonic improvement.\n",
    "\n",
    "**The Core Idea: Trust Regions**\n",
    "\n",
    "Instead of clipping like PPO, TRPO explicitly constrains the policy update to stay within a \"trust region\" where our approximations are valid.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "TRPO solves a constrained optimization problem:\n",
    "\n",
    "$\\max_\\theta \\mathbb{E}_{s,a \\sim \\pi_{\\theta_{old}}}\\left[\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)}\\hat{A}(s,a)\\right]$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$\\mathbb{E}_s\\left[D_{KL}(\\pi_{\\theta_{old}}(\\cdot|s) \\| \\pi_\\theta(\\cdot|s))\\right] \\leq \\delta$\n",
    "\n",
    "where:\n",
    "- $D_{KL}$ is the KL divergence between old and new policies\n",
    "- $\\delta$ is the maximum allowed divergence (trust region size)\n",
    "\n",
    "**Why KL Divergence?**\n",
    "\n",
    "KL divergence measures how different two probability distributions are:\n",
    "- $D_{KL} = 0$: Policies are identical\n",
    "- Small $D_{KL}$: Policies are similar\n",
    "- Large $D_{KL}$: Policies are very different\n",
    "\n",
    "By constraining KL divergence, we ensure the new policy doesn't deviate too far from the old one.\n",
    "\n",
    "**TRPO Algorithm (High-Level):**\n",
    "\n",
    "```\n",
    "for iteration = 1, 2, ...:\n",
    "    # Collect trajectories\n",
    "    trajectories = collect_trajectories(\u03c0_\u03b8)\n",
    "    \n",
    "    # Compute advantages\n",
    "    advantages = compute_advantages(trajectories)\n",
    "    \n",
    "    # Compute policy gradient g\n",
    "    g = \u2207_\u03b8 L(\u03b8) at \u03b8_old\n",
    "    \n",
    "    # Compute Fisher Information Matrix F (Hessian of KL)\n",
    "    F = \u2207\u00b2_\u03b8 D_KL(\u03c0_old || \u03c0_\u03b8) at \u03b8_old\n",
    "    \n",
    "    # Compute natural gradient direction\n",
    "    # Solve: F * x = g  (using conjugate gradient)\n",
    "    x = conjugate_gradient(F, g)\n",
    "    \n",
    "    # Compute step size using trust region constraint\n",
    "    step_size = sqrt(2\u03b4 / (x^T F x))\n",
    "    \n",
    "    # Update with line search to ensure improvement\n",
    "    \u03b8_new = \u03b8_old + step_size * x\n",
    "    \n",
    "    # Verify KL constraint and improvement\n",
    "    if D_KL(\u03c0_old || \u03c0_new) > \u03b4 or L(\u03b8_new) < L(\u03b8_old):\n",
    "        # Backtrack step size\n",
    "        \u03b8_new = line_search(\u03b8_old, x)\n",
    "```\n",
    "\n",
    "**Key Differences from PPO:**\n",
    "\n",
    "| Aspect | TRPO | PPO |\n",
    "|--------|------|-----|\n",
    "| Constraint | Hard KL constraint | Soft clipping |\n",
    "| Optimization | Constrained (conjugate gradient) | Unconstrained (SGD) |\n",
    "| Complexity | Higher | Lower |\n",
    "| Guarantees | Theoretical monotonic improvement | Empirical stability |\n",
    "| Implementation | Complex | Simple |\n",
    "\n",
    "**Advantages of TRPO:**\n",
    "- Theoretical guarantees for policy improvement\n",
    "- Very stable training\n",
    "- Works well in continuous control\n",
    "\n",
    "**Disadvantages:**\n",
    "- Complex implementation (conjugate gradient, line search)\n",
    "- Computationally expensive (Fisher matrix)\n",
    "- Harder to scale to large networks\n",
    "\n",
    "**When to Use TRPO vs PPO:**\n",
    "\n",
    "- **Use PPO**: Most cases - simpler, faster, works well\n",
    "- **Use TRPO**: When you need guaranteed stability, research applications, or when PPO fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary: Policy Optimization Methods\n",
    "\n",
    "**Evolution of Policy Gradient Methods:**\n",
    "\n",
    "```\n",
    "REINFORCE (1992)\n",
    "    \u2502\n",
    "    \u251c\u2500\u2500 High variance problem\n",
    "    \u2502\n",
    "    \u25bc\n",
    "Actor-Critic\n",
    "    \u2502\n",
    "    \u251c\u2500\u2500 Stability issues with large updates\n",
    "    \u2502\n",
    "    \u25bc\n",
    "TRPO (2015)\n",
    "    \u2502\n",
    "    \u251c\u2500\u2500 Complex implementation\n",
    "    \u2502\n",
    "    \u25bc\n",
    "PPO (2017) \u2190 Current standard\n",
    "    \u2502\n",
    "    \u25bc\n",
    "A3C/A2C (Parallel training)\n",
    "```\n",
    "\n",
    "**Comparison Table:**\n",
    "\n",
    "| Method | Variance | Stability | Complexity | Sample Efficiency |\n",
    "|--------|----------|-----------|------------|-------------------|\n",
    "| REINFORCE | High | Low | Low | Low |\n",
    "| Actor-Critic | Medium | Medium | Medium | Medium |\n",
    "| A3C | Medium | High | Medium | High |\n",
    "| TRPO | Low | Very High | High | High |\n",
    "| PPO | Low | High | Low | High |\n",
    "\n",
    "**Practical Recommendations:**\n",
    "\n",
    "1. **Start with PPO**: It's the best default choice for most problems\n",
    "2. **Use A2C/A3C**: When you have multiple CPUs and want parallelism\n",
    "3. **Consider TRPO**: For research or when you need theoretical guarantees\n",
    "4. **REINFORCE**: Only for simple problems or educational purposes\n",
    "\n",
    "**Modern Trends:**\n",
    "\n",
    "- **PPO** remains the workhorse of practical RL\n",
    "- **SAC (Soft Actor-Critic)**: Popular for continuous control\n",
    "- **TD3**: Another strong continuous control algorithm\n",
    "- **Distributed training**: Scaling up with many parallel workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Policy Gradient Implementation with Baseline\n",
    "\n",
    "Now let's implement a complete, production-ready policy gradient method that incorporates all the best practices we've discussed:\n",
    "\n",
    "1. **Neural network policy** with proper initialization\n",
    "2. **Learned baseline** (value function) for variance reduction\n",
    "3. **Advantage estimation** using the baseline\n",
    "4. **Entropy bonus** for exploration\n",
    "5. **Gradient clipping** for stability\n",
    "\n",
    "This implementation combines elements from REINFORCE, Actor-Critic, and PPO to create a robust policy gradient agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Policy Gradient Implementation with Baseline\n",
    "\n",
    "class PolicyGradientNetwork(nn.Module):\n",
    "    \"\"\"Combined policy and value network with shared features.\n",
    "    \n",
    "    Uses a shared feature extractor with separate heads for:\n",
    "    - Policy (action probabilities)\n",
    "    - Value (state value estimate for baseline)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PolicyGradientNetwork, self).__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Policy head (actor)\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Value head (critic/baseline)\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using orthogonal initialization.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass returning both policy and value.\"\"\"\n",
    "        features = self.shared(state)\n",
    "        policy = self.policy_head(features)\n",
    "        value = self.value_head(features)\n",
    "        return policy, value\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample action and return action, log_prob, value, entropy.\"\"\"\n",
    "        policy, value = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action.item(), log_prob, value.squeeze(), entropy\n",
    "\n",
    "\n",
    "class PolicyGradientAgent:\n",
    "    \"\"\"Complete Policy Gradient agent with baseline and entropy bonus.\n",
    "    \n",
    "    Features:\n",
    "    - Learned value function baseline for variance reduction\n",
    "    - Entropy bonus for exploration\n",
    "    - Gradient clipping for stability\n",
    "    - Advantage normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, \n",
    "                 entropy_coef=0.01, value_coef=0.5, max_grad_norm=0.5):\n",
    "        \"\"\"Initialize the Policy Gradient agent.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Number of discrete actions\n",
    "            lr: Learning rate\n",
    "            gamma: Discount factor\n",
    "            entropy_coef: Coefficient for entropy bonus\n",
    "            value_coef: Coefficient for value loss\n",
    "            max_grad_norm: Maximum gradient norm for clipping\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        # Combined network\n",
    "        self.network = PolicyGradientNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # Episode storage\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.entropies = []\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using current policy.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, log_prob, value, entropy = self.network.get_action(state_tensor)\n",
    "        \n",
    "        # Store for later update\n",
    "        self.states.append(state)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        self.entropies.append(entropy)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward for current step.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def compute_returns_and_advantages(self):\n",
    "        \"\"\"Compute discounted returns and advantages.\n",
    "        \n",
    "        Returns:\n",
    "            returns: Discounted cumulative rewards\n",
    "            advantages: Returns minus baseline (value estimates)\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        \n",
    "        # Compute returns (backwards)\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        values = torch.stack(self.values)\n",
    "        \n",
    "        # Compute advantages (returns - baseline)\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        # Normalize advantages for stability\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        return returns, advantages\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy using collected episode data.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with loss components\n",
    "        \"\"\"\n",
    "        returns, advantages = self.compute_returns_and_advantages()\n",
    "        \n",
    "        # Stack tensors\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        values = torch.stack(self.values)\n",
    "        entropies = torch.stack(self.entropies)\n",
    "        \n",
    "        # Policy loss (negative because we want to maximize)\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        # Value loss (MSE between predicted and actual returns)\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        \n",
    "        # Entropy bonus (negative because we want to maximize entropy)\n",
    "        entropy_loss = -entropies.mean()\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (policy_loss + \n",
    "                      self.value_coef * value_loss + \n",
    "                      self.entropy_coef * entropy_loss)\n",
    "        \n",
    "        # Backpropagate\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.entropies = []\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': -entropy_loss.item()\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Complete Policy Gradient Agent implemented!\")\n",
    "print(\"Features:\")\n",
    "print(\"  \u2713 Shared feature network with policy and value heads\")\n",
    "print(\"  \u2713 Learned baseline (value function) for variance reduction\")\n",
    "print(\"  \u2713 Entropy bonus for exploration\")\n",
    "print(\"  \u2713 Gradient clipping for stability\")\n",
    "print(\"  \u2713 Advantage normalization\")\n",
    "print(\"  \u2713 Orthogonal weight initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_gradient(env_name='CartPole-v1', num_episodes=1000, lr=0.001, \n",
    "                          gamma=0.99, entropy_coef=0.01, print_every=100, seed=42):\n",
    "    \"\"\"Train the complete Policy Gradient agent.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Name of the Gym environment\n",
    "        num_episodes: Number of episodes to train\n",
    "        lr: Learning rate\n",
    "        gamma: Discount factor\n",
    "        entropy_coef: Entropy bonus coefficient\n",
    "        print_every: Print progress every N episodes\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        agent: Trained agent\n",
    "        results: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    # Set seeds\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # Create agent\n",
    "    agent = PolicyGradientAgent(\n",
    "        state_dim, action_dim, \n",
    "        lr=lr, gamma=gamma, entropy_coef=entropy_coef\n",
    "    )\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "    \n",
    "    print(f\"Training Policy Gradient with Baseline on {env_name}\")\n",
    "    print(f\"State dim: {state_dim}, Action dim: {action_dim}\")\n",
    "    print(f\"Entropy coefficient: {entropy_coef}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset(seed=seed + episode)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        # Collect episode\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_reward(reward)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        # Update policy\n",
    "        losses = agent.update()\n",
    "        \n",
    "        # Store metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        policy_losses.append(losses['policy_loss'])\n",
    "        value_losses.append(losses['value_loss'])\n",
    "        entropies.append(losses['entropy'])\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            avg_entropy = np.mean(entropies[-print_every:])\n",
    "            print(f\"Episode {episode + 1:4d} | \"\n",
    "                  f\"Avg Reward: {avg_reward:7.2f} | \"\n",
    "                  f\"Entropy: {avg_entropy:.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    results = {\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'policy_losses': policy_losses,\n",
    "        'value_losses': value_losses,\n",
    "        'entropies': entropies\n",
    "    }\n",
    "    \n",
    "    return agent, results\n",
    "\n",
    "\n",
    "# Train the complete policy gradient agent\n",
    "print(\"Starting Policy Gradient with Baseline training...\")\n",
    "pg_agent, pg_results = train_policy_gradient(\n",
    "    env_name='CartPole-v1',\n",
    "    num_episodes=1000,\n",
    "    lr=0.001,\n",
    "    gamma=0.99,\n",
    "    entropy_coef=0.01,\n",
    "    print_every=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Final average reward (last 100 episodes): {np.mean(pg_results['rewards'][-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization of Policy Gradient training\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "window = 50\n",
    "\n",
    "# Plot 1: Learning curve\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(pg_results['rewards'], alpha=0.3, color='green')\n",
    "if len(pg_results['rewards']) >= window:\n",
    "    ma = np.convolve(pg_results['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(pg_results['rewards'])), ma, \n",
    "             color='darkgreen', linewidth=2.5, label=f'{window}-Episode MA')\n",
    "ax1.axhline(y=195, color='red', linestyle='--', linewidth=2, label='Solved Threshold')\n",
    "ax1.set_xlabel('Episode', fontsize=11)\n",
    "ax1.set_ylabel('Total Reward', fontsize=11)\n",
    "ax1.set_title('Policy Gradient: Learning Curve', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Policy loss\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(pg_results['policy_losses'], alpha=0.3, color='blue')\n",
    "if len(pg_results['policy_losses']) >= window:\n",
    "    ma = np.convolve(pg_results['policy_losses'], np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(range(window-1, len(pg_results['policy_losses'])), ma, \n",
    "             color='darkblue', linewidth=2.5)\n",
    "ax2.set_xlabel('Episode', fontsize=11)\n",
    "ax2.set_ylabel('Policy Loss', fontsize=11)\n",
    "ax2.set_title('Policy Loss Over Time', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Value loss\n",
    "ax3 = axes[0, 2]\n",
    "ax3.plot(pg_results['value_losses'], alpha=0.3, color='orange')\n",
    "if len(pg_results['value_losses']) >= window:\n",
    "    ma = np.convolve(pg_results['value_losses'], np.ones(window)/window, mode='valid')\n",
    "    ax3.plot(range(window-1, len(pg_results['value_losses'])), ma, \n",
    "             color='darkorange', linewidth=2.5)\n",
    "ax3.set_xlabel('Episode', fontsize=11)\n",
    "ax3.set_ylabel('Value Loss', fontsize=11)\n",
    "ax3.set_title('Value (Baseline) Loss Over Time', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Entropy\n",
    "ax4 = axes[1, 0]\n",
    "ax4.plot(pg_results['entropies'], alpha=0.3, color='purple')\n",
    "if len(pg_results['entropies']) >= window:\n",
    "    ma = np.convolve(pg_results['entropies'], np.ones(window)/window, mode='valid')\n",
    "    ax4.plot(range(window-1, len(pg_results['entropies'])), ma, \n",
    "             color='darkviolet', linewidth=2.5)\n",
    "ax4.set_xlabel('Episode', fontsize=11)\n",
    "ax4.set_ylabel('Entropy', fontsize=11)\n",
    "ax4.set_title('Policy Entropy Over Time', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Compare all methods\n",
    "ax5 = axes[1, 1]\n",
    "if len(reinforce_results['rewards']) >= window:\n",
    "    reinforce_ma = np.convolve(reinforce_results['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax5.plot(range(window-1, len(reinforce_results['rewards'])), reinforce_ma, \n",
    "             color='blue', linewidth=2, label='REINFORCE')\n",
    "if len(ac_results['rewards']) >= window:\n",
    "    ac_ma = np.convolve(ac_results['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax5.plot(range(window-1, len(ac_results['rewards'])), ac_ma, \n",
    "             color='red', linewidth=2, label='Actor-Critic')\n",
    "if len(pg_results['rewards']) >= window:\n",
    "    pg_ma = np.convolve(pg_results['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax5.plot(range(window-1, len(pg_results['rewards'])), pg_ma, \n",
    "             color='green', linewidth=2, label='PG + Baseline')\n",
    "ax5.axhline(y=195, color='black', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "ax5.set_xlabel('Episode', fontsize=11)\n",
    "ax5.set_ylabel('Total Reward', fontsize=11)\n",
    "ax5.set_title('All Methods Comparison', fontsize=12, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final performance comparison\n",
    "ax6 = axes[1, 2]\n",
    "methods = ['REINFORCE', 'Actor-Critic', 'PG + Baseline']\n",
    "final_rewards = [\n",
    "    np.mean(reinforce_results['rewards'][-100:]),\n",
    "    np.mean(ac_results['rewards'][-100:]),\n",
    "    np.mean(pg_results['rewards'][-100:])\n",
    "]\n",
    "colors = ['blue', 'red', 'green']\n",
    "bars = ax6.bar(methods, final_rewards, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax6.axhline(y=195, color='black', linestyle='--', linewidth=2, label='Solved Threshold')\n",
    "ax6.set_ylabel('Average Reward (Last 100 Episodes)', fontsize=11)\n",
    "ax6.set_title('Final Performance Comparison', fontsize=12, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, reward in zip(bars, final_rewards):\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "             f'{reward:.1f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\"\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE COMPARISON: Policy Optimization Methods\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Method':<20} {'Mean Reward':>15} {'Std':>10} {'Final 100 Avg':>15} {'Variance':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "results_dict = {\n",
    "    'REINFORCE': reinforce_results['rewards'],\n",
    "    'Actor-Critic': ac_results['rewards'],\n",
    "    'PG + Baseline': pg_results['rewards']\n",
    "}\n",
    "\n",
    "for name, rewards in results_dict.items():\n",
    "    print(f\"{name:<20} {np.mean(rewards):>15.2f} {np.std(rewards):>10.2f} \"\n",
    "          f\"{np.mean(rewards[-100:]):>15.2f} {np.var(rewards):>12.2f}\")\n",
    "\n",
    "print(\"\" + \"=\" * 80)\n",
    "print(\"\ud83d\udcca Key Insights:\")\n",
    "print(\"   - REINFORCE: Simple but high variance\")\n",
    "print(\"   - Actor-Critic: Lower variance through TD learning\")\n",
    "print(\"   - PG + Baseline: Best of both worlds with learned baseline\")\n",
    "print(\"   The baseline (value function) significantly reduces variance\")\n",
    "print(\"   while maintaining the benefits of policy gradient methods.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Policy Optimization Methods\n",
    "\n",
    "In this section, we've covered the major policy optimization methods in reinforcement learning:\n",
    "\n",
    "**What We Implemented:**\n",
    "\n",
    "1. **REINFORCE**: The foundational policy gradient algorithm\n",
    "   - Simple Monte Carlo policy gradient\n",
    "   - High variance but unbiased\n",
    "   - Foundation for all policy gradient methods\n",
    "\n",
    "2. **Actor-Critic**: Combining policy gradients with value estimation\n",
    "   - Actor (policy) + Critic (value function)\n",
    "   - Lower variance through TD learning\n",
    "   - Online updates (every step)\n",
    "\n",
    "3. **Complete Policy Gradient with Baseline**: Production-ready implementation\n",
    "   - Shared network architecture\n",
    "   - Learned baseline for variance reduction\n",
    "   - Entropy bonus for exploration\n",
    "   - Gradient clipping for stability\n",
    "\n",
    "**What We Explained:**\n",
    "\n",
    "4. **A3C**: Asynchronous parallel training\n",
    "5. **PPO**: Clipped surrogate objective for stable updates\n",
    "6. **TRPO**: Trust region constraints with theoretical guarantees\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- Policy gradients directly optimize the policy, enabling continuous actions and stochastic policies\n",
    "- Variance reduction is crucial: baselines, advantage normalization, and entropy bonuses all help\n",
    "- Modern methods (PPO, A3C) combine multiple techniques for robust performance\n",
    "- PPO is the current go-to algorithm for most practical applications\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "In the following sections, we'll explore advanced topics like reward engineering, scaling to complex environments, and real-world applications of these algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## Section 3: Advanced Topics\n",
    "\n",
    "In this section, we explore sophisticated concepts that build upon the foundational algorithms covered earlier. These advanced topics are essential for tackling real-world RL challenges and understanding cutting-edge research directions.\n",
    "\n",
    "We'll cover:\n",
    "- **Reward Engineering**: Designing effective reward functions and understanding their impact\n",
    "- **Scaling and Generalization**: Handling high-dimensional spaces and transferring knowledge\n",
    "- **Advanced Policy Methods**: Eligibility traces and trust region optimization\n",
    "- **Specialized RL Techniques**: Hierarchical RL, inverse RL, and partial observability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reward-engineering'></a>\n",
    "### Reward Engineering\n",
    "\n",
    "**The Art and Science of Designing Reward Functions**\n",
    "\n",
    "Reward engineering is one of the most critical yet challenging aspects of reinforcement learning. The reward function defines what we want the agent to achieve, and even small changes can dramatically alter agent behavior.\n",
    "\n",
    "#### What is Reward Shaping?\n",
    "\n",
    "**Reward shaping** is the practice of modifying the reward function to guide the agent toward desired behavior more efficiently. Instead of only providing sparse rewards (e.g., +1 for winning, 0 otherwise), we add intermediate rewards that provide more frequent feedback.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Given an original reward function $R(s, a, s')$, a shaped reward function is:\n",
    "\n",
    "$$R'(s, a, s') = R(s, a, s') + F(s, a, s')$$\n",
    "\n",
    "where $F(s, a, s')$ is the shaping function.\n",
    "\n",
    "**Potential-Based Reward Shaping:**\n",
    "\n",
    "To guarantee that the optimal policy remains unchanged, we use potential-based shaping:\n",
    "\n",
    "$$F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)$$\n",
    "\n",
    "where $\\Phi(s)$ is a potential function and $\\gamma$ is the discount factor.\n",
    "\n",
    "#### Effects of Reward Shaping\n",
    "\n",
    "**Positive Effects:**\n",
    "- Faster learning by providing more frequent feedback\n",
    "- Guides exploration toward promising regions\n",
    "- Can make sparse reward problems tractable\n",
    "\n",
    "**Negative Effects (if done incorrectly):**\n",
    "- Can change the optimal policy (reward hacking)\n",
    "- May introduce local optima\n",
    "- Can lead to unintended behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Challenges with Reward Functions\n",
    "\n",
    "**1. Sparse Rewards**\n",
    "\n",
    "Many real-world problems have sparse rewards where feedback is only given at the end of an episode or upon achieving a goal.\n",
    "\n",
    "- *Problem*: Agent receives no learning signal for most actions\n",
    "- *Example*: Chess - only get reward at game end\n",
    "- *Solutions*: Reward shaping, curiosity-driven exploration, hindsight experience replay\n",
    "\n",
    "**2. Reward Hacking**\n",
    "\n",
    "Agents may find unexpected ways to maximize reward that don't align with the designer's intent.\n",
    "\n",
    "- *Problem*: Agent exploits loopholes in reward function\n",
    "- *Example*: A cleaning robot that hides mess instead of cleaning it\n",
    "- *Solutions*: Careful reward design, multiple objectives, human oversight\n",
    "\n",
    "**3. Credit Assignment Problem**\n",
    "\n",
    "Determining which actions were responsible for a delayed reward.\n",
    "\n",
    "- *Problem*: Hard to know which past actions led to current reward\n",
    "- *Example*: In a game, which move 20 turns ago led to winning?\n",
    "- *Solutions*: Eligibility traces, attention mechanisms, temporal abstraction\n",
    "\n",
    "**4. Reward Scaling**\n",
    "\n",
    "The magnitude of rewards affects learning stability and speed.\n",
    "\n",
    "- *Problem*: Very large or small rewards can destabilize learning\n",
    "- *Solutions*: Reward normalization, reward clipping\n",
    "\n",
    "**5. Multi-Objective Trade-offs**\n",
    "\n",
    "Real problems often have multiple, potentially conflicting objectives.\n",
    "\n",
    "- *Problem*: How to balance safety vs. efficiency vs. other goals?\n",
    "- *Solutions*: Weighted sum, constrained optimization, Pareto optimization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Demonstrating the impact of different reward functions on agent behavior\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GridWorldWithRewardShaping:\n",
    "    \"\"\"A simple grid world to demonstrate reward shaping effects.\"\"\"\n",
    "    \n",
    "    def __init__(self, size=5, goal=(4, 4)):\n",
    "        self.size = size\n",
    "        self.goal = goal\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action, reward_type='sparse'):\n",
    "        \"\"\"Take action and return next_state, reward, done.\n",
    "        \n",
    "        Actions: 0=up, 1=right, 2=down, 3=left\n",
    "        reward_type: 'sparse', 'dense', or 'shaped'\n",
    "        \"\"\"\n",
    "        x, y = self.state\n",
    "        \n",
    "        # Apply action\n",
    "        if action == 0 and y < self.size - 1:  # up\n",
    "            y += 1\n",
    "        elif action == 1 and x < self.size - 1:  # right\n",
    "            x += 1\n",
    "        elif action == 2 and y > 0:  # down\n",
    "            y -= 1\n",
    "        elif action == 3 and x > 0:  # left\n",
    "            x -= 1\n",
    "        \n",
    "        old_state = self.state\n",
    "        self.state = (x, y)\n",
    "        done = self.state == self.goal\n",
    "        \n",
    "        # Calculate reward based on type\n",
    "        if reward_type == 'sparse':\n",
    "            reward = 10.0 if done else 0.0\n",
    "        elif reward_type == 'dense':\n",
    "            # Negative distance to goal\n",
    "            dist = abs(x - self.goal[0]) + abs(y - self.goal[1])\n",
    "            reward = 10.0 if done else -dist * 0.1\n",
    "        elif reward_type == 'shaped':\n",
    "            # Potential-based shaping\n",
    "            old_dist = abs(old_state[0] - self.goal[0]) + abs(old_state[1] - self.goal[1])\n",
    "            new_dist = abs(x - self.goal[0]) + abs(y - self.goal[1])\n",
    "            shaping = (old_dist - new_dist)  # Reward for getting closer\n",
    "            reward = (10.0 if done else 0.0) + shaping\n",
    "        \n",
    "        return self.state, reward, done\n",
    "\n",
    "\n",
    "class SimpleQLearner:\n",
    "    \"\"\"Simple Q-learning agent for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, n_actions, lr=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.q_table = np.zeros((state_size, state_size, n_actions))\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        return np.argmax(self.q_table[state[0], state[1]])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        current_q = self.q_table[state[0], state[1], action]\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state[0], next_state[1]])\n",
    "        self.q_table[state[0], state[1], action] += self.lr * (target - current_q)\n",
    "\n",
    "\n",
    "def train_agent(reward_type, episodes=500, max_steps=50):\n",
    "    \"\"\"Train an agent with a specific reward type.\"\"\"\n",
    "    env = GridWorldWithRewardShaping(size=5)\n",
    "    agent = SimpleQLearner(state_size=5, n_actions=4)\n",
    "    \n",
    "    episode_lengths = []\n",
    "    cumulative_rewards = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action, reward_type)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_lengths.append(step + 1)\n",
    "        cumulative_rewards.append(total_reward)\n",
    "    \n",
    "    return episode_lengths, cumulative_rewards\n",
    "\n",
    "\n",
    "# Train agents with different reward types\n",
    "print(\"Training agents with different reward functions...\")\n",
    "print(\"This demonstrates how reward shaping affects learning speed.\")\n",
    "\n",
    "np.random.seed(42)\n",
    "results = {}\n",
    "for reward_type in ['sparse', 'dense', 'shaped']:\n",
    "    lengths, rewards = train_agent(reward_type)\n",
    "    results[reward_type] = {'lengths': lengths, 'rewards': rewards}\n",
    "    print(f\"{reward_type.capitalize():8s} rewards: Avg episode length (last 50): {np.mean(lengths[-50:]):.1f} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the impact of different reward functions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = {'sparse': 'red', 'dense': 'blue', 'shaped': 'green'}\n",
    "window = 20  # Smoothing window\n",
    "\n",
    "# Plot episode lengths\n",
    "ax1 = axes[0]\n",
    "for reward_type, data in results.items():\n",
    "    lengths = np.array(data['lengths'])\n",
    "    smoothed = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(smoothed, label=f'{reward_type.capitalize()}', color=colors[reward_type], linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Steps to Goal (smoothed)', fontsize=12)\n",
    "ax1.set_title('Learning Speed with Different Reward Functions', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot final performance comparison\n",
    "ax2 = axes[1]\n",
    "final_lengths = [np.mean(results[rt]['lengths'][-50:]) for rt in ['sparse', 'dense', 'shaped']]\n",
    "bars = ax2.bar(['Sparse', 'Dense', 'Shaped'], final_lengths, color=[colors[rt] for rt in ['sparse', 'dense', 'shaped']])\n",
    "ax2.set_ylabel('Avg Steps to Goal (last 50 episodes)', fontsize=12)\n",
    "ax2.set_title('Final Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, final_lengths):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{val:.1f}', \n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Key Observations:\")\n",
    "print(\"   - Sparse rewards lead to slower initial learning (agent wanders randomly)\")\n",
    "print(\"   - Dense rewards provide continuous feedback, speeding up learning\")\n",
    "print(\"   - Shaped rewards (potential-based) guide the agent efficiently\")\n",
    "print(\"   - All eventually converge, but shaped rewards learn fastest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scaling'></a>\n",
    "### Scaling and Generalization\n",
    "\n",
    "Real-world RL problems often involve high-dimensional state spaces, require knowledge transfer between tasks, and must generalize to unseen situations. This section covers strategies for handling these challenges.\n",
    "\n",
    "#### Strategies for High-Dimensional State Spaces\n",
    "\n",
    "**The Curse of Dimensionality**\n",
    "\n",
    "As state space dimensions increase, the number of possible states grows exponentially. A 10x10 grid has 100 states, but a 100-dimensional continuous space is essentially infinite.\n",
    "\n",
    "**Key Strategies:**\n",
    "\n",
    "**1. Function Approximation**\n",
    "\n",
    "Instead of storing values for every state, approximate the value function:\n",
    "\n",
    "$$V(s) \\approx \\hat{V}(s; \\theta)$$\n",
    "\n",
    "where $\\theta$ are learnable parameters (e.g., neural network weights).\n",
    "\n",
    "- *Linear Function Approximation*: $\\hat{V}(s) = \\theta^T \\phi(s)$ where $\\phi(s)$ are features\n",
    "- *Neural Network Approximation*: Deep networks can learn complex value functions\n",
    "\n",
    "**2. State Abstraction**\n",
    "\n",
    "Group similar states together to reduce effective state space:\n",
    "- Tile coding: Discretize continuous spaces with overlapping tiles\n",
    "- State aggregation: Cluster states with similar values\n",
    "- Feature extraction: Use domain knowledge to extract relevant features\n",
    "\n",
    "**3. Dimensionality Reduction**\n",
    "\n",
    "- PCA/Autoencoders: Learn compressed state representations\n",
    "- Attention mechanisms: Focus on relevant parts of the state\n",
    "- Factored representations: Exploit structure in the state space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learning Strategies\n",
    "\n",
    "Transfer learning allows agents to leverage knowledge from one task to accelerate learning on related tasks.\n",
    "\n",
    "**Types of Transfer:**\n",
    "\n",
    "**1. Policy Transfer**\n",
    "- Use a policy learned in one environment as initialization for another\n",
    "- Works well when tasks share similar dynamics\n",
    "\n",
    "**2. Value Function Transfer**\n",
    "- Transfer learned value functions between similar tasks\n",
    "- Can provide good initial estimates for new tasks\n",
    "\n",
    "**3. Representation Transfer**\n",
    "- Transfer learned state representations (e.g., neural network features)\n",
    "- Most robust form of transfer\n",
    "\n",
    "**4. Model Transfer**\n",
    "- Transfer learned dynamics models between environments\n",
    "- Useful when physics are similar but goals differ\n",
    "\n",
    "#### Generalization to Unseen Environments\n",
    "\n",
    "A key challenge is ensuring agents perform well in environments they haven't seen during training.\n",
    "\n",
    "**Approaches:**\n",
    "\n",
    "**1. Domain Randomization**\n",
    "- Train on many variations of the environment\n",
    "- Randomize visual appearance, physics parameters, etc.\n",
    "- Agent learns robust policies that work across variations\n",
    "\n",
    "**2. Meta-Learning**\n",
    "- Learn to learn: train on distribution of tasks\n",
    "- Agent learns to quickly adapt to new tasks\n",
    "- Examples: MAML, RL\u00b2\n",
    "\n",
    "**3. Sim-to-Real Transfer**\n",
    "- Train in simulation, deploy in real world\n",
    "- Key challenge: bridging the \"reality gap\"\n",
    "- Solutions: domain adaptation, system identification\n",
    "\n",
    "#### Overfitting Issues and Mitigation\n",
    "\n",
    "**What is Overfitting in RL?**\n",
    "\n",
    "An agent that memorizes specific trajectories rather than learning generalizable policies.\n",
    "\n",
    "**Signs of Overfitting:**\n",
    "- Excellent performance on training environments\n",
    "- Poor performance on slightly different environments\n",
    "- Brittle policies that fail with small perturbations\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "\n",
    "1. **Regularization**: L2 regularization, dropout in neural networks\n",
    "2. **Data Augmentation**: Augment observations (noise, transformations)\n",
    "3. **Ensemble Methods**: Train multiple agents, combine predictions\n",
    "4. **Early Stopping**: Monitor validation performance\n",
    "5. **Procedural Generation**: Train on procedurally generated environments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Demonstrating function approximation for high-dimensional spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LinearFunctionApproximator:\n",
    "    \"\"\"Linear function approximation for value estimation.\n",
    "    \n",
    "    Uses tile coding features for continuous state spaces.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, lr=0.01):\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def get_features(self, state):\n",
    "        \"\"\"Convert state to feature vector using polynomial features.\"\"\"\n",
    "        x = state[0]\n",
    "        # Polynomial features: [1, x, x^2, x^3, ...]\n",
    "        return np.array([x**i for i in range(len(self.weights))])\n",
    "    \n",
    "    def predict(self, state):\n",
    "        \"\"\"Predict value for a state.\"\"\"\n",
    "        features = self.get_features(state)\n",
    "        return np.dot(self.weights, features)\n",
    "    \n",
    "    def update(self, state, target):\n",
    "        \"\"\"Update weights using gradient descent.\"\"\"\n",
    "        features = self.get_features(state)\n",
    "        prediction = np.dot(self.weights, features)\n",
    "        error = target - prediction\n",
    "        self.weights += self.lr * error * features\n",
    "\n",
    "\n",
    "# Demonstrate learning a value function\n",
    "print(\"Demonstrating Linear Function Approximation\")\n",
    "print(\"=\"*50)\n",
    "print(\"Learning to approximate V(s) = sin(s) + 0.5*s\")\n",
    "\n",
    "# True value function we want to approximate\n",
    "def true_value(s):\n",
    "    return np.sin(s * 2) + 0.5 * s\n",
    "\n",
    "# Create approximator with polynomial features\n",
    "approximator = LinearFunctionApproximator(n_features=8, lr=0.001)\n",
    "\n",
    "# Training\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "states = np.random.uniform(-2, 2, n_samples)\n",
    "\n",
    "errors = []\n",
    "for i, s in enumerate(states):\n",
    "    target = true_value(s)\n",
    "    pred = approximator.predict([s])\n",
    "    errors.append((target - pred)**2)\n",
    "    approximator.update([s], target)\n",
    "\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Initial MSE: {np.mean(errors[:50]):.4f}\")\n",
    "print(f\"Final MSE: {np.mean(errors[-50:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the approximation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: True vs Approximated function\n",
    "ax1 = axes[0]\n",
    "x_test = np.linspace(-2, 2, 100)\n",
    "y_true = [true_value(x) for x in x_test]\n",
    "y_approx = [approximator.predict([x]) for x in x_test]\n",
    "\n",
    "ax1.plot(x_test, y_true, 'b-', linewidth=2, label='True V(s)')\n",
    "ax1.plot(x_test, y_approx, 'r--', linewidth=2, label='Approximated V(s)')\n",
    "ax1.set_xlabel('State (s)', fontsize=12)\n",
    "ax1.set_ylabel('Value V(s)', fontsize=12)\n",
    "ax1.set_title('Function Approximation Result', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Learning curve\n",
    "ax2 = axes[1]\n",
    "window = 50\n",
    "smoothed_errors = np.convolve(errors, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(smoothed_errors, 'g-', linewidth=2)\n",
    "ax2.set_xlabel('Training Sample', fontsize=12)\n",
    "ax2.set_ylabel('MSE (smoothed)', fontsize=12)\n",
    "ax2.set_title('Learning Curve', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Key Points:\")\n",
    "print(\"   - Function approximation allows handling continuous/large state spaces\")\n",
    "print(\"   - Linear approximation with good features can work well\")\n",
    "print(\"   - Neural networks can learn features automatically (deep RL)\")\n",
    "print(\"   - Trade-off: approximation error vs. generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='advanced-policy'></a>\n",
    "### Advanced Policy Methods\n",
    "\n",
    "This section covers sophisticated techniques for policy optimization, including eligibility traces and trust region methods.\n",
    "\n",
    "#### Eligibility Traces\n",
    "\n",
    "**What are Eligibility Traces?**\n",
    "\n",
    "Eligibility traces provide a mechanism to bridge the gap between Monte Carlo methods (which use complete returns) and TD methods (which bootstrap from single steps). They allow credit to be assigned to states visited in the recent past.\n",
    "\n",
    "**The Idea:**\n",
    "\n",
    "When we receive a reward, we want to update not just the immediately preceding state, but also states visited earlier that may have contributed to that reward. Eligibility traces keep track of which states are \"eligible\" for updates.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "For each state $s$, we maintain an eligibility trace $e_t(s)$:\n",
    "\n",
    "$$e_t(s) = \\begin{cases}\n",
    "\\gamma \\lambda e_{t-1}(s) + 1 & \\text{if } s = S_t \\\\\n",
    "\\gamma \\lambda e_{t-1}(s) & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "where:\n",
    "- $\\gamma$ is the discount factor\n",
    "- $\\lambda \\in [0, 1]$ is the trace decay parameter\n",
    "\n",
    "**TD(\u03bb) Update Rule:**\n",
    "\n",
    "$$V(s) \\leftarrow V(s) + \\alpha \\delta_t e_t(s)$$\n",
    "\n",
    "where $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$ is the TD error.\n",
    "\n",
    "**Special Cases:**\n",
    "\n",
    "- $\\lambda = 0$: TD(0) - only update the current state\n",
    "- $\\lambda = 1$: Equivalent to Monte Carlo (in episodic tasks)\n",
    "- $0 < \\lambda < 1$: Blend of TD and MC\n",
    "\n",
    "**Benefits of Eligibility Traces:**\n",
    "\n",
    "1. Faster credit assignment to earlier states\n",
    "2. More efficient learning in many problems\n",
    "3. Smooth interpolation between TD and MC\n",
    "4. Can be combined with function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trust Region Policy Optimization (TRPO)\n",
    "\n",
    "**The Problem with Standard Policy Gradients**\n",
    "\n",
    "Standard policy gradient methods can be unstable because:\n",
    "- Large policy updates can drastically change behavior\n",
    "- Bad updates can be catastrophic and hard to recover from\n",
    "- Step size selection is difficult\n",
    "\n",
    "**TRPO's Solution: Constrained Optimization**\n",
    "\n",
    "TRPO ensures stable learning by constraining how much the policy can change in each update.\n",
    "\n",
    "**The Objective:**\n",
    "\n",
    "Maximize the expected improvement while staying close to the old policy:\n",
    "\n",
    "$$\\max_{\\theta} \\mathbb{E}_{s \\sim \\rho_{\\theta_{old}}, a \\sim \\pi_{\\theta_{old}}} \\left[ \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)} A^{\\pi_{\\theta_{old}}}(s, a) \\right]$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\\mathbb{E}_{s \\sim \\rho_{\\theta_{old}}} \\left[ D_{KL}(\\pi_{\\theta_{old}}(\\cdot|s) \\| \\pi_\\theta(\\cdot|s)) \\right] \\leq \\delta$$\n",
    "\n",
    "where:\n",
    "- $\\pi_\\theta$ is the new policy\n",
    "- $\\pi_{\\theta_{old}}$ is the old policy\n",
    "- $A^{\\pi}(s, a)$ is the advantage function\n",
    "- $D_{KL}$ is the KL divergence\n",
    "- $\\delta$ is the trust region size\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "**1. Surrogate Objective**\n",
    "\n",
    "The ratio $\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)}$ measures how much more/less likely an action is under the new policy.\n",
    "\n",
    "**2. KL Divergence Constraint**\n",
    "\n",
    "Ensures the new policy doesn't deviate too far from the old one:\n",
    "\n",
    "$$D_{KL}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "**3. Natural Gradient**\n",
    "\n",
    "TRPO uses the natural gradient, which accounts for the geometry of the policy space:\n",
    "\n",
    "$$\\theta_{new} = \\theta_{old} + \\sqrt{\\frac{2\\delta}{g^T F^{-1} g}} F^{-1} g$$\n",
    "\n",
    "where $F$ is the Fisher information matrix and $g$ is the policy gradient.\n",
    "\n",
    "**How TRPO Differs from Other Methods:**\n",
    "\n",
    "| Method | Update Strategy | Stability |\n",
    "|--------|----------------|----------|\n",
    "| Vanilla PG | Fixed step size | Low |\n",
    "| TRPO | KL constraint | High |\n",
    "| PPO | Clipped objective | High |\n",
    "\n",
    "**Advantages of TRPO:**\n",
    "- Guaranteed monotonic improvement (in theory)\n",
    "- More stable than vanilla policy gradients\n",
    "- Works well with neural network policies\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally expensive (requires computing Fisher matrix)\n",
    "- Complex implementation\n",
    "- PPO often achieves similar results with simpler implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='specialized'></a>\n",
    "### Specialized RL Techniques\n",
    "\n",
    "This section covers advanced RL paradigms designed for specific challenging scenarios.\n",
    "\n",
    "#### Hierarchical Reinforcement Learning\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "Many real-world tasks have natural hierarchical structure. For example, making coffee involves:\n",
    "- High-level: Get cup \u2192 Fill with water \u2192 Add coffee \u2192 Heat\n",
    "- Low-level: Each step involves many primitive actions\n",
    "\n",
    "Flat RL struggles with such tasks due to:\n",
    "- Long time horizons\n",
    "- Sparse rewards\n",
    "- Credit assignment over many steps\n",
    "\n",
    "**Hierarchical RL Solution:**\n",
    "\n",
    "Decompose the problem into multiple levels of abstraction:\n",
    "\n",
    "```\n",
    "High-Level Policy (Manager)\n",
    "    \u2193 selects subgoals\n",
    "Mid-Level Policy (Sub-manager)\n",
    "    \u2193 selects options\n",
    "Low-Level Policy (Worker)\n",
    "    \u2193 executes primitive actions\n",
    "Environment\n",
    "```\n",
    "\n",
    "**Key Frameworks:**\n",
    "\n",
    "**1. Options Framework**\n",
    "\n",
    "An option $\\omega$ consists of:\n",
    "- Initiation set $I_\\omega$: states where option can start\n",
    "- Policy $\\pi_\\omega$: how to behave while option is active\n",
    "- Termination condition $\\beta_\\omega$: when to stop\n",
    "\n",
    "**2. Feudal Networks (FuN)**\n",
    "\n",
    "- Manager sets goals in a learned goal space\n",
    "- Worker tries to achieve goals\n",
    "- Manager operates at lower temporal resolution\n",
    "\n",
    "**3. MAXQ Decomposition**\n",
    "\n",
    "- Decomposes value function hierarchically\n",
    "- Each subtask has its own value function\n",
    "\n",
    "**Benefits:**\n",
    "- Temporal abstraction: reason over longer time scales\n",
    "- Transfer: reuse low-level skills across tasks\n",
    "- Exploration: structured exploration at multiple levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse Reinforcement Learning\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "In standard RL, we're given a reward function and learn a policy. But what if:\n",
    "- The reward function is hard to specify?\n",
    "- We have expert demonstrations but no reward signal?\n",
    "\n",
    "**Inverse RL (IRL) flips the problem:**\n",
    "\n",
    "Given expert demonstrations, learn the reward function that the expert is optimizing.\n",
    "\n",
    "**Formal Definition:**\n",
    "\n",
    "Given:\n",
    "- MDP without reward: $(S, A, P, \\gamma)$\n",
    "- Expert demonstrations: $\\{\\tau_1, \\tau_2, ..., \\tau_n\\}$\n",
    "\n",
    "Find:\n",
    "- Reward function $R(s, a)$ such that expert policy is optimal\n",
    "\n",
    "**Key Approaches:**\n",
    "\n",
    "**1. Maximum Entropy IRL**\n",
    "\n",
    "Assume expert is noisily optimal with maximum entropy:\n",
    "\n",
    "$$P(\\tau) \\propto \\exp(R(\\tau))$$\n",
    "\n",
    "**2. Generative Adversarial Imitation Learning (GAIL)**\n",
    "\n",
    "- Generator: policy trying to match expert\n",
    "- Discriminator: distinguishes expert from generated trajectories\n",
    "- Reward = discriminator output\n",
    "\n",
    "**Applications:**\n",
    "- Learning from human demonstrations\n",
    "- Autonomous driving from human drivers\n",
    "- Robot manipulation from teleoperation\n",
    "\n",
    "#### Partial Observability\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "In many real-world scenarios, the agent cannot observe the full state:\n",
    "- Robot with limited sensors\n",
    "- Poker player who can't see opponents' cards\n",
    "- Medical diagnosis with incomplete patient history\n",
    "\n",
    "**Partially Observable MDP (POMDP):**\n",
    "\n",
    "A POMDP extends an MDP with:\n",
    "- Observation space $\\Omega$\n",
    "- Observation function $O(o|s, a)$: probability of observation given state and action\n",
    "\n",
    "The agent receives observations $o \\in \\Omega$ instead of states $s \\in S$.\n",
    "\n",
    "**Challenges:**\n",
    "- State is hidden: must infer from observations\n",
    "- History matters: same observation can mean different things\n",
    "- Belief tracking: maintain probability distribution over states\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "**1. Belief State Methods**\n",
    "\n",
    "Maintain a belief $b(s) = P(s|h)$ over states given history $h$:\n",
    "\n",
    "$$b'(s') = \\frac{O(o|s',a) \\sum_s P(s'|s,a) b(s)}{P(o|b,a)}$$\n",
    "\n",
    "**2. Recurrent Neural Networks**\n",
    "\n",
    "Use LSTM/GRU to maintain implicit belief state:\n",
    "- Input: sequence of observations\n",
    "- Hidden state: learned representation of history\n",
    "- Output: action or value\n",
    "\n",
    "**3. Attention Mechanisms**\n",
    "\n",
    "- Transformer architectures for RL\n",
    "- Attend to relevant parts of history\n",
    "- Decision Transformer: RL as sequence modeling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Simple demonstration of hierarchical RL concepts\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class HierarchicalGridWorld:\n",
    "    \"\"\"A grid world with rooms to demonstrate hierarchical RL.\n",
    "    \n",
    "    The grid has 4 rooms connected by doorways.\n",
    "    High-level: choose which room to go to\n",
    "    Low-level: navigate within/between rooms\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=11):\n",
    "        self.size = size\n",
    "        self.mid = size // 2\n",
    "        \n",
    "        # Define room centers\n",
    "        self.rooms = {\n",
    "            0: (self.mid // 2, self.mid // 2),           # Bottom-left\n",
    "            1: (self.mid + self.mid // 2, self.mid // 2), # Bottom-right\n",
    "            2: (self.mid // 2, self.mid + self.mid // 2), # Top-left\n",
    "            3: (self.mid + self.mid // 2, self.mid + self.mid // 2)  # Top-right\n",
    "        }\n",
    "        \n",
    "        # Define doorways (passages between rooms)\n",
    "        self.doorways = [\n",
    "            (self.mid, self.mid // 2),      # Between rooms 0 and 1\n",
    "            (self.mid, self.mid + self.mid // 2),  # Between rooms 2 and 3\n",
    "            (self.mid // 2, self.mid),      # Between rooms 0 and 2\n",
    "            (self.mid + self.mid // 2, self.mid)   # Between rooms 1 and 3\n",
    "        ]\n",
    "        \n",
    "        self.state = None\n",
    "        self.goal = None\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        # Start in room 0, goal in room 3\n",
    "        self.state = self.rooms[0]\n",
    "        self.goal = self.rooms[3]\n",
    "        return self.state\n",
    "    \n",
    "    def get_current_room(self):\n",
    "        \"\"\"Determine which room the agent is in.\"\"\"\n",
    "        x, y = self.state\n",
    "        if x < self.mid and y < self.mid:\n",
    "            return 0\n",
    "        elif x >= self.mid and y < self.mid:\n",
    "            return 1\n",
    "        elif x < self.mid and y >= self.mid:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take a primitive action (0=up, 1=right, 2=down, 3=left).\"\"\"\n",
    "        x, y = self.state\n",
    "        \n",
    "        # Calculate new position\n",
    "        if action == 0 and y < self.size - 1:\n",
    "            new_y = y + 1\n",
    "            new_x = x\n",
    "        elif action == 1 and x < self.size - 1:\n",
    "            new_x = x + 1\n",
    "            new_y = y\n",
    "        elif action == 2 and y > 0:\n",
    "            new_y = y - 1\n",
    "            new_x = x\n",
    "        elif action == 3 and x > 0:\n",
    "            new_x = x - 1\n",
    "            new_y = y\n",
    "        else:\n",
    "            new_x, new_y = x, y\n",
    "        \n",
    "        # Check for walls (can only cross at doorways)\n",
    "        crosses_vertical = (x < self.mid and new_x >= self.mid) or (x >= self.mid and new_x < self.mid)\n",
    "        crosses_horizontal = (y < self.mid and new_y >= self.mid) or (y >= self.mid and new_y < self.mid)\n",
    "        \n",
    "        if crosses_vertical and (x, y) not in self.doorways and (new_x, new_y) not in self.doorways:\n",
    "            new_x = x  # Blocked by wall\n",
    "        if crosses_horizontal and (x, y) not in self.doorways and (new_x, new_y) not in self.doorways:\n",
    "            new_y = y  # Blocked by wall\n",
    "        \n",
    "        self.state = (new_x, new_y)\n",
    "        done = self.state == self.goal\n",
    "        reward = 10.0 if done else -0.1\n",
    "        \n",
    "        return self.state, reward, done\n",
    "\n",
    "\n",
    "# Demonstrate the environment\n",
    "print(\"Hierarchical Grid World Environment\")\n",
    "print(\"=\"*50)\n",
    "print(\"This environment has 4 rooms connected by doorways.\")\n",
    "print(\"A hierarchical agent would:\")\n",
    "print(\"  - High-level: Plan which rooms to visit\")\n",
    "print(\"  - Low-level: Navigate to doorways and within rooms\")\n",
    "\n",
    "env = HierarchicalGridWorld()\n",
    "print(f\"Grid size: {env.size}x{env.size}\")\n",
    "print(f\"Start: {env.state} (Room {env.get_current_room()})\")\n",
    "print(f\"Goal: {env.goal} (Room 3)\")\n",
    "print(f\"Doorways: {env.doorways}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the hierarchical grid world\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Draw grid\n",
    "for i in range(env.size + 1):\n",
    "    ax.axhline(y=i, color='lightgray', linewidth=0.5)\n",
    "    ax.axvline(x=i, color='lightgray', linewidth=0.5)\n",
    "\n",
    "# Draw walls (thick lines at room boundaries)\n",
    "mid = env.mid\n",
    "# Vertical wall\n",
    "ax.plot([mid, mid], [0, mid], 'k-', linewidth=3)\n",
    "ax.plot([mid, mid], [mid, env.size], 'k-', linewidth=3)\n",
    "# Horizontal wall\n",
    "ax.plot([0, mid], [mid, mid], 'k-', linewidth=3)\n",
    "ax.plot([mid, env.size], [mid, mid], 'k-', linewidth=3)\n",
    "\n",
    "# Draw doorways (gaps in walls)\n",
    "for dx, dy in env.doorways:\n",
    "    ax.plot(dx, dy, 'yo', markersize=15, label='Doorway' if (dx, dy) == env.doorways[0] else '')\n",
    "\n",
    "# Draw room centers and labels\n",
    "room_colors = ['#FFB3BA', '#BAFFC9', '#BAE1FF', '#FFFFBA']\n",
    "for room_id, (rx, ry) in env.rooms.items():\n",
    "    ax.add_patch(plt.Circle((rx, ry), 1.5, color=room_colors[room_id], alpha=0.5))\n",
    "    ax.text(rx, ry, f'Room {room_id}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Draw start and goal\n",
    "ax.plot(env.rooms[0][0], env.rooms[0][1], 'go', markersize=20, label='Start')\n",
    "ax.plot(env.goal[0], env.goal[1], 'r*', markersize=25, label='Goal')\n",
    "\n",
    "# Draw hierarchical plan\n",
    "plan = [env.rooms[0], env.doorways[2], env.rooms[2], env.doorways[1], env.rooms[3]]\n",
    "plan_x = [p[0] for p in plan]\n",
    "plan_y = [p[1] for p in plan]\n",
    "ax.plot(plan_x, plan_y, 'b--', linewidth=2, alpha=0.7, label='High-level plan')\n",
    "\n",
    "ax.set_xlim(-0.5, env.size - 0.5)\n",
    "ax.set_ylim(-0.5, env.size - 0.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Hierarchical Grid World(4 Rooms with Doorways)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Hierarchical Decomposition:\")\n",
    "print(\"   High-level policy: Room 0 \u2192 Room 2 \u2192 Room 3\")\n",
    "print(\"   Low-level skills: Navigate to doorway, cross doorway, navigate to goal\")\n",
    "print(\"   Benefits:\")\n",
    "print(\"   - Temporal abstraction: plan over rooms, not individual steps\")\n",
    "print(\"   - Reusable skills: 'go to doorway' works in any room\")\n",
    "print(\"   - Efficient exploration: explore at multiple levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 Summary\n",
    "\n",
    "In this section, we covered advanced topics that extend the foundational RL algorithms:\n",
    "\n",
    "**Reward Engineering**\n",
    "- Reward shaping can dramatically speed up learning\n",
    "- Potential-based shaping preserves optimal policies\n",
    "- Common challenges: sparse rewards, reward hacking, credit assignment\n",
    "\n",
    "**Scaling and Generalization**\n",
    "- Function approximation handles high-dimensional spaces\n",
    "- Transfer learning leverages knowledge across tasks\n",
    "- Domain randomization and meta-learning improve generalization\n",
    "\n",
    "**Advanced Policy Methods**\n",
    "- Eligibility traces bridge TD and Monte Carlo methods\n",
    "- TRPO provides stable policy updates via trust regions\n",
    "- KL divergence constraints prevent catastrophic updates\n",
    "\n",
    "**Specialized Techniques**\n",
    "- Hierarchical RL decomposes complex tasks into subtasks\n",
    "- Inverse RL learns reward functions from demonstrations\n",
    "- POMDPs handle partial observability with belief states or RNNs\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "In the following sections, we'll see how these concepts are applied in real-world applications and explore current research trends in reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## Section 4: Code Implementations\n",
    "\n",
    "This notebook follows an integrated approach where code implementations are embedded directly within their conceptual sections. This design choice ensures that you learn the theory and immediately see it applied in code.\n",
    "\n",
    "**Quick Navigation to Implementations:**\n",
    "\n",
    "<a id='bandit-implementations'></a>\n",
    "**Bandit Algorithms** (Section 1):\n",
    "- `GreedyAgent` - Pure exploitation strategy\n",
    "- `EpsilonGreedyAgent` - Balanced exploration-exploitation\n",
    "- `OptimisticGreedyAgent` - Optimistic initial values\n",
    "- `UCBAgent` - Upper Confidence Bound selection\n",
    "\n",
    "<a id='mdp-implementations'></a>\n",
    "**MDP and Dynamic Programming** (Section 1):\n",
    "- `SimpleMDP` - MDP environment class\n",
    "- `policy_evaluation()` - Iterative policy evaluation\n",
    "- `policy_improvement()` - Greedy policy improvement\n",
    "- `value_iteration()` - Combined evaluation and improvement\n",
    "\n",
    "<a id='mc-implementations'></a>\n",
    "**Monte Carlo Methods** (Section 2):\n",
    "- `mc_prediction_first_visit()` - First-visit MC prediction\n",
    "- `mc_prediction_every_visit()` - Every-visit MC prediction\n",
    "- `mc_control_on_policy()` - On-policy MC control\n",
    "- `mc_prediction_off_policy()` - Off-policy with importance sampling\n",
    "\n",
    "<a id='td-implementations'></a>\n",
    "**Temporal Difference Methods** (Section 2):\n",
    "- `td_prediction()` - TD(0) prediction\n",
    "- `SARSAAgent` - On-policy TD control\n",
    "- `QLearningAgent` - Off-policy TD control\n",
    "- `QLearningAgentWithDecay` - Q-learning with epsilon decay\n",
    "\n",
    "<a id='deep-rl-implementations'></a>\n",
    "**Deep RL Implementations** (Section 2):\n",
    "- `QNetwork` - Neural network for Q-value approximation\n",
    "- `ReplayBuffer` - Experience replay memory\n",
    "- `DQNAgent` - Deep Q-Network with target network\n",
    "- `DoubleDQNAgent` - Double DQN for reduced overestimation\n",
    "- `PolicyNetwork` - Stochastic policy network\n",
    "- `REINFORCEAgent` - Monte Carlo policy gradient\n",
    "- `ActorCriticAgent` - Actor-Critic with TD advantage\n",
    "\n",
    "Each implementation includes detailed comments and follows PyTorch best practices for neural network-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## Section 5: Real-World Applications\n",
    "\n",
    "In this section, we explore how reinforcement learning is applied to solve real-world problems across various domains. Each application demonstrates unique challenges and solutions that arise when applying RL beyond toy environments.\n",
    "\n",
    "We'll cover:\n",
    "- **Traffic Signal Control**: Optimizing urban traffic flow\n",
    "- **Robotics**: Learning control policies for physical systems\n",
    "- **Autonomous Trading**: Financial decision-making under uncertainty\n",
    "- **Recommendation Systems**: Personalized content delivery\n",
    "- **Healthcare**: Treatment optimization and clinical decisions\n",
    "- **Hyperparameter Tuning**: Automating ML model optimization\n",
    "- **Game Playing**: Strategic decision-making in games\n",
    "- **Energy Management**: Smart grid optimization\n",
    "- **Chess**: Complex strategic planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='traffic'></a>\n",
    "### Traffic Signal Control\n",
    "\n",
    "**RL for Traffic Optimization**\n",
    "\n",
    "Traffic signal control is a compelling application of reinforcement learning because:\n",
    "\n",
    "1. **Dynamic Environment**: Traffic patterns change throughout the day and are affected by weather, events, and accidents\n",
    "2. **Sequential Decisions**: Signal timing decisions affect future traffic states\n",
    "3. **Measurable Outcomes**: We can directly measure waiting times, throughput, and congestion\n",
    "4. **Scalability Challenges**: Real cities have hundreds of interconnected intersections\n",
    "\n",
    "**Traditional vs RL Approaches:**\n",
    "\n",
    "| Traditional Methods | RL-Based Methods |\n",
    "|---------------------|------------------|\n",
    "| Fixed timing cycles | Adaptive to real-time traffic |\n",
    "| Rule-based adjustments | Learns optimal policies from data |\n",
    "| Requires manual tuning | Self-optimizing |\n",
    "| Limited adaptability | Handles novel situations |\n",
    "\n",
    "**State/Action Space Formulation:**\n",
    "\n",
    "**State Space** typically includes:\n",
    "- Queue lengths at each lane\n",
    "- Current signal phase\n",
    "- Time since last phase change\n",
    "- Vehicle speeds and positions (if sensors available)\n",
    "- Time of day (for pattern recognition)\n",
    "\n",
    "**Action Space** options:\n",
    "- **Phase Selection**: Choose which traffic phase to activate next\n",
    "- **Duration Control**: Decide how long to maintain current phase\n",
    "- **Combined**: Select both phase and duration\n",
    "\n",
    "**Reward Design:**\n",
    "- Negative reward for total waiting time\n",
    "- Negative reward for queue lengths\n",
    "- Positive reward for throughput (vehicles passing through)\n",
    "- Penalties for frequent phase changes (causes confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Traffic Signal Control Environment\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import deque\n\nclass TrafficIntersection:\n    \"\"\"\n    Simplified traffic intersection environment.\n    \n    Models a 4-way intersection with:\n    - North-South (NS) and East-West (EW) traffic flows\n    - Two signal phases: NS green or EW green\n    - Vehicles arriving according to Poisson process\n    \"\"\"\n    \n    def __init__(self, arrival_rates=(0.3, 0.3), max_queue=20):\n        \"\"\"\n        Args:\n            arrival_rates: (NS_rate, EW_rate) - avg vehicles per time step\n            max_queue: Maximum queue length per direction\n        \"\"\"\n        self.arrival_rates = arrival_rates\n        self.max_queue = max_queue\n        self.service_rate = 2  # Vehicles that can pass per green time step\n        self.min_green_time = 3  # Minimum time before phase change\n        \n        self.reset()\n    \n    def reset(self):\n        \"\"\"Reset the environment.\"\"\"\n        self.ns_queue = 0  # North-South queue\n        self.ew_queue = 0  # East-West queue\n        self.current_phase = 0  # 0 = NS green, 1 = EW green\n        self.phase_time = 0  # Time in current phase\n        self.total_waiting = 0\n        self.total_throughput = 0\n        self.step_count = 0\n        \n        return self._get_state()\n    \n    def _get_state(self):\n        \"\"\"Return current state as numpy array.\"\"\"\n        return np.array([\n            self.ns_queue / self.max_queue,  # Normalized NS queue\n            self.ew_queue / self.max_queue,  # Normalized EW queue\n            self.current_phase,               # Current phase\n            min(self.phase_time / 10, 1.0)   # Normalized phase time\n        ], dtype=np.float32)\n    \n    def step(self, action):\n        \"\"\"\n        Execute one time step.\n        \n        Args:\n            action: 0 = keep current phase, 1 = switch phase\n            \n        Returns:\n            state, reward, done, info\n        \"\"\"\n        self.step_count += 1\n        \n        # Handle phase switching\n        if action == 1 and self.phase_time >= self.min_green_time:\n            self.current_phase = 1 - self.current_phase\n            self.phase_time = 0\n        else:\n            self.phase_time += 1\n        \n        # Process vehicles (green direction can pass)\n        if self.current_phase == 0:  # NS green\n            passed = min(self.ns_queue, self.service_rate)\n            self.ns_queue -= passed\n        else:  # EW green\n            passed = min(self.ew_queue, self.service_rate)\n            self.ew_queue -= passed\n        \n        self.total_throughput += passed\n        \n        # New arrivals (Poisson process)\n        ns_arrivals = np.random.poisson(self.arrival_rates[0])\n        ew_arrivals = np.random.poisson(self.arrival_rates[1])\n        \n        self.ns_queue = min(self.ns_queue + ns_arrivals, self.max_queue)\n        self.ew_queue = min(self.ew_queue + ew_arrivals, self.max_queue)\n        \n        # Calculate waiting (vehicles in queue)\n        current_waiting = self.ns_queue + self.ew_queue\n        self.total_waiting += current_waiting\n        \n        # Reward: negative waiting time, bonus for throughput\n        reward = -0.1 * current_waiting + 0.5 * passed\n        \n        # Penalty for unnecessary phase switches\n        if action == 1 and self.phase_time < self.min_green_time:\n            reward -= 1.0\n        \n        done = self.step_count >= 200  # Episode length\n        \n        info = {\n            'throughput': self.total_throughput,\n            'avg_waiting': self.total_waiting / self.step_count,\n            'ns_queue': self.ns_queue,\n            'ew_queue': self.ew_queue\n        }\n        \n        return self._get_state(), reward, done, info\n\n\nclass TrafficQLearningAgent:\n    \"\"\"Q-Learning agent for traffic control.\"\"\"\n    \n    def __init__(self, state_bins=10, n_actions=2, lr=0.1, gamma=0.95, epsilon=1.0):\n        self.state_bins = state_bins\n        self.n_actions = n_actions\n        self.lr = lr\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        \n        # Q-table: discretized state space\n        self.q_table = {}\n    \n    def _discretize_state(self, state):\n        \"\"\"Convert continuous state to discrete bins.\"\"\"\n        bins = np.linspace(0, 1, self.state_bins)\n        discrete = tuple(np.digitize(s, bins) for s in state[:2])  # Queue states\n        discrete += (int(state[2]),)  # Phase\n        discrete += (min(int(state[3] * 5), 4),)  # Phase time (5 bins)\n        return discrete\n    \n    def get_action(self, state, training=True):\n        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n        discrete_state = self._discretize_state(state)\n        \n        if training and np.random.random() < self.epsilon:\n            return np.random.randint(self.n_actions)\n        \n        if discrete_state not in self.q_table:\n            self.q_table[discrete_state] = np.zeros(self.n_actions)\n        \n        return np.argmax(self.q_table[discrete_state])\n    \n    def update(self, state, action, reward, next_state, done):\n        \"\"\"Update Q-values.\"\"\"\n        discrete_state = self._discretize_state(state)\n        discrete_next = self._discretize_state(next_state)\n        \n        if discrete_state not in self.q_table:\n            self.q_table[discrete_state] = np.zeros(self.n_actions)\n        if discrete_next not in self.q_table:\n            self.q_table[discrete_next] = np.zeros(self.n_actions)\n        \n        # Q-learning update\n        if done:\n            target = reward\n        else:\n            target = reward + self.gamma * np.max(self.q_table[discrete_next])\n        \n        self.q_table[discrete_state][action] += self.lr * (\n            target - self.q_table[discrete_state][action]\n        )\n        \n        # Decay epsilon\n        if done:\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n\nprint(\"Traffic Signal Control classes defined successfully!\")\nprint(\"\\nEnvironment features:\")\nprint(\"  - 4-way intersection with NS and EW traffic\")\nprint(\"  - Poisson arrival process for vehicles\")\nprint(\"  - Actions: keep phase or switch phase\")\nprint(\"  - Reward based on waiting time and throughput\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the traffic control agent\nnp.random.seed(42)\n\nenv = TrafficIntersection(arrival_rates=(0.4, 0.3))  # Asymmetric traffic\nagent = TrafficQLearningAgent()\n\n# Training loop\nn_episodes = 300\nepisode_rewards = []\nepisode_throughputs = []\nepisode_waiting = []\n\nprint(\"Training Traffic Control Agent...\")\nprint(\"=\"*50)\n\nfor episode in range(n_episodes):\n    state = env.reset()\n    total_reward = 0\n    done = False\n    \n    while not done:\n        action = agent.get_action(state)\n        next_state, reward, done, info = env.step(action)\n        agent.update(state, action, reward, next_state, done)\n        state = next_state\n        total_reward += reward\n    \n    episode_rewards.append(total_reward)\n    episode_throughputs.append(info['throughput'])\n    episode_waiting.append(info['avg_waiting'])\n    \n    if (episode + 1) % 50 == 0:\n        avg_reward = np.mean(episode_rewards[-50:])\n        avg_throughput = np.mean(episode_throughputs[-50:])\n        print(f\"Episode {episode+1:3d} | Avg Reward: {avg_reward:7.1f} | \"\n              f\"Throughput: {avg_throughput:5.1f} | Epsilon: {agent.epsilon:.3f}\")\n\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize training progress and compare with baseline\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: Episode rewards\nax1 = axes[0, 0]\nwindow = 20\nsmoothed_rewards = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\nax1.plot(episode_rewards, alpha=0.3, color='blue')\nax1.plot(range(window-1, len(episode_rewards)), smoothed_rewards, \n         color='blue', linewidth=2, label='Smoothed')\nax1.set_xlabel('Episode')\nax1.set_ylabel('Total Reward')\nax1.set_title('Training Progress: Episode Rewards', fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Throughput over episodes\nax2 = axes[0, 1]\nsmoothed_throughput = np.convolve(episode_throughputs, np.ones(window)/window, mode='valid')\nax2.plot(episode_throughputs, alpha=0.3, color='green')\nax2.plot(range(window-1, len(episode_throughputs)), smoothed_throughput,\n         color='green', linewidth=2, label='Smoothed')\nax2.set_xlabel('Episode')\nax2.set_ylabel('Total Throughput')\nax2.set_title('Vehicles Passed Per Episode', fontweight='bold')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Average waiting time\nax3 = axes[1, 0]\nsmoothed_waiting = np.convolve(episode_waiting, np.ones(window)/window, mode='valid')\nax3.plot(episode_waiting, alpha=0.3, color='red')\nax3.plot(range(window-1, len(episode_waiting)), smoothed_waiting,\n         color='red', linewidth=2, label='Smoothed')\nax3.set_xlabel('Episode')\nax3.set_ylabel('Avg Queue Length')\nax3.set_title('Average Waiting (Queue Length)', fontweight='bold')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Plot 4: Compare RL agent vs Fixed timing baseline\nax4 = axes[1, 1]\n\n# Run comparison\ndef run_fixed_timing(env, phase_duration=10, n_episodes=50):\n    \"\"\"Baseline: Fixed timing controller.\"\"\"\n    throughputs = []\n    for _ in range(n_episodes):\n        state = env.reset()\n        done = False\n        step = 0\n        while not done:\n            # Switch every phase_duration steps\n            action = 1 if (step % phase_duration == phase_duration - 1) else 0\n            state, _, done, info = env.step(action)\n            step += 1\n        throughputs.append(info['throughput'])\n    return throughputs\n\ndef run_rl_agent(env, agent, n_episodes=50):\n    \"\"\"Run trained RL agent.\"\"\"\n    throughputs = []\n    for _ in range(n_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            action = agent.get_action(state, training=False)\n            state, _, done, info = env.step(action)\n        throughputs.append(info['throughput'])\n    return throughputs\n\nnp.random.seed(123)\nfixed_throughputs = run_fixed_timing(env, phase_duration=10, n_episodes=50)\nrl_throughputs = run_rl_agent(env, agent, n_episodes=50)\n\nmethods = ['Fixed Timing\\n(10 steps)', 'RL Agent']\nmeans = [np.mean(fixed_throughputs), np.mean(rl_throughputs)]\nstds = [np.std(fixed_throughputs), np.std(rl_throughputs)]\n\nbars = ax4.bar(methods, means, yerr=stds, capsize=5, color=['gray', 'blue'], alpha=0.7)\nax4.set_ylabel('Average Throughput')\nax4.set_title('RL Agent vs Fixed Timing Baseline', fontweight='bold')\nax4.grid(True, alpha=0.3, axis='y')\n\n# Add value labels\nfor bar, mean in zip(bars, means):\n    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n             f'{mean:.1f}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nimprovement = (np.mean(rl_throughputs) - np.mean(fixed_throughputs)) / np.mean(fixed_throughputs) * 100\nprint(f\"\\n\ud83d\udcca Results Summary:\")\nprint(f\"   Fixed Timing Throughput: {np.mean(fixed_throughputs):.1f} \u00b1 {np.std(fixed_throughputs):.1f}\")\nprint(f\"   RL Agent Throughput:     {np.mean(rl_throughputs):.1f} \u00b1 {np.std(rl_throughputs):.1f}\")\nprint(f\"   Improvement: {improvement:+.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='robotics'></a>\n",
    "### Robotics\n",
    "\n",
    "**Considerations for Real-World Robotics**\n",
    "\n",
    "Applying RL to robotics presents unique challenges that don't exist in simulated environments:\n",
    "\n",
    "**Key Challenges:**\n",
    "\n",
    "1. **Sample Efficiency**: Real robots are slow and expensive to operate\n",
    "   - Each interaction takes real time (can't speed up physics)\n",
    "   - Hardware wear and potential damage\n",
    "   - Need algorithms that learn from fewer samples\n",
    "\n",
    "2. **Safety Constraints**: Actions have real consequences\n",
    "   - Can't explore dangerous actions freely\n",
    "   - Need safe exploration strategies\n",
    "   - Must handle hardware limits (joint angles, velocities)\n",
    "\n",
    "3. **Continuous State/Action Spaces**: Physical systems are continuous\n",
    "   - Joint angles, velocities, torques are continuous\n",
    "   - Requires function approximation\n",
    "   - Discretization loses precision\n",
    "\n",
    "4. **Partial Observability**: Sensors don't capture everything\n",
    "   - Noisy sensor readings\n",
    "   - Occluded objects\n",
    "   - Unobservable internal states\n",
    "\n",
    "**Sim-to-Real Transfer:**\n",
    "\n",
    "A common approach is to train in simulation and transfer to real robots:\n",
    "\n",
    "| Simulation Advantages | Transfer Challenges |\n",
    "|----------------------|--------------------|\n",
    "| Fast data collection | Reality gap |\n",
    "| Safe exploration | Sensor differences |\n",
    "| Parallelizable | Actuator dynamics |\n",
    "| Reproducible | Environmental factors |\n",
    "\n",
    "**Techniques for Sim-to-Real:**\n",
    "- **Domain Randomization**: Vary simulation parameters to cover real-world variations\n",
    "- **System Identification**: Calibrate simulation to match real robot\n",
    "- **Progressive Training**: Fine-tune on real robot after simulation training\n",
    "- **Robust Policies**: Train policies that work across parameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple 2D Robotic Arm Environment\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, FancyArrow\nfrom matplotlib.collections import PatchCollection\n\nclass SimpleRoboticArm:\n    \"\"\"\n    A simple 2-link planar robotic arm environment.\n    \n    The arm must reach a target position by controlling joint angles.\n    This demonstrates basic robotic control with continuous states/actions.\n    \"\"\"\n    \n    def __init__(self, link_lengths=(1.0, 0.8)):\n        \"\"\"\n        Args:\n            link_lengths: (L1, L2) lengths of the two arm links\n        \"\"\"\n        self.L1, self.L2 = link_lengths\n        self.max_angle_change = 0.1  # Max radians per step\n        self.target_threshold = 0.1  # Distance to consider target reached\n        \n        self.reset()\n    \n    def reset(self, target=None):\n        \"\"\"Reset arm to initial position and set new target.\"\"\"\n        # Start with arm pointing right\n        self.theta1 = 0.0  # First joint angle\n        self.theta2 = 0.0  # Second joint angle\n        \n        # Random target within reachable workspace\n        if target is None:\n            # Generate random target in reachable area\n            max_reach = self.L1 + self.L2\n            min_reach = abs(self.L1 - self.L2)\n            r = np.random.uniform(min_reach + 0.1, max_reach - 0.1)\n            angle = np.random.uniform(-np.pi, np.pi)\n            self.target = np.array([r * np.cos(angle), r * np.sin(angle)])\n        else:\n            self.target = np.array(target)\n        \n        self.steps = 0\n        self.max_steps = 200\n        \n        return self._get_state()\n    \n    def _forward_kinematics(self):\n        \"\"\"Calculate end-effector position from joint angles.\"\"\"\n        # Position of first joint (elbow)\n        x1 = self.L1 * np.cos(self.theta1)\n        y1 = self.L1 * np.sin(self.theta1)\n        \n        # Position of end-effector\n        x2 = x1 + self.L2 * np.cos(self.theta1 + self.theta2)\n        y2 = y1 + self.L2 * np.sin(self.theta1 + self.theta2)\n        \n        return np.array([x1, y1]), np.array([x2, y2])\n    \n    def _get_state(self):\n        \"\"\"Return current state.\"\"\"\n        _, end_effector = self._forward_kinematics()\n        \n        # State: [joint angles, end-effector pos, target pos, distance to target]\n        distance = np.linalg.norm(end_effector - self.target)\n        \n        return np.array([\n            np.sin(self.theta1), np.cos(self.theta1),\n            np.sin(self.theta2), np.cos(self.theta2),\n            end_effector[0], end_effector[1],\n            self.target[0], self.target[1],\n            distance\n        ], dtype=np.float32)\n    \n    def step(self, action):\n        \"\"\"\n        Execute action and return new state.\n        \n        Args:\n            action: [delta_theta1, delta_theta2] - changes to joint angles\n                   Values should be in [-1, 1], scaled by max_angle_change\n        \"\"\"\n        self.steps += 1\n        \n        # Apply action (clip to valid range)\n        action = np.clip(action, -1, 1)\n        self.theta1 += action[0] * self.max_angle_change\n        self.theta2 += action[1] * self.max_angle_change\n        \n        # Keep angles in [-pi, pi]\n        self.theta1 = np.arctan2(np.sin(self.theta1), np.cos(self.theta1))\n        self.theta2 = np.arctan2(np.sin(self.theta2), np.cos(self.theta2))\n        \n        # Calculate reward\n        _, end_effector = self._forward_kinematics()\n        distance = np.linalg.norm(end_effector - self.target)\n        \n        # Reward: negative distance (closer is better)\n        reward = -distance\n        \n        # Bonus for reaching target\n        if distance < self.target_threshold:\n            reward += 10.0\n            done = True\n        elif self.steps >= self.max_steps:\n            done = True\n        else:\n            done = False\n        \n        info = {\n            'distance': distance,\n            'end_effector': end_effector,\n            'reached': distance < self.target_threshold\n        }\n        \n        return self._get_state(), reward, done, info\n    \n    def render(self, ax=None):\n        \"\"\"Visualize the arm and target.\"\"\"\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(8, 8))\n        \n        elbow, end_effector = self._forward_kinematics()\n        \n        # Draw arm links\n        ax.plot([0, elbow[0]], [0, elbow[1]], 'b-', linewidth=4, label='Link 1')\n        ax.plot([elbow[0], end_effector[0]], [elbow[1], end_effector[1]], \n                'g-', linewidth=4, label='Link 2')\n        \n        # Draw joints\n        ax.plot(0, 0, 'ko', markersize=10)  # Base\n        ax.plot(elbow[0], elbow[1], 'bo', markersize=8)  # Elbow\n        ax.plot(end_effector[0], end_effector[1], 'go', markersize=8)  # End-effector\n        \n        # Draw target\n        ax.plot(self.target[0], self.target[1], 'r*', markersize=15, label='Target')\n        \n        # Draw workspace boundary\n        theta = np.linspace(0, 2*np.pi, 100)\n        max_reach = self.L1 + self.L2\n        ax.plot(max_reach * np.cos(theta), max_reach * np.sin(theta), \n                'k--', alpha=0.3, label='Workspace')\n        \n        ax.set_xlim(-2.5, 2.5)\n        ax.set_ylim(-2.5, 2.5)\n        ax.set_aspect('equal')\n        ax.grid(True, alpha=0.3)\n        ax.legend(loc='upper right')\n        ax.set_title('2-Link Robotic Arm')\n        \n        return ax\n\n\nprint(\"Robotic Arm Environment created!\")\nprint(\"\\nFeatures:\")\nprint(\"  - 2-link planar arm with continuous joint angles\")\nprint(\"  - Goal: reach target position\")\nprint(\"  - State: joint angles, end-effector position, target, distance\")\nprint(\"  - Action: joint angle changes (continuous)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple Policy Gradient Agent for Robotic Arm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions import Normal\n\nclass RoboticArmPolicy(nn.Module):\n    \"\"\"Neural network policy for continuous control.\"\"\"\n    \n    def __init__(self, state_dim=9, action_dim=2, hidden_dim=64):\n        super().__init__()\n        \n        self.shared = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        \n        # Output mean and log_std for each action dimension\n        self.mean_head = nn.Linear(hidden_dim, action_dim)\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n    \n    def forward(self, state):\n        \"\"\"Return action distribution parameters.\"\"\"\n        x = self.shared(state)\n        mean = torch.tanh(self.mean_head(x))  # Actions in [-1, 1]\n        std = torch.exp(self.log_std)\n        return mean, std\n    \n    def get_action(self, state, deterministic=False):\n        \"\"\"Sample action from policy.\"\"\"\n        state = torch.FloatTensor(state).unsqueeze(0)\n        mean, std = self.forward(state)\n        \n        if deterministic:\n            return mean.squeeze(0).detach().numpy()\n        \n        dist = Normal(mean, std)\n        action = dist.sample()\n        log_prob = dist.log_prob(action).sum(dim=-1)\n        \n        return action.squeeze(0).detach().numpy(), log_prob\n\n\ndef train_robotic_arm(env, policy, n_episodes=500, lr=0.001, gamma=0.99):\n    \"\"\"Train policy using REINFORCE.\"\"\"\n    optimizer = optim.Adam(policy.parameters(), lr=lr)\n    \n    episode_rewards = []\n    success_rate = []\n    \n    for episode in range(n_episodes):\n        state = env.reset()\n        log_probs = []\n        rewards = []\n        done = False\n        \n        while not done:\n            action, log_prob = policy.get_action(state)\n            next_state, reward, done, info = env.step(action)\n            \n            log_probs.append(log_prob)\n            rewards.append(reward)\n            state = next_state\n        \n        # Calculate returns\n        returns = []\n        G = 0\n        for r in reversed(rewards):\n            G = r + gamma * G\n            returns.insert(0, G)\n        returns = torch.FloatTensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n        \n        # Policy gradient update\n        policy_loss = []\n        for log_prob, G in zip(log_probs, returns):\n            policy_loss.append(-log_prob * G)\n        \n        optimizer.zero_grad()\n        loss = torch.stack(policy_loss).sum()\n        loss.backward()\n        optimizer.step()\n        \n        episode_rewards.append(sum(rewards))\n        success_rate.append(1 if info['reached'] else 0)\n        \n        if (episode + 1) % 100 == 0:\n            avg_reward = np.mean(episode_rewards[-100:])\n            avg_success = np.mean(success_rate[-100:]) * 100\n            print(f\"Episode {episode+1:3d} | Avg Reward: {avg_reward:7.1f} | \"\n                  f\"Success Rate: {avg_success:.1f}%\")\n    \n    return episode_rewards, success_rate\n\n\n# Train the robotic arm agent\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nenv = SimpleRoboticArm()\npolicy = RoboticArmPolicy()\n\nprint(\"Training Robotic Arm Policy...\")\nprint(\"=\"*50)\n\nepisode_rewards, success_rate = train_robotic_arm(env, policy, n_episodes=500)\n\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize training and demonstrate learned policy\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Plot 1: Training rewards\nax1 = axes[0, 0]\nwindow = 50\nsmoothed = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\nax1.plot(episode_rewards, alpha=0.3, color='blue')\nax1.plot(range(window-1, len(episode_rewards)), smoothed, color='blue', linewidth=2)\nax1.set_xlabel('Episode')\nax1.set_ylabel('Total Reward')\nax1.set_title('Training Progress: Episode Rewards', fontweight='bold')\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Success rate\nax2 = axes[0, 1]\nsmoothed_success = np.convolve(success_rate, np.ones(window)/window, mode='valid')\nax2.plot(np.array(success_rate) * 100, alpha=0.3, color='green')\nax2.plot(range(window-1, len(success_rate)), smoothed_success * 100, \n         color='green', linewidth=2)\nax2.set_xlabel('Episode')\nax2.set_ylabel('Success Rate (%)')\nax2.set_title('Target Reaching Success Rate', fontweight='bold')\nax2.grid(True, alpha=0.3)\nax2.set_ylim([0, 100])\n\n# Plot 3: Demonstrate learned policy - trajectory\nax3 = axes[1, 0]\nnp.random.seed(123)\nstate = env.reset()\ntrajectory = [env._forward_kinematics()[1].copy()]\n\ndone = False\nwhile not done:\n    action = policy.get_action(state, deterministic=True)\n    state, _, done, info = env.step(action)\n    trajectory.append(env._forward_kinematics()[1].copy())\n\ntrajectory = np.array(trajectory)\nax3.plot(trajectory[:, 0], trajectory[:, 1], 'b-', alpha=0.5, linewidth=2, label='Trajectory')\nax3.scatter(trajectory[0, 0], trajectory[0, 1], c='green', s=100, marker='o', label='Start', zorder=5)\nax3.scatter(trajectory[-1, 0], trajectory[-1, 1], c='blue', s=100, marker='s', label='End', zorder=5)\nax3.scatter(env.target[0], env.target[1], c='red', s=200, marker='*', label='Target', zorder=5)\n\n# Draw workspace\ntheta = np.linspace(0, 2*np.pi, 100)\nmax_reach = env.L1 + env.L2\nax3.plot(max_reach * np.cos(theta), max_reach * np.sin(theta), 'k--', alpha=0.3)\n\nax3.set_xlim(-2.5, 2.5)\nax3.set_ylim(-2.5, 2.5)\nax3.set_aspect('equal')\nax3.grid(True, alpha=0.3)\nax3.legend()\nax3.set_title('End-Effector Trajectory', fontweight='bold')\n\n# Plot 4: Final arm configuration\nax4 = axes[1, 1]\nenv.render(ax4)\nax4.set_title(f'Final Configuration (Distance: {info[\"distance\"]:.3f})', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Test on multiple targets\nprint(\"\\n\ud83d\udcca Testing on 20 random targets:\")\nsuccesses = 0\ndistances = []\n\nfor i in range(20):\n    state = env.reset()\n    done = False\n    while not done:\n        action = policy.get_action(state, deterministic=True)\n        state, _, done, info = env.step(action)\n    \n    distances.append(info['distance'])\n    if info['reached']:\n        successes += 1\n\nprint(f\"   Success Rate: {successes/20*100:.0f}%\")\nprint(f\"   Average Final Distance: {np.mean(distances):.3f}\")\nprint(f\"   Min Distance: {np.min(distances):.3f}\")\nprint(f\"   Max Distance: {np.max(distances):.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trading'></a>\n",
    "### Autonomous Trading\n",
    "\n",
    "**Trading as an RL Problem**\n",
    "\n",
    "Financial trading is a natural fit for reinforcement learning:\n",
    "\n",
    "- **Sequential Decisions**: Buy, sell, or hold decisions over time\n",
    "- **Uncertain Environment**: Market dynamics are complex and partially observable\n",
    "- **Clear Rewards**: Profit/loss provides direct feedback\n",
    "- **Exploration vs Exploitation**: Balance trying new strategies vs using proven ones\n",
    "\n",
    "**MDP Formulation for Trading:**\n",
    "\n",
    "**State Space:**\n",
    "- Current portfolio holdings\n",
    "- Cash balance\n",
    "- Price history (recent prices, moving averages)\n",
    "- Technical indicators (RSI, MACD, etc.)\n",
    "- Market features (volume, volatility)\n",
    "\n",
    "**Action Space:**\n",
    "- Discrete: {Buy, Sell, Hold}\n",
    "- Continuous: Position size (-1 to +1 representing short to long)\n",
    "\n",
    "**Reward Design Considerations:**\n",
    "\n",
    "| Reward Type | Pros | Cons |\n",
    "|-------------|------|------|\n",
    "| Raw P&L | Direct, interpretable | High variance, sparse |\n",
    "| Sharpe Ratio | Risk-adjusted | Requires window of returns |\n",
    "| Log Returns | Handles compounding | Can be unstable |\n",
    "| Risk-Adjusted P&L | Balances risk/reward | Requires risk model |\n",
    "\n",
    "**Risk Management:**\n",
    "- Position limits (max exposure)\n",
    "- Stop-loss mechanisms\n",
    "- Drawdown constraints\n",
    "- Transaction cost modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple Trading Environment\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass SimpleTradingEnv:\n    \"\"\"\n    A simplified trading environment for a single asset.\n    \n    Features:\n    - Simulated price series with trend and noise\n    - Transaction costs\n    - Position tracking\n    - Risk-adjusted rewards\n    \"\"\"\n    \n    def __init__(self, n_steps=252, initial_cash=10000, transaction_cost=0.001):\n        \"\"\"\n        Args:\n            n_steps: Number of trading days\n            initial_cash: Starting capital\n            transaction_cost: Cost per trade as fraction of trade value\n        \"\"\"\n        self.n_steps = n_steps\n        self.initial_cash = initial_cash\n        self.transaction_cost = transaction_cost\n        self.lookback = 10  # Days of price history in state\n        \n        self.reset()\n    \n    def _generate_prices(self):\n        \"\"\"Generate synthetic price series with realistic properties.\"\"\"\n        # Geometric Brownian Motion with mean reversion\n        dt = 1/252  # Daily\n        mu = 0.1    # Annual drift\n        sigma = 0.2  # Annual volatility\n        \n        prices = [100.0]  # Starting price\n        for _ in range(self.n_steps + self.lookback):\n            # Add some mean reversion\n            mean_price = 100\n            reversion = 0.01 * (mean_price - prices[-1])\n            \n            # Random walk with drift\n            drift = (mu - 0.5 * sigma**2) * dt + reversion\n            diffusion = sigma * np.sqrt(dt) * np.random.randn()\n            \n            new_price = prices[-1] * np.exp(drift + diffusion)\n            prices.append(new_price)\n        \n        return np.array(prices)\n    \n    def reset(self):\n        \"\"\"Reset the environment.\"\"\"\n        self.prices = self._generate_prices()\n        self.current_step = self.lookback\n        self.cash = self.initial_cash\n        self.shares = 0\n        self.portfolio_values = [self.initial_cash]\n        \n        return self._get_state()\n    \n    def _get_state(self):\n        \"\"\"Return current state.\"\"\"\n        # Price features\n        recent_prices = self.prices[self.current_step - self.lookback:self.current_step + 1]\n        normalized_prices = recent_prices / recent_prices[0] - 1  # Returns from lookback start\n        \n        # Technical indicators\n        current_price = self.prices[self.current_step]\n        sma_5 = np.mean(self.prices[self.current_step-4:self.current_step+1])\n        sma_10 = np.mean(self.prices[self.current_step-9:self.current_step+1])\n        \n        # Position info\n        portfolio_value = self.cash + self.shares * current_price\n        position_ratio = (self.shares * current_price) / portfolio_value if portfolio_value > 0 else 0\n        \n        state = np.concatenate([\n            normalized_prices,\n            [current_price / 100 - 1],  # Normalized current price\n            [(sma_5 - current_price) / current_price],  # Price vs SMA5\n            [(sma_10 - current_price) / current_price],  # Price vs SMA10\n            [position_ratio],  # Current position\n            [self.cash / self.initial_cash]  # Normalized cash\n        ])\n        \n        return state.astype(np.float32)\n    \n    def step(self, action):\n        \"\"\"\n        Execute trading action.\n        \n        Args:\n            action: 0 = Sell, 1 = Hold, 2 = Buy\n            \n        Returns:\n            state, reward, done, info\n        \"\"\"\n        current_price = self.prices[self.current_step]\n        old_portfolio_value = self.cash + self.shares * current_price\n        \n        # Execute action\n        if action == 0:  # Sell\n            if self.shares > 0:\n                sell_value = self.shares * current_price\n                cost = sell_value * self.transaction_cost\n                self.cash += sell_value - cost\n                self.shares = 0\n        elif action == 2:  # Buy\n            if self.cash > 0:\n                # Buy as many shares as possible\n                max_shares = self.cash / (current_price * (1 + self.transaction_cost))\n                shares_to_buy = int(max_shares)\n                if shares_to_buy > 0:\n                    cost = shares_to_buy * current_price * (1 + self.transaction_cost)\n                    self.cash -= cost\n                    self.shares += shares_to_buy\n        # action == 1: Hold - do nothing\n        \n        # Move to next day\n        self.current_step += 1\n        new_price = self.prices[self.current_step]\n        new_portfolio_value = self.cash + self.shares * new_price\n        \n        self.portfolio_values.append(new_portfolio_value)\n        \n        # Calculate reward (daily return)\n        daily_return = (new_portfolio_value - old_portfolio_value) / old_portfolio_value\n        \n        # Risk-adjusted reward (penalize large drawdowns)\n        max_value = max(self.portfolio_values)\n        drawdown = (max_value - new_portfolio_value) / max_value\n        reward = daily_return - 0.5 * drawdown  # Penalize drawdowns\n        \n        done = self.current_step >= self.n_steps + self.lookback - 1\n        \n        info = {\n            'portfolio_value': new_portfolio_value,\n            'cash': self.cash,\n            'shares': self.shares,\n            'price': new_price,\n            'daily_return': daily_return,\n            'total_return': (new_portfolio_value - self.initial_cash) / self.initial_cash\n        }\n        \n        return self._get_state(), reward, done, info\n\n\nclass TradingQLearningAgent:\n    \"\"\"Q-Learning agent for trading.\"\"\"\n    \n    def __init__(self, state_dim, n_actions=3, lr=0.1, gamma=0.95, epsilon=1.0):\n        self.n_actions = n_actions\n        self.lr = lr\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        \n        self.q_table = {}\n    \n    def _discretize_state(self, state):\n        \"\"\"Discretize continuous state.\"\"\"\n        # Use key features for discretization\n        bins = [-0.1, -0.05, -0.02, 0, 0.02, 0.05, 0.1]\n        \n        # Discretize recent return (last element of price history)\n        recent_return = state[10]  # Normalized current price\n        price_bin = np.digitize(recent_return, bins)\n        \n        # Discretize position\n        position = state[-2]\n        pos_bin = 0 if position < 0.3 else (1 if position < 0.7 else 2)\n        \n        # Discretize trend (SMA comparison)\n        trend = state[12]  # Price vs SMA5\n        trend_bin = 0 if trend < -0.02 else (1 if trend < 0.02 else 2)\n        \n        return (price_bin, pos_bin, trend_bin)\n    \n    def get_action(self, state, training=True):\n        \"\"\"Select action using epsilon-greedy.\"\"\"\n        if training and np.random.random() < self.epsilon:\n            return np.random.randint(self.n_actions)\n        \n        discrete_state = self._discretize_state(state)\n        if discrete_state not in self.q_table:\n            self.q_table[discrete_state] = np.zeros(self.n_actions)\n        \n        return np.argmax(self.q_table[discrete_state])\n    \n    def update(self, state, action, reward, next_state, done):\n        \"\"\"Update Q-values.\"\"\"\n        discrete_state = self._discretize_state(state)\n        discrete_next = self._discretize_state(next_state)\n        \n        if discrete_state not in self.q_table:\n            self.q_table[discrete_state] = np.zeros(self.n_actions)\n        if discrete_next not in self.q_table:\n            self.q_table[discrete_next] = np.zeros(self.n_actions)\n        \n        if done:\n            target = reward\n        else:\n            target = reward + self.gamma * np.max(self.q_table[discrete_next])\n        \n        self.q_table[discrete_state][action] += self.lr * (\n            target - self.q_table[discrete_state][action]\n        )\n        \n        if done:\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n\nprint(\"Trading Environment created!\")\nprint(\"\\nFeatures:\")\nprint(\"  - Simulated price series with realistic dynamics\")\nprint(\"  - Transaction costs modeled\")\nprint(\"  - State includes price history and technical indicators\")\nprint(\"  - Actions: Buy, Hold, Sell\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the trading agent\nnp.random.seed(42)\n\nenv = SimpleTradingEnv(n_steps=252)  # One year of trading\nagent = TradingQLearningAgent(state_dim=16)\n\nn_episodes = 200\nepisode_returns = []\nepisode_sharpes = []\n\nprint(\"Training Trading Agent...\")\nprint(\"=\"*50)\n\nfor episode in range(n_episodes):\n    state = env.reset()\n    done = False\n    daily_returns = []\n    \n    while not done:\n        action = agent.get_action(state)\n        next_state, reward, done, info = env.step(action)\n        agent.update(state, action, reward, next_state, done)\n        daily_returns.append(info['daily_return'])\n        state = next_state\n    \n    total_return = info['total_return']\n    episode_returns.append(total_return)\n    \n    # Calculate Sharpe ratio (annualized)\n    if len(daily_returns) > 1 and np.std(daily_returns) > 0:\n        sharpe = np.mean(daily_returns) / np.std(daily_returns) * np.sqrt(252)\n    else:\n        sharpe = 0\n    episode_sharpes.append(sharpe)\n    \n    if (episode + 1) % 40 == 0:\n        avg_return = np.mean(episode_returns[-40:]) * 100\n        avg_sharpe = np.mean(episode_sharpes[-40:])\n        print(f\"Episode {episode+1:3d} | Avg Return: {avg_return:6.1f}% | \"\n              f\"Avg Sharpe: {avg_sharpe:.2f} | Epsilon: {agent.epsilon:.3f}\")\n\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize trading performance\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: Episode returns\nax1 = axes[0, 0]\nax1.plot(np.array(episode_returns) * 100, alpha=0.5, color='blue')\nwindow = 20\nsmoothed = np.convolve(episode_returns, np.ones(window)/window, mode='valid')\nax1.plot(range(window-1, len(episode_returns)), np.array(smoothed) * 100, \n         color='blue', linewidth=2)\nax1.axhline(y=0, color='black', linestyle='--', alpha=0.5)\nax1.set_xlabel('Episode')\nax1.set_ylabel('Total Return (%)')\nax1.set_title('Training Progress: Episode Returns', fontweight='bold')\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Sharpe ratios\nax2 = axes[0, 1]\nax2.plot(episode_sharpes, alpha=0.5, color='green')\nsmoothed_sharpe = np.convolve(episode_sharpes, np.ones(window)/window, mode='valid')\nax2.plot(range(window-1, len(episode_sharpes)), smoothed_sharpe, \n         color='green', linewidth=2)\nax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\nax2.set_xlabel('Episode')\nax2.set_ylabel('Sharpe Ratio')\nax2.set_title('Risk-Adjusted Performance', fontweight='bold')\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Sample trading episode - portfolio value\nax3 = axes[1, 0]\nnp.random.seed(456)\nstate = env.reset()\ndone = False\nactions_taken = []\n\nwhile not done:\n    action = agent.get_action(state, training=False)\n    actions_taken.append(action)\n    state, _, done, info = env.step(action)\n\n# Plot portfolio value\nax3.plot(env.portfolio_values, color='blue', linewidth=2, label='RL Agent')\n\n# Buy and hold baseline\nbuy_hold_values = [env.initial_cash]\nshares_bh = env.initial_cash / env.prices[env.lookback]\nfor i in range(env.lookback + 1, len(env.prices)):\n    buy_hold_values.append(shares_bh * env.prices[i])\nax3.plot(buy_hold_values[:len(env.portfolio_values)], color='gray', \n         linewidth=2, linestyle='--', label='Buy & Hold')\n\nax3.set_xlabel('Trading Day')\nax3.set_ylabel('Portfolio Value ($)')\nax3.set_title('Sample Episode: Portfolio Value', fontweight='bold')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Plot 4: Action distribution\nax4 = axes[1, 1]\naction_names = ['Sell', 'Hold', 'Buy']\naction_counts = [actions_taken.count(i) for i in range(3)]\ncolors = ['red', 'gray', 'green']\nbars = ax4.bar(action_names, action_counts, color=colors, alpha=0.7)\nax4.set_ylabel('Count')\nax4.set_title('Action Distribution in Sample Episode', fontweight='bold')\nax4.grid(True, alpha=0.3, axis='y')\n\nfor bar, count in zip(bars, action_counts):\n    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n             str(count), ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Performance comparison\nrl_return = (env.portfolio_values[-1] - env.initial_cash) / env.initial_cash * 100\nbh_return = (buy_hold_values[-1] - env.initial_cash) / env.initial_cash * 100\n\nprint(f\"\\n\ud83d\udcca Sample Episode Results:\")\nprint(f\"   RL Agent Return: {rl_return:+.1f}%\")\nprint(f\"   Buy & Hold Return: {bh_return:+.1f}%\")\nprint(f\"   Outperformance: {rl_return - bh_return:+.1f}%\")\nprint(f\"\\n   Actions: {action_counts[2]} buys, {action_counts[0]} sells, {action_counts[1]} holds\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recommendations'></a>\n",
    "### Recommendation Systems\n",
    "\n",
    "**Personalization with RL**\n",
    "\n",
    "Traditional recommendation systems use collaborative filtering or content-based methods. RL-based recommendations offer key advantages:\n",
    "\n",
    "**Why RL for Recommendations?**\n",
    "\n",
    "1. **Long-term Engagement**: Optimize for user retention, not just immediate clicks\n",
    "2. **Exploration**: Discover user preferences through strategic exploration\n",
    "3. **Adaptation**: Continuously adapt to changing user preferences\n",
    "4. **Sequential Nature**: Account for how recommendations affect future behavior\n",
    "\n",
    "**The Exploration-Exploitation Dilemma in Recommendations:**\n",
    "\n",
    "| Exploitation | Exploration |\n",
    "|--------------|-------------|\n",
    "| Recommend items similar to past likes | Try new item categories |\n",
    "| High short-term engagement | Discover hidden preferences |\n",
    "| Risk of filter bubbles | May show irrelevant items |\n",
    "| User satisfaction now | User satisfaction long-term |\n",
    "\n",
    "**MDP Formulation:**\n",
    "\n",
    "**State**: User profile, interaction history, context (time, device)\n",
    "\n",
    "**Action**: Which item(s) to recommend\n",
    "\n",
    "**Reward**: \n",
    "- Clicks, purchases, ratings\n",
    "- Watch time, engagement duration\n",
    "- Return visits, subscription retention\n",
    "\n",
    "**Challenges:**\n",
    "- Large action space (millions of items)\n",
    "- Sparse rewards (most items not interacted with)\n",
    "- Delayed feedback (subscription churn happens later)\n",
    "- Fairness and diversity concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple Recommendation Environment\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass SimpleRecommendationEnv:\n    \"\"\"\n    A simplified recommendation environment.\n    \n    Simulates a user with hidden preferences across item categories.\n    The agent must learn these preferences through recommendations.\n    \"\"\"\n    \n    def __init__(self, n_items=20, n_categories=5):\n        \"\"\"\n        Args:\n            n_items: Total number of items\n            n_categories: Number of item categories\n        \"\"\"\n        self.n_items = n_items\n        self.n_categories = n_categories\n        \n        # Assign items to categories\n        self.item_categories = np.random.randint(0, n_categories, n_items)\n        \n        # Item quality (affects click probability)\n        self.item_quality = np.random.uniform(0.3, 0.9, n_items)\n        \n        self.reset()\n    \n    def reset(self):\n        \"\"\"Reset with a new user.\"\"\"\n        # User's hidden category preferences (unknown to agent)\n        self.user_preferences = np.random.dirichlet(np.ones(self.n_categories))\n        \n        # User's interaction history\n        self.interaction_history = []\n        self.clicks = []\n        self.step_count = 0\n        self.max_steps = 50\n        \n        return self._get_state()\n    \n    def _get_state(self):\n        \"\"\"Return current state based on interaction history.\"\"\"\n        # Category click rates from history\n        category_clicks = np.zeros(self.n_categories)\n        category_shows = np.zeros(self.n_categories) + 0.1  # Smoothing\n        \n        for item, clicked in zip(self.interaction_history, self.clicks):\n            cat = self.item_categories[item]\n            category_shows[cat] += 1\n            if clicked:\n                category_clicks[cat] += 1\n        \n        click_rates = category_clicks / category_shows\n        \n        # Overall engagement\n        total_clicks = sum(self.clicks)\n        total_shows = len(self.clicks) + 1\n        overall_ctr = total_clicks / total_shows\n        \n        state = np.concatenate([\n            click_rates,\n            [overall_ctr],\n            [self.step_count / self.max_steps]\n        ])\n        \n        return state.astype(np.float32)\n    \n    def step(self, action):\n        \"\"\"\n        Recommend an item.\n        \n        Args:\n            action: Item index to recommend\n            \n        Returns:\n            state, reward, done, info\n        \"\"\"\n        self.step_count += 1\n        \n        # Get item properties\n        item_cat = self.item_categories[action]\n        item_qual = self.item_quality[action]\n        \n        # Click probability based on user preference and item quality\n        base_prob = self.user_preferences[item_cat]\n        click_prob = base_prob * item_qual\n        \n        # Add some noise and fatigue (repeated items less likely to be clicked)\n        times_shown = self.interaction_history.count(action)\n        fatigue = 0.8 ** times_shown  # Diminishing returns\n        click_prob *= fatigue\n        \n        # Simulate click\n        clicked = np.random.random() < click_prob\n        \n        self.interaction_history.append(action)\n        self.clicks.append(clicked)\n        \n        # Reward\n        reward = 1.0 if clicked else -0.1\n        \n        # Bonus for diversity (exploring different categories)\n        recent_cats = [self.item_categories[i] for i in self.interaction_history[-5:]]\n        diversity_bonus = len(set(recent_cats)) / 5 * 0.2\n        reward += diversity_bonus\n        \n        done = self.step_count >= self.max_steps\n        \n        info = {\n            'clicked': clicked,\n            'item_category': item_cat,\n            'click_prob': click_prob,\n            'total_clicks': sum(self.clicks),\n            'ctr': sum(self.clicks) / len(self.clicks)\n        }\n        \n        return self._get_state(), reward, done, info\n\n\nclass RecommendationAgent:\n    \"\"\"Epsilon-greedy agent with UCB exploration for recommendations.\"\"\"\n    \n    def __init__(self, n_items, lr=0.1, epsilon=0.3):\n        self.n_items = n_items\n        self.lr = lr\n        self.epsilon = epsilon\n        \n        # Track item performance\n        self.item_values = np.zeros(n_items)\n        self.item_counts = np.zeros(n_items)\n        self.total_steps = 0\n    \n    def get_action(self, state, training=True):\n        \"\"\"Select item using UCB-style exploration.\"\"\"\n        self.total_steps += 1\n        \n        if training and np.random.random() < self.epsilon:\n            return np.random.randint(self.n_items)\n        \n        # UCB scores\n        ucb_scores = self.item_values.copy()\n        \n        # Add exploration bonus for less-tried items\n        for i in range(self.n_items):\n            if self.item_counts[i] > 0:\n                exploration_bonus = np.sqrt(2 * np.log(self.total_steps) / self.item_counts[i])\n                ucb_scores[i] += exploration_bonus\n            else:\n                ucb_scores[i] = float('inf')  # Try untried items\n        \n        return np.argmax(ucb_scores)\n    \n    def update(self, action, reward):\n        \"\"\"Update item value estimates.\"\"\"\n        self.item_counts[action] += 1\n        n = self.item_counts[action]\n        \n        # Incremental average update\n        self.item_values[action] += (1/n) * (reward - self.item_values[action])\n\n\nprint(\"Recommendation Environment created!\")\nprint(\"\\nFeatures:\")\nprint(f\"  - {20} items across {5} categories\")\nprint(\"  - Users have hidden category preferences\")\nprint(\"  - Click probability depends on preference + item quality\")\nprint(\"  - Fatigue effect for repeated recommendations\")\nprint(\"  - Diversity bonus in rewards\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the recommendation agent\nnp.random.seed(42)\n\nenv = SimpleRecommendationEnv(n_items=20, n_categories=5)\nagent = RecommendationAgent(n_items=20)\n\nn_episodes = 200\nepisode_ctrs = []\nepisode_rewards = []\n\nprint(\"Training Recommendation Agent...\")\nprint(\"=\"*50)\n\nfor episode in range(n_episodes):\n    state = env.reset()\n    total_reward = 0\n    done = False\n    \n    while not done:\n        action = agent.get_action(state)\n        next_state, reward, done, info = env.step(action)\n        agent.update(action, reward)\n        total_reward += reward\n        state = next_state\n    \n    episode_ctrs.append(info['ctr'])\n    episode_rewards.append(total_reward)\n    \n    if (episode + 1) % 40 == 0:\n        avg_ctr = np.mean(episode_ctrs[-40:]) * 100\n        avg_reward = np.mean(episode_rewards[-40:])\n        print(f\"Episode {episode+1:3d} | Avg CTR: {avg_ctr:5.1f}% | Avg Reward: {avg_reward:6.1f}\")\n\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize recommendation performance\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: CTR over episodes\nax1 = axes[0, 0]\nax1.plot(np.array(episode_ctrs) * 100, alpha=0.5, color='blue')\nwindow = 20\nsmoothed_ctr = np.convolve(episode_ctrs, np.ones(window)/window, mode='valid')\nax1.plot(range(window-1, len(episode_ctrs)), np.array(smoothed_ctr) * 100,\n         color='blue', linewidth=2)\nax1.set_xlabel('Episode')\nax1.set_ylabel('Click-Through Rate (%)')\nax1.set_title('Training Progress: CTR', fontweight='bold')\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Rewards\nax2 = axes[0, 1]\nax2.plot(episode_rewards, alpha=0.5, color='green')\nsmoothed_rewards = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\nax2.plot(range(window-1, len(episode_rewards)), smoothed_rewards,\n         color='green', linewidth=2)\nax2.set_xlabel('Episode')\nax2.set_ylabel('Total Reward')\nax2.set_title('Training Progress: Rewards', fontweight='bold')\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Learned item values vs true quality\nax3 = axes[1, 0]\n# Run a test episode to see learned values\nstate = env.reset()\ntrue_values = env.user_preferences[env.item_categories] * env.item_quality\n\nax3.scatter(true_values, agent.item_values, alpha=0.7, s=100)\nax3.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect correlation')\nax3.set_xlabel('True Item Value (Pref \u00d7 Quality)')\nax3.set_ylabel('Learned Item Value')\nax3.set_title('Learned vs True Item Values', fontweight='bold')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Plot 4: Category exploration\nax4 = axes[1, 1]\n# Run test episode\nstate = env.reset()\ndone = False\nrecommended_cats = []\n\nwhile not done:\n    action = agent.get_action(state, training=False)\n    recommended_cats.append(env.item_categories[action])\n    state, _, done, _ = env.step(action)\n\n# Count recommendations per category\ncat_counts = [recommended_cats.count(i) for i in range(env.n_categories)]\ncat_prefs = env.user_preferences\n\nx = np.arange(env.n_categories)\nwidth = 0.35\n\nbars1 = ax4.bar(x - width/2, cat_counts, width, label='Recommendations', color='blue', alpha=0.7)\nbars2 = ax4.bar(x + width/2, cat_prefs * 50, width, label='User Preference (scaled)', color='red', alpha=0.7)\n\nax4.set_xlabel('Category')\nax4.set_ylabel('Count / Scaled Preference')\nax4.set_title('Recommendations vs User Preferences', fontweight='bold')\nax4.set_xticks(x)\nax4.set_xticklabels([f'Cat {i}' for i in range(env.n_categories)])\nax4.legend()\nax4.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n# Compare with random baseline\nprint(\"\\n\ud83d\udcca Performance Comparison:\")\nrandom_ctrs = []\nfor _ in range(50):\n    state = env.reset()\n    done = False\n    while not done:\n        action = np.random.randint(env.n_items)\n        state, _, done, info = env.step(action)\n    random_ctrs.append(info['ctr'])\n\nagent_ctrs = []\nfor _ in range(50):\n    state = env.reset()\n    done = False\n    while not done:\n        action = agent.get_action(state, training=False)\n        state, _, done, info = env.step(action)\n    agent_ctrs.append(info['ctr'])\n\nprint(f\"   Random Policy CTR: {np.mean(random_ctrs)*100:.1f}%\")\nprint(f\"   RL Agent CTR:      {np.mean(agent_ctrs)*100:.1f}%\")\nprint(f\"   Improvement:       {(np.mean(agent_ctrs) - np.mean(random_ctrs))*100:+.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='healthcare'></a>\n",
    "### Healthcare Applications\n",
    "\n",
    "**RL for Treatment Optimization**\n",
    "\n",
    "Healthcare presents unique opportunities for RL to improve patient outcomes through personalized treatment strategies.\n",
    "\n",
    "**Key Application Areas:**\n",
    "\n",
    "1. **Dynamic Treatment Regimes (DTRs)**\n",
    "   - Personalized treatment sequences\n",
    "   - Adapting to patient response\n",
    "   - Chronic disease management\n",
    "\n",
    "2. **Clinical Trial Optimization**\n",
    "   - Adaptive trial designs\n",
    "   - Dose-finding studies\n",
    "   - Patient allocation\n",
    "\n",
    "3. **Resource Allocation**\n",
    "   - ICU bed management\n",
    "   - Staff scheduling\n",
    "   - Equipment utilization\n",
    "\n",
    "**Challenges in Healthcare RL:**\n",
    "\n",
    "| Challenge | Description | Mitigation |\n",
    "|-----------|-------------|------------|\n",
    "| Safety | Wrong decisions can harm patients | Conservative policies, human oversight |\n",
    "| Data scarcity | Limited patient data | Transfer learning, simulation |\n",
    "| Delayed outcomes | Treatment effects take time | Appropriate discount factors |\n",
    "| Interpretability | Doctors need to understand decisions | Explainable RL methods |\n",
    "| Heterogeneity | Patients respond differently | Personalized policies |\n",
    "\n",
    "**Clinical Trial Applications:**\n",
    "\n",
    "- **Adaptive Randomization**: Allocate more patients to better-performing treatments\n",
    "- **Response-Adaptive Designs**: Modify trial based on interim results\n",
    "- **Biomarker-Guided Trials**: Personalize treatment based on patient characteristics\n",
    "\n",
    "**Ethical Considerations:**\n",
    "- Patient consent and autonomy\n",
    "- Fairness across patient populations\n",
    "- Transparency in decision-making\n",
    "- Regulatory compliance (FDA, EMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simplified Treatment Policy Example\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass SimpleTreatmentEnv:\n    \"\"\"\n    Simplified treatment environment for chronic disease management.\n    \n    Models a patient whose health state evolves based on treatment decisions.\n    This is a highly simplified model for educational purposes.\n    \"\"\"\n    \n    def __init__(self):\n        # Health state ranges from 0 (critical) to 100 (healthy)\n        self.min_health = 0\n        self.max_health = 100\n        \n        # Treatment options\n        self.treatments = {\n            0: {'name': 'No treatment', 'effect': -5, 'side_effect': 0, 'cost': 0},\n            1: {'name': 'Mild treatment', 'effect': 5, 'side_effect': 2, 'cost': 1},\n            2: {'name': 'Moderate treatment', 'effect': 15, 'side_effect': 5, 'cost': 3},\n            3: {'name': 'Aggressive treatment', 'effect': 25, 'side_effect': 10, 'cost': 5}\n        }\n        \n        self.reset()\n    \n    def reset(self):\n        \"\"\"Reset patient to initial state.\"\"\"\n        # Patient starts with moderate health\n        self.health = np.random.uniform(40, 60)\n        self.side_effects = 0\n        self.total_cost = 0\n        self.step_count = 0\n        self.max_steps = 20  # Treatment period\n        \n        # Patient-specific response (hidden from agent)\n        self.treatment_sensitivity = np.random.uniform(0.8, 1.2)\n        self.side_effect_sensitivity = np.random.uniform(0.8, 1.2)\n        \n        return self._get_state()\n    \n    def _get_state(self):\n        \"\"\"Return observable state.\"\"\"\n        return np.array([\n            self.health / self.max_health,\n            self.side_effects / 20,  # Normalized side effects\n            self.step_count / self.max_steps\n        ], dtype=np.float32)\n    \n    def step(self, action):\n        \"\"\"Apply treatment and observe outcome.\"\"\"\n        self.step_count += 1\n        \n        treatment = self.treatments[action]\n        \n        # Apply treatment effect (with patient-specific response)\n        effect = treatment['effect'] * self.treatment_sensitivity\n        effect += np.random.normal(0, 3)  # Random variation\n        \n        # Apply side effects\n        side_effect = treatment['side_effect'] * self.side_effect_sensitivity\n        side_effect += np.random.normal(0, 1)\n        \n        # Update health\n        self.health = np.clip(self.health + effect - side_effect, \n                              self.min_health, self.max_health)\n        \n        # Accumulate side effects (they persist)\n        self.side_effects = max(0, self.side_effects + side_effect * 0.5 - 1)\n        \n        # Track cost\n        self.total_cost += treatment['cost']\n        \n        # Natural disease progression (health tends to decline without treatment)\n        self.health -= 3\n        self.health = max(self.min_health, self.health)\n        \n        # Reward: health improvement minus side effects and cost\n        reward = self.health / 100 - self.side_effects / 20 - treatment['cost'] / 10\n        \n        # Bonus for maintaining good health\n        if self.health > 70:\n            reward += 0.5\n        \n        # Penalty for critical health\n        if self.health < 30:\n            reward -= 1.0\n        \n        done = self.step_count >= self.max_steps or self.health <= 0\n        \n        info = {\n            'health': self.health,\n            'side_effects': self.side_effects,\n            'total_cost': self.total_cost,\n            'treatment': treatment['name']\n        }\n        \n        return self._get_state(), reward, done, info\n\n\nclass TreatmentPolicyAgent:\n    \"\"\"Simple Q-learning agent for treatment decisions.\"\"\"\n    \n    def __init__(self, n_actions=4, lr=0.1, gamma=0.95, epsilon=1.0):\n        self.n_actions = n_actions\n        self.lr = lr\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.99\n        \n        self.q_table = {}\n    \n    def _discretize_state(self, state):\n        \"\"\"Discretize continuous state.\"\"\"\n        health_bin = int(state[0] * 10)  # 0-10\n        side_effect_bin = int(state[1] * 5)  # 0-5\n        time_bin = int(state[2] * 4)  # 0-4\n        return (health_bin, side_effect_bin, time_bin)\n    \n    def get_action(self, state, training=True):\n        \"\"\"Select action.\"\"\"\n        if training and np.random.random() < self.epsilon:\n            return np.random.randint(self.n_actions)\n        \n        discrete_state = self._discretize_state(state)\n        if discrete_state not in self.q_table:\n            self.q_table[discrete_state] = np.zeros(self.n_actions)\n        \n        return np.argmax(self.q_table[discrete_state])\n    \n    def update(self, state, action, reward, next_state, done):\n        \"\"\"Update Q-values.\"\"\"\n        discrete_state = self._discretize_state(state)\n        discrete_next = self._discretize_state(next_state)\n        \n        if discrete_state not in self.q_table:\n            self.q_table[discrete_state] = np.zeros(self.n_actions)\n        if discrete_next not in self.q_table:\n            self.q_table[discrete_next] = np.zeros(self.n_actions)\n        \n        if done:\n            target = reward\n        else:\n            target = reward + self.gamma * np.max(self.q_table[discrete_next])\n        \n        self.q_table[discrete_state][action] += self.lr * (\n            target - self.q_table[discrete_state][action]\n        )\n        \n        if done:\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n\nprint(\"Treatment Environment created!\")\nprint(\"\\nTreatment options:\")\nfor i, t in enumerate([\n    \"No treatment (natural progression)\",\n    \"Mild treatment (low effect, low side effects)\",\n    \"Moderate treatment (medium effect, medium side effects)\",\n    \"Aggressive treatment (high effect, high side effects)\"\n]):\n    print(f\"  {i}: {t}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train treatment policy\nnp.random.seed(42)\n\nenv = SimpleTreatmentEnv()\nagent = TreatmentPolicyAgent()\n\nn_episodes = 300\nepisode_health = []\nepisode_rewards = []\n\nprint(\"Training Treatment Policy...\")\nprint(\"=\"*50)\n\nfor episode in range(n_episodes):\n    state = env.reset()\n    total_reward = 0\n    done = False\n    \n    while not done:\n        action = agent.get_action(state)\n        next_state, reward, done, info = env.step(action)\n        agent.update(state, action, reward, next_state, done)\n        total_reward += reward\n        state = next_state\n    \n    episode_health.append(info['health'])\n    episode_rewards.append(total_reward)\n    \n    if (episode + 1) % 60 == 0:\n        avg_health = np.mean(episode_health[-60:])\n        avg_reward = np.mean(episode_rewards[-60:])\n        print(f\"Episode {episode+1:3d} | Avg Final Health: {avg_health:5.1f} | \"\n              f\"Avg Reward: {avg_reward:6.2f} | Epsilon: {agent.epsilon:.3f}\")\n\nprint(\"\\nTraining complete!\")\n\n# Visualize results\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: Final health over episodes\nax1 = axes[0, 0]\nax1.plot(episode_health, alpha=0.5, color='blue')\nwindow = 30\nsmoothed = np.convolve(episode_health, np.ones(window)/window, mode='valid')\nax1.plot(range(window-1, len(episode_health)), smoothed, color='blue', linewidth=2)\nax1.axhline(y=70, color='green', linestyle='--', alpha=0.5, label='Good health threshold')\nax1.axhline(y=30, color='red', linestyle='--', alpha=0.5, label='Critical threshold')\nax1.set_xlabel('Episode')\nax1.set_ylabel('Final Health')\nax1.set_title('Training Progress: Patient Health', fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Rewards\nax2 = axes[0, 1]\nax2.plot(episode_rewards, alpha=0.5, color='green')\nsmoothed_rewards = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\nax2.plot(range(window-1, len(episode_rewards)), smoothed_rewards, color='green', linewidth=2)\nax2.set_xlabel('Episode')\nax2.set_ylabel('Total Reward')\nax2.set_title('Training Progress: Rewards', fontweight='bold')\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Sample treatment trajectory\nax3 = axes[1, 0]\nstate = env.reset()\nhealth_trajectory = [env.health]\ntreatments_used = []\ndone = False\n\nwhile not done:\n    action = agent.get_action(state, training=False)\n    treatments_used.append(action)\n    state, _, done, info = env.step(action)\n    health_trajectory.append(info['health'])\n\nax3.plot(health_trajectory, 'b-', linewidth=2, marker='o', markersize=4)\nax3.axhline(y=70, color='green', linestyle='--', alpha=0.5)\nax3.axhline(y=30, color='red', linestyle='--', alpha=0.5)\nax3.set_xlabel('Time Step')\nax3.set_ylabel('Health')\nax3.set_title('Sample Patient Trajectory', fontweight='bold')\nax3.grid(True, alpha=0.3)\n\n# Annotate treatments\ntreatment_names = ['None', 'Mild', 'Mod', 'Agg']\nfor i, (t, h) in enumerate(zip(treatments_used, health_trajectory[:-1])):\n    if i % 3 == 0:  # Show every 3rd annotation\n        ax3.annotate(treatment_names[t], (i, h), textcoords=\"offset points\",\n                    xytext=(0, 10), ha='center', fontsize=8)\n\n# Plot 4: Treatment distribution by health state\nax4 = axes[1, 1]\n\n# Collect treatment decisions by health state\nhealth_bins = ['Critical\\n(0-30)', 'Low\\n(30-50)', 'Medium\\n(50-70)', 'Good\\n(70-100)']\ntreatment_counts = np.zeros((4, 4))  # health_bin x treatment\n\nfor _ in range(100):\n    state = env.reset()\n    done = False\n    while not done:\n        health = state[0] * 100\n        if health < 30:\n            h_bin = 0\n        elif health < 50:\n            h_bin = 1\n        elif health < 70:\n            h_bin = 2\n        else:\n            h_bin = 3\n        \n        action = agent.get_action(state, training=False)\n        treatment_counts[h_bin, action] += 1\n        state, _, done, _ = env.step(action)\n\n# Normalize\ntreatment_counts = treatment_counts / treatment_counts.sum(axis=1, keepdims=True)\n\nx = np.arange(4)\nwidth = 0.2\ncolors = ['lightgray', 'lightblue', 'orange', 'red']\n\nfor i in range(4):\n    ax4.bar(x + i*width, treatment_counts[:, i], width, \n            label=treatment_names[i], color=colors[i], alpha=0.8)\n\nax4.set_xlabel('Health State')\nax4.set_ylabel('Treatment Probability')\nax4.set_title('Learned Treatment Policy', fontweight='bold')\nax4.set_xticks(x + 1.5*width)\nax4.set_xticklabels(health_bins)\nax4.legend(title='Treatment')\nax4.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udcca Policy Interpretation:\")\nprint(\"   The agent learns to apply more aggressive treatment when health is low,\")\nprint(\"   and reduce treatment intensity as health improves to minimize side effects.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hyperparameter-tuning'></a>\n",
    "### Hyperparameter Tuning with RL\n",
    "\n",
    "**RL for Hyperparameter Optimization**\n",
    "\n",
    "Hyperparameter tuning is a natural fit for RL because:\n",
    "\n",
    "1. **Sequential Decisions**: Each hyperparameter choice affects subsequent evaluations\n",
    "2. **Expensive Evaluations**: Training ML models is costly, so sample efficiency matters\n",
    "3. **Exploration-Exploitation**: Balance trying new configurations vs refining promising ones\n",
    "4. **Transfer Learning**: Knowledge from previous tuning tasks can help\n",
    "\n",
    "**Traditional Methods vs RL:**\n",
    "\n",
    "| Method | Pros | Cons |\n",
    "|--------|------|------|\n",
    "| Grid Search | Simple, exhaustive | Exponential cost, no learning |\n",
    "| Random Search | Better coverage | No learning from results |\n",
    "| Bayesian Optimization | Sample efficient | Assumes smooth objective |\n",
    "| RL-based | Learns search strategy | Requires meta-training |\n",
    "\n",
    "**RL Formulation:**\n",
    "\n",
    "**State**: Current best performance, budget remaining, history of tried configurations\n",
    "\n",
    "**Action**: Next hyperparameter configuration to try\n",
    "\n",
    "**Reward**: Improvement in validation performance (or negative regret)\n",
    "\n",
    "**Key Insight**: The agent learns a *search strategy* that can generalize across different ML problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameter Tuning Comparison: Grid Search vs RL-based\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a simple classification problem\nnp.random.seed(42)\nX, y = make_classification(n_samples=500, n_features=20, n_informative=10,\n                           n_redundant=5, random_state=42)\n\ndef evaluate_hyperparameters(n_estimators, max_depth, min_samples_split):\n    \"\"\"Evaluate a hyperparameter configuration.\"\"\"\n    clf = RandomForestClassifier(\n        n_estimators=int(n_estimators),\n        max_depth=int(max_depth) if max_depth > 0 else None,\n        min_samples_split=int(min_samples_split),\n        random_state=42,\n        n_jobs=-1\n    )\n    # Use 3-fold CV for speed\n    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n    return scores.mean()\n\n\nclass HyperparameterSearchEnv:\n    \"\"\"Environment for hyperparameter search.\"\"\"\n    \n    def __init__(self, budget=20):\n        self.budget = budget\n        \n        # Define hyperparameter search space\n        self.param_ranges = {\n            'n_estimators': [10, 25, 50, 100, 200],\n            'max_depth': [3, 5, 10, 15, 0],  # 0 means None\n            'min_samples_split': [2, 5, 10, 20]\n        }\n        \n        self.n_configs = (len(self.param_ranges['n_estimators']) * \n                         len(self.param_ranges['max_depth']) * \n                         len(self.param_ranges['min_samples_split']))\n        \n        self.reset()\n    \n    def reset(self):\n        \"\"\"Reset the search.\"\"\"\n        self.tried_configs = set()\n        self.best_score = 0\n        self.scores_history = []\n        self.remaining_budget = self.budget\n        \n        return self._get_state()\n    \n    def _config_to_idx(self, n_est_idx, depth_idx, split_idx):\n        \"\"\"Convert parameter indices to flat index.\"\"\"\n        return (n_est_idx * len(self.param_ranges['max_depth']) * len(self.param_ranges['min_samples_split']) +\n                depth_idx * len(self.param_ranges['min_samples_split']) + split_idx)\n    \n    def _idx_to_config(self, idx):\n        \"\"\"Convert flat index to parameter values.\"\"\"\n        n_split = len(self.param_ranges['min_samples_split'])\n        n_depth = len(self.param_ranges['max_depth'])\n        \n        split_idx = idx % n_split\n        depth_idx = (idx // n_split) % n_depth\n        n_est_idx = idx // (n_split * n_depth)\n        \n        return (self.param_ranges['n_estimators'][n_est_idx],\n                self.param_ranges['max_depth'][depth_idx],\n                self.param_ranges['min_samples_split'][split_idx])\n    \n    def _get_state(self):\n        \"\"\"Return current state.\"\"\"\n        # Encode which configs have been tried\n        tried_mask = np.zeros(self.n_configs)\n        for idx in self.tried_configs:\n            tried_mask[idx] = 1\n        \n        state = np.concatenate([\n            tried_mask,\n            [self.best_score],\n            [self.remaining_budget / self.budget]\n        ])\n        return state.astype(np.float32)\n    \n    def step(self, action):\n        \"\"\"Try a hyperparameter configuration.\"\"\"\n        if action in self.tried_configs:\n            # Penalty for trying same config\n            return self._get_state(), -0.1, False, {'score': self.best_score}\n        \n        self.tried_configs.add(action)\n        self.remaining_budget -= 1\n        \n        # Evaluate configuration\n        n_est, max_depth, min_split = self._idx_to_config(action)\n        score = evaluate_hyperparameters(n_est, max_depth, min_split)\n        \n        self.scores_history.append(score)\n        \n        # Reward is improvement over best\n        improvement = max(0, score - self.best_score)\n        self.best_score = max(self.best_score, score)\n        \n        reward = improvement * 10  # Scale reward\n        \n        done = self.remaining_budget <= 0\n        \n        info = {\n            'score': score,\n            'best_score': self.best_score,\n            'config': (n_est, max_depth, min_split)\n        }\n        \n        return self._get_state(), reward, done, info\n\n\nclass HyperparameterAgent:\n    \"\"\"Simple agent for hyperparameter search.\"\"\"\n    \n    def __init__(self, n_configs, epsilon=0.3):\n        self.n_configs = n_configs\n        self.epsilon = epsilon\n        self.config_values = np.zeros(n_configs)\n        self.config_counts = np.zeros(n_configs)\n    \n    def get_action(self, state, tried_configs, training=True):\n        \"\"\"Select next configuration to try.\"\"\"\n        # Mask out already tried configs\n        available = [i for i in range(self.n_configs) if i not in tried_configs]\n        \n        if not available:\n            return np.random.randint(self.n_configs)\n        \n        if training and np.random.random() < self.epsilon:\n            return np.random.choice(available)\n        \n        # UCB-style selection among available configs\n        ucb_scores = np.full(self.n_configs, -np.inf)\n        total = sum(self.config_counts) + 1\n        \n        for i in available:\n            if self.config_counts[i] == 0:\n                ucb_scores[i] = float('inf')\n            else:\n                exploration = np.sqrt(2 * np.log(total) / self.config_counts[i])\n                ucb_scores[i] = self.config_values[i] + exploration\n        \n        return np.argmax(ucb_scores)\n    \n    def update(self, action, reward):\n        \"\"\"Update value estimates.\"\"\"\n        self.config_counts[action] += 1\n        n = self.config_counts[action]\n        self.config_values[action] += (1/n) * (reward - self.config_values[action])\n\n\nprint(\"Hyperparameter Search Environment created!\")\nprint(f\"\\nSearch space: {100} configurations\")\nprint(\"Parameters: n_estimators, max_depth, min_samples_split\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare Grid Search vs RL-based Search\nprint(\"Comparing search strategies...\")\nprint(\"=\"*50)\n\n# Grid Search (exhaustive within budget)\ndef grid_search(budget=20):\n    \"\"\"Simple grid search with limited budget.\"\"\"\n    configs = []\n    for n_est in [10, 50, 100, 200]:\n        for depth in [3, 5, 10, 0]:\n            for split in [2, 5, 10]:\n                configs.append((n_est, depth, split))\n    \n    # Randomly sample from grid\n    np.random.shuffle(configs)\n    configs = configs[:budget]\n    \n    best_score = 0\n    scores = []\n    for n_est, depth, split in configs:\n        score = evaluate_hyperparameters(n_est, depth, split)\n        scores.append(score)\n        best_score = max(best_score, score)\n    \n    return best_score, scores\n\n# Random Search\ndef random_search(env, budget=20):\n    \"\"\"Random search baseline.\"\"\"\n    env.reset()\n    best_score = 0\n    scores = []\n    \n    tried = set()\n    for _ in range(budget):\n        # Pick random untried config\n        available = [i for i in range(env.n_configs) if i not in tried]\n        if not available:\n            break\n        action = np.random.choice(available)\n        tried.add(action)\n        \n        n_est, depth, split = env._idx_to_config(action)\n        score = evaluate_hyperparameters(n_est, depth, split)\n        scores.append(score)\n        best_score = max(best_score, score)\n    \n    return best_score, scores\n\n# RL-based Search\ndef rl_search(env, agent, budget=20):\n    \"\"\"RL-based search.\"\"\"\n    state = env.reset()\n    best_score = 0\n    scores = []\n    \n    for _ in range(budget):\n        action = agent.get_action(state, env.tried_configs, training=False)\n        state, reward, done, info = env.step(action)\n        agent.update(action, reward)\n        scores.append(info['score'])\n        best_score = max(best_score, info['score'])\n        if done:\n            break\n    \n    return best_score, scores\n\n# Train the RL agent first\nenv = HyperparameterSearchEnv(budget=20)\nagent = HyperparameterAgent(n_configs=env.n_configs)\n\nprint(\"Training RL agent on hyperparameter search...\")\nfor episode in range(30):\n    state = env.reset()\n    done = False\n    while not done:\n        action = agent.get_action(state, env.tried_configs, training=True)\n        state, reward, done, info = env.step(action)\n        agent.update(action, reward)\n\nprint(\"Training complete!\\n\")\n\n# Run comparisons\nn_trials = 5\ngrid_results = []\nrandom_results = []\nrl_results = []\n\nprint(\"Running comparison trials...\")\nfor trial in range(n_trials):\n    np.random.seed(trial * 100)\n    \n    grid_best, grid_scores = grid_search(budget=20)\n    grid_results.append((grid_best, grid_scores))\n    \n    random_best, random_scores = random_search(env, budget=20)\n    random_results.append((random_best, random_scores))\n    \n    rl_best, rl_scores = rl_search(env, agent, budget=20)\n    rl_results.append((rl_best, rl_scores))\n    \n    print(f\"Trial {trial+1}: Grid={grid_best:.4f}, Random={random_best:.4f}, RL={rl_best:.4f}\")\n\nprint(\"\\nComparison complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize comparison results\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Best score comparison\nax1 = axes[0]\nmethods = ['Grid Search', 'Random Search', 'RL-based']\nbest_scores = [\n    [r[0] for r in grid_results],\n    [r[0] for r in random_results],\n    [r[0] for r in rl_results]\n]\n\nmeans = [np.mean(s) for s in best_scores]\nstds = [np.std(s) for s in best_scores]\n\nbars = ax1.bar(methods, means, yerr=stds, capsize=5, \n               color=['gray', 'blue', 'green'], alpha=0.7)\nax1.set_ylabel('Best Accuracy Found')\nax1.set_title('Hyperparameter Search: Best Score Comparison', fontweight='bold')\nax1.grid(True, alpha=0.3, axis='y')\n\nfor bar, mean in zip(bars, means):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n             f'{mean:.4f}', ha='center', va='bottom', fontweight='bold')\n\n# Plot 2: Convergence curves\nax2 = axes[1]\n\n# Average convergence curves\ndef get_best_so_far(scores_list):\n    \"\"\"Convert scores to best-so-far.\"\"\"\n    all_curves = []\n    for _, scores in scores_list:\n        best_so_far = []\n        best = 0\n        for s in scores:\n            best = max(best, s)\n            best_so_far.append(best)\n        all_curves.append(best_so_far)\n    \n    # Pad to same length\n    max_len = max(len(c) for c in all_curves)\n    padded = []\n    for c in all_curves:\n        padded.append(c + [c[-1]] * (max_len - len(c)))\n    \n    return np.mean(padded, axis=0)\n\ngrid_curve = get_best_so_far(grid_results)\nrandom_curve = get_best_so_far(random_results)\nrl_curve = get_best_so_far(rl_results)\n\nax2.plot(grid_curve, 'gray', linewidth=2, label='Grid Search')\nax2.plot(random_curve, 'blue', linewidth=2, label='Random Search')\nax2.plot(rl_curve, 'green', linewidth=2, label='RL-based')\n\nax2.set_xlabel('Number of Evaluations')\nax2.set_ylabel('Best Accuracy So Far')\nax2.set_title('Convergence Comparison', fontweight='bold')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udcca Summary:\")\nprint(f\"   Grid Search:   {np.mean([r[0] for r in grid_results]):.4f} \u00b1 {np.std([r[0] for r in grid_results]):.4f}\")\nprint(f\"   Random Search: {np.mean([r[0] for r in random_results]):.4f} \u00b1 {np.std([r[0] for r in random_results]):.4f}\")\nprint(f\"   RL-based:      {np.mean([r[0] for r in rl_results]):.4f} \u00b1 {np.std([r[0] for r in rl_results]):.4f}\")\nprint(\"\\n   Note: RL-based search learns to prioritize promising regions of the search space.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='game-playing'></a>\n",
    "### Game Playing\n",
    "\n",
    "**Game AI and Strategy Learning**\n",
    "\n",
    "Games have been a proving ground for RL since the field's inception. Key milestones include:\n",
    "\n",
    "- **TD-Gammon (1992)**: Backgammon at expert level\n",
    "- **Deep Blue (1997)**: Chess (primarily search-based)\n",
    "- **AlphaGo (2016)**: Go at superhuman level\n",
    "- **AlphaStar (2019)**: StarCraft II at grandmaster level\n",
    "- **OpenAI Five (2019)**: Dota 2 at professional level\n",
    "\n",
    "**Why Games are Great for RL:**\n",
    "\n",
    "1. **Clear Rules**: Well-defined state transitions and legal actions\n",
    "2. **Objective Rewards**: Win/lose provides unambiguous feedback\n",
    "3. **Self-Play**: Can generate unlimited training data\n",
    "4. **Measurable Progress**: Can compare against human experts\n",
    "\n",
    "**Key Techniques:**\n",
    "\n",
    "| Technique | Description | Used In |\n",
    "|-----------|-------------|--------|\n",
    "| Self-Play | Agent plays against itself | AlphaGo, AlphaZero |\n",
    "| MCTS | Monte Carlo Tree Search for planning | AlphaGo, MuZero |\n",
    "| Population Training | Multiple agents evolve together | OpenAI Five |\n",
    "| Curriculum Learning | Gradually increase difficulty | Many games |\n",
    "\n",
    "Let's implement a simple game environment and train an RL agent to play it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tic-Tac-Toe Environment and RL Agent\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass TicTacToe:\n    \"\"\"\n    Tic-Tac-Toe environment.\n    \n    Board positions: 0-8 (3x3 grid)\n    Players: 1 (X) and -1 (O)\n    \"\"\"\n    \n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        \"\"\"Reset the board.\"\"\"\n        self.board = np.zeros(9, dtype=int)\n        self.current_player = 1  # X starts\n        self.done = False\n        self.winner = None\n        return self._get_state()\n    \n    def _get_state(self):\n        \"\"\"Return current state.\"\"\"\n        # State from current player's perspective\n        return self.board * self.current_player\n    \n    def _check_winner(self):\n        \"\"\"Check if there's a winner.\"\"\"\n        # Winning combinations\n        lines = [\n            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Rows\n            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Columns\n            [0, 4, 8], [2, 4, 6]              # Diagonals\n        ]\n        \n        for line in lines:\n            total = sum(self.board[i] for i in line)\n            if total == 3:\n                return 1  # X wins\n            elif total == -3:\n                return -1  # O wins\n        \n        return None\n    \n    def get_valid_actions(self):\n        \"\"\"Return list of valid actions (empty positions).\"\"\"\n        return [i for i in range(9) if self.board[i] == 0]\n    \n    def step(self, action):\n        \"\"\"\n        Make a move.\n        \n        Args:\n            action: Position to place mark (0-8)\n            \n        Returns:\n            state, reward, done, info\n        \"\"\"\n        if self.done:\n            return self._get_state(), 0, True, {'winner': self.winner}\n        \n        if self.board[action] != 0:\n            # Invalid move - heavy penalty\n            return self._get_state(), -10, True, {'winner': None, 'invalid': True}\n        \n        # Make move\n        self.board[action] = self.current_player\n        \n        # Check for winner\n        self.winner = self._check_winner()\n        \n        if self.winner is not None:\n            self.done = True\n            # Reward from current player's perspective\n            reward = 1 if self.winner == self.current_player else -1\n        elif len(self.get_valid_actions()) == 0:\n            # Draw\n            self.done = True\n            reward = 0.5  # Small reward for draw\n            self.winner = 0\n        else:\n            reward = 0\n        \n        # Switch player\n        self.current_player *= -1\n        \n        info = {\n            'winner': self.winner,\n            'current_player': self.current_player\n        }\n        \n        return self._get_state(), reward, self.done, info\n    \n    def render(self):\n        \"\"\"Print the board.\"\"\"\n        symbols = {0: '.', 1: 'X', -1: 'O'}\n        print()\n        for i in range(3):\n            row = ' '.join(symbols[self.board[i*3 + j]] for j in range(3))\n            print(f\"  {row}\")\n        print()\n\n\nclass TicTacToeAgent:\n    \"\"\"Q-learning agent for Tic-Tac-Toe.\"\"\"\n    \n    def __init__(self, epsilon=0.3, lr=0.1, gamma=0.95):\n        self.epsilon = epsilon\n        self.lr = lr\n        self.gamma = gamma\n        self.q_table = {}\n    \n    def _state_to_key(self, state):\n        \"\"\"Convert state to hashable key.\"\"\"\n        return tuple(state)\n    \n    def get_action(self, state, valid_actions, training=True):\n        \"\"\"Select action using epsilon-greedy.\"\"\"\n        if training and np.random.random() < self.epsilon:\n            return np.random.choice(valid_actions)\n        \n        key = self._state_to_key(state)\n        if key not in self.q_table:\n            self.q_table[key] = np.zeros(9)\n        \n        # Get Q-values for valid actions only\n        q_values = self.q_table[key]\n        valid_q = [(a, q_values[a]) for a in valid_actions]\n        \n        # Select best valid action\n        best_q = max(q for _, q in valid_q)\n        best_actions = [a for a, q in valid_q if q == best_q]\n        \n        return np.random.choice(best_actions)\n    \n    def update(self, state, action, reward, next_state, done, next_valid_actions):\n        \"\"\"Update Q-values.\"\"\"\n        key = self._state_to_key(state)\n        next_key = self._state_to_key(next_state)\n        \n        if key not in self.q_table:\n            self.q_table[key] = np.zeros(9)\n        if next_key not in self.q_table:\n            self.q_table[next_key] = np.zeros(9)\n        \n        if done:\n            target = reward\n        else:\n            # Max Q-value for valid next actions\n            next_q = max(self.q_table[next_key][a] for a in next_valid_actions) if next_valid_actions else 0\n            target = reward + self.gamma * next_q\n        \n        self.q_table[key][action] += self.lr * (target - self.q_table[key][action])\n\n\nclass RandomPlayer:\n    \"\"\"Random player for baseline comparison.\"\"\"\n    \n    def get_action(self, state, valid_actions, training=True):\n        return np.random.choice(valid_actions)\n\n\nprint(\"Tic-Tac-Toe Environment created!\")\nprint(\"\\nBoard positions:\")\nprint(\"  0 | 1 | 2\")\nprint(\"  ---------\")\nprint(\"  3 | 4 | 5\")\nprint(\"  ---------\")\nprint(\"  6 | 7 | 8\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train agent through self-play\nnp.random.seed(42)\n\nenv = TicTacToe()\nagent = TicTacToeAgent(epsilon=0.3)\n\n# Training through self-play\nn_episodes = 10000\nwins = {'agent': 0, 'opponent': 0, 'draw': 0}\nwin_rates = []\n\nprint(\"Training through self-play...\")\nprint(\"=\"*50)\n\nfor episode in range(n_episodes):\n    state = env.reset()\n    \n    # Store experience for both players\n    experiences = {1: [], -1: []}\n    \n    while not env.done:\n        valid_actions = env.get_valid_actions()\n        player = env.current_player\n        \n        # Both players use the same agent (self-play)\n        action = agent.get_action(state, valid_actions)\n        \n        next_state, reward, done, info = env.step(action)\n        \n        # Store experience\n        experiences[player].append({\n            'state': state.copy(),\n            'action': action,\n            'reward': reward,\n            'next_state': next_state.copy(),\n            'done': done,\n            'next_valid': env.get_valid_actions() if not done else []\n        })\n        \n        state = next_state\n    \n    # Update Q-values for both players\n    for player in [1, -1]:\n        for exp in experiences[player]:\n            # Adjust reward based on final outcome\n            if env.winner == player:\n                final_reward = 1\n            elif env.winner == -player:\n                final_reward = -1\n            else:\n                final_reward = 0.1  # Draw\n            \n            agent.update(\n                exp['state'] * player,  # State from player's perspective\n                exp['action'],\n                final_reward if exp['done'] else 0,\n                exp['next_state'] * player,\n                exp['done'],\n                exp['next_valid']\n            )\n    \n    # Track wins (agent as player 1)\n    if env.winner == 1:\n        wins['agent'] += 1\n    elif env.winner == -1:\n        wins['opponent'] += 1\n    else:\n        wins['draw'] += 1\n    \n    # Record win rate every 500 episodes\n    if (episode + 1) % 500 == 0:\n        recent_rate = wins['agent'] / (episode + 1)\n        win_rates.append(recent_rate)\n        \n        if (episode + 1) % 2000 == 0:\n            print(f\"Episode {episode+1:5d} | Win Rate: {recent_rate*100:.1f}% | \"\n                  f\"States Learned: {len(agent.q_table)}\")\n\n# Decay epsilon for evaluation\nagent.epsilon = 0.05\n\nprint(f\"\\nTraining complete!\")\nprint(f\"Final stats: {wins['agent']} wins, {wins['opponent']} losses, {wins['draw']} draws\")\nprint(f\"States in Q-table: {len(agent.q_table)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate and visualize\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: Win rate over training\nax1 = axes[0, 0]\nepisodes = np.arange(500, n_episodes + 1, 500)\nax1.plot(episodes, np.array(win_rates) * 100, 'b-', linewidth=2)\nax1.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='Random baseline')\nax1.set_xlabel('Episode')\nax1.set_ylabel('Win Rate (%)')\nax1.set_title('Training Progress: Win Rate', fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Test against random player\nax2 = axes[0, 1]\n\ndef test_against_random(agent, n_games=100):\n    \"\"\"Test agent against random player.\"\"\"\n    results = {'win': 0, 'loss': 0, 'draw': 0}\n    \n    for game in range(n_games):\n        env = TicTacToe()\n        state = env.reset()\n        \n        # Alternate who goes first\n        agent_player = 1 if game % 2 == 0 else -1\n        \n        while not env.done:\n            valid_actions = env.get_valid_actions()\n            \n            if env.current_player == agent_player:\n                action = agent.get_action(state * agent_player, valid_actions, training=False)\n            else:\n                action = np.random.choice(valid_actions)\n            \n            state, _, _, _ = env.step(action)\n        \n        if env.winner == agent_player:\n            results['win'] += 1\n        elif env.winner == -agent_player:\n            results['loss'] += 1\n        else:\n            results['draw'] += 1\n    \n    return results\n\nresults = test_against_random(agent, n_games=200)\nlabels = ['Wins', 'Losses', 'Draws']\nvalues = [results['win'], results['loss'], results['draw']]\ncolors = ['green', 'red', 'gray']\n\nbars = ax2.bar(labels, values, color=colors, alpha=0.7)\nax2.set_ylabel('Number of Games')\nax2.set_title('Performance vs Random Player (200 games)', fontweight='bold')\nax2.grid(True, alpha=0.3, axis='y')\n\nfor bar, val in zip(bars, values):\n    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n             str(val), ha='center', va='bottom', fontweight='bold')\n\n# Plot 3: Sample game visualization\nax3 = axes[1, 0]\n\n# Play a sample game\nenv = TicTacToe()\nstate = env.reset()\nmoves = []\n\nwhile not env.done:\n    valid_actions = env.get_valid_actions()\n    \n    if env.current_player == 1:\n        action = agent.get_action(state, valid_actions, training=False)\n    else:\n        action = np.random.choice(valid_actions)\n    \n    moves.append((env.current_player, action))\n    state, _, _, _ = env.step(action)\n\n# Visualize final board\nboard_display = np.zeros((3, 3))\nfor player, pos in moves:\n    board_display[pos // 3, pos % 3] = player\n\nim = ax3.imshow(board_display, cmap='RdBu', vmin=-1, vmax=1)\nax3.set_xticks([0, 1, 2])\nax3.set_yticks([0, 1, 2])\n\n# Add X and O symbols\nfor i in range(3):\n    for j in range(3):\n        val = board_display[i, j]\n        if val == 1:\n            ax3.text(j, i, 'X', ha='center', va='center', fontsize=30, fontweight='bold')\n        elif val == -1:\n            ax3.text(j, i, 'O', ha='center', va='center', fontsize=30, fontweight='bold')\n\nwinner_text = 'X wins!' if env.winner == 1 else ('O wins!' if env.winner == -1 else 'Draw!')\nax3.set_title(f'Sample Game: {winner_text}', fontweight='bold')\n\n# Plot 4: Q-value heatmap for empty board\nax4 = axes[1, 1]\n\nempty_state = tuple([0] * 9)\nif empty_state in agent.q_table:\n    q_values = agent.q_table[empty_state].reshape(3, 3)\nelse:\n    q_values = np.zeros((3, 3))\n\nim = ax4.imshow(q_values, cmap='YlOrRd')\nax4.set_xticks([0, 1, 2])\nax4.set_yticks([0, 1, 2])\n\nfor i in range(3):\n    for j in range(3):\n        ax4.text(j, i, f'{q_values[i, j]:.2f}', ha='center', va='center', fontsize=12)\n\nax4.set_title('Learned Q-values for First Move', fontweight='bold')\nplt.colorbar(im, ax=ax4, label='Q-value')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n\ud83d\udcca Test Results vs Random Player:\")\nprint(f\"   Wins:   {results['win']} ({results['win']/200*100:.0f}%)\")\nprint(f\"   Losses: {results['loss']} ({results['loss']/200*100:.0f}%)\")\nprint(f\"   Draws:  {results['draw']} ({results['draw']/200*100:.0f}%)\")\nprint(f\"\\n   The agent learns that center (position 4) is the best opening move!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='energy'></a>\n",
    "### Energy Management\n",
    "\n",
    "**Smart Grid Optimization with RL**\n",
    "\n",
    "Energy management is a critical application of RL with significant environmental and economic impact:\n",
    "\n",
    "**Key Applications:**\n",
    "\n",
    "1. **Battery Storage Optimization**\n",
    "   - When to charge/discharge batteries\n",
    "   - Arbitrage between peak and off-peak prices\n",
    "   - Grid stability support\n",
    "\n",
    "2. **Demand Response**\n",
    "   - Shift flexible loads to off-peak hours\n",
    "   - Reduce peak demand charges\n",
    "   - Balance renewable generation\n",
    "\n",
    "3. **HVAC Control**\n",
    "   - Optimize heating/cooling schedules\n",
    "   - Pre-cool/pre-heat based on forecasts\n",
    "   - Comfort vs efficiency trade-offs\n",
    "\n",
    "4. **Renewable Integration**\n",
    "   - Handle intermittent solar/wind\n",
    "   - Coordinate with storage\n",
    "   - Minimize curtailment\n",
    "\n",
    "**Why RL for Energy?**\n",
    "\n",
    "| Challenge | RL Advantage |\n",
    "|-----------|-------------|\n",
    "| Uncertain demand | Learns from patterns |\n",
    "| Variable prices | Adapts to market dynamics |\n",
    "| Weather dependence | Incorporates forecasts |\n",
    "| Complex constraints | Handles multi-objective optimization |\n",
    "\n",
    "**MDP Formulation:**\n",
    "\n",
    "**State**: Battery level, current price, demand forecast, time of day, weather\n",
    "\n",
    "**Action**: Charge rate, discharge rate, or idle\n",
    "\n",
    "**Reward**: Negative electricity cost + penalties for constraint violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Energy Storage Optimization Environment\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass EnergyStorageEnv:\n    \"\"\"\n    Battery energy storage optimization environment.\n    \n    The agent controls a battery to minimize electricity costs\n    by charging during low-price periods and discharging during high-price periods.\n    \"\"\"\n    \n    def __init__(self, battery_capacity=100, max_charge_rate=20, efficiency=0.9):\n        \"\"\"\n        Args:\n            battery_capacity: Maximum battery capacity (kWh)\n            max_charge_rate: Maximum charge/discharge rate (kW)\n            efficiency: Round-trip efficiency\n        \"\"\"\n        self.battery_capacity = battery_capacity\n        self.max_charge_rate = max_charge_rate\n        self.efficiency = efficiency\n        \n        self.hours_per_day = 24\n        self.reset()\n    \n    def _generate_price_profile(self):\n        \"\"\"Generate realistic electricity price profile.\"\"\"\n        # Base price pattern (higher during day, lower at night)\n        hours = np.arange(24)\n        base_price = 0.10 + 0.08 * np.sin((hours - 6) * np.pi / 12)\n        \n        # Add peak pricing (morning and evening peaks)\n        morning_peak = 0.05 * np.exp(-((hours - 8) ** 2) / 4)\n        evening_peak = 0.08 * np.exp(-((hours - 18) ** 2) / 4)\n        \n        # Add random variation\n        noise = np.random.normal(0, 0.02, 24)\n        \n        prices = base_price + morning_peak + evening_peak + noise\n        return np.maximum(prices, 0.05)  # Minimum price\n    \n    def _generate_demand_profile(self):\n        \"\"\"Generate building demand profile.\"\"\"\n        hours = np.arange(24)\n        \n        # Base load\n        base_load = 30\n        \n        # Daytime increase\n        daytime = 20 * np.exp(-((hours - 14) ** 2) / 20)\n        \n        # Morning and evening peaks\n        morning = 15 * np.exp(-((hours - 8) ** 2) / 2)\n        evening = 25 * np.exp(-((hours - 19) ** 2) / 3)\n        \n        # Random variation\n        noise = np.random.normal(0, 5, 24)\n        \n        demand = base_load + daytime + morning + evening + noise\n        return np.maximum(demand, 10)\n    \n    def reset(self):\n        \"\"\"Reset the environment for a new day.\"\"\"\n        self.prices = self._generate_price_profile()\n        self.demand = self._generate_demand_profile()\n        \n        self.battery_level = self.battery_capacity * 0.5  # Start at 50%\n        self.current_hour = 0\n        self.total_cost = 0\n        self.actions_taken = []\n        self.battery_history = [self.battery_level]\n        \n        return self._get_state()\n    \n    def _get_state(self):\n        \"\"\"Return current state.\"\"\"\n        hour = self.current_hour\n        \n        # Normalize features\n        state = np.array([\n            self.battery_level / self.battery_capacity,  # Battery SOC\n            self.prices[hour] / 0.3,  # Normalized price\n            self.demand[hour] / 100,  # Normalized demand\n            np.sin(2 * np.pi * hour / 24),  # Time encoding\n            np.cos(2 * np.pi * hour / 24),\n            # Look-ahead prices (next 3 hours)\n            self.prices[(hour + 1) % 24] / 0.3,\n            self.prices[(hour + 2) % 24] / 0.3,\n            self.prices[(hour + 3) % 24] / 0.3,\n        ], dtype=np.float32)\n        \n        return state\n    \n    def step(self, action):\n        \"\"\"\n        Execute action for one hour.\n        \n        Args:\n            action: 0 = discharge max, 1 = discharge half, 2 = idle,\n                   3 = charge half, 4 = charge max\n        \"\"\"\n        hour = self.current_hour\n        price = self.prices[hour]\n        demand = self.demand[hour]\n        \n        # Convert action to power flow\n        action_map = {\n            0: -self.max_charge_rate,      # Full discharge\n            1: -self.max_charge_rate / 2,  # Half discharge\n            2: 0,                           # Idle\n            3: self.max_charge_rate / 2,   # Half charge\n            4: self.max_charge_rate        # Full charge\n        }\n        power = action_map[action]\n        \n        # Apply battery constraints\n        if power > 0:  # Charging\n            max_charge = (self.battery_capacity - self.battery_level) / self.efficiency\n            power = min(power, max_charge)\n            energy_change = power * self.efficiency\n        else:  # Discharging\n            max_discharge = self.battery_level\n            power = max(power, -max_discharge)\n            energy_change = power  # Discharge doesn't have efficiency loss in output\n        \n        # Update battery\n        self.battery_level += energy_change\n        self.battery_level = np.clip(self.battery_level, 0, self.battery_capacity)\n        \n        # Calculate cost\n        # Grid power = demand + charging - discharging\n        grid_power = demand + max(0, power) - max(0, -power) * self.efficiency\n        hour_cost = grid_power * price\n        \n        self.total_cost += hour_cost\n        self.actions_taken.append(action)\n        self.battery_history.append(self.battery_level)\n        \n        # Move to next hour\n        self.current_hour += 1\n        done = self.current_hour >= self.hours_per_day\n        \n        # Reward: negative cost (we want to minimize cost)\n        reward = -hour_cost\n        \n        # Bonus for maintaining reasonable battery level\n        soc = self.battery_level / self.battery_capacity\n        if 0.2 <= soc <= 0.8:\n            reward += 0.1\n        \n        info = {\n            'cost': hour_cost,\n            'total_cost': self.total_cost,\n            'battery_level': self.battery_level,\n            'price': price,\n            'demand': demand\n        }\n        \n        return self._get_state(), reward, done, info\n\n\nclass EnergyAgent:\n    \"\"\"Q-learning agent for energy management.\"\"\"\n    \n    def __init__(self, n_actions=5, lr=0.1, gamma=0.95, epsilon=1.0):\n        self.n_actions = n_actions\n        self.lr = lr\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.995\n        \n        self.q_table = {}\n    \n    def _discretize_state(self, state):\n        \"\"\"Discretize continuous state.\"\"\"\n        soc_bin = int(state[0] * 5)  # 5 bins for SOC\n        price_bin = int(state[1] * 5)  # 5 bins for price\n        hour_sin = int((state[3] + 1) * 2)  # 4 bins for time\n        future_price = int(state[5] * 3)  # 3 bins for future price\n        \n        return (soc_bin, price_bin, hour_sin, future_price)\n    \n    def get_action(self, state, training=True):\n        \"\"\"Select action.\"\"\"\n        if training and np.random.random() < self.epsilon:\n            return np.random.randint(self.n_actions)\n        \n        key = self._discretize_state(state)\n        if key not in self.q_table:\n            self.q_table[key] = np.zeros(self.n_actions)\n        \n        return np.argmax(self.q_table[key])\n    \n    def update(self, state, action, reward, next_state, done):\n        \"\"\"Update Q-values.\"\"\"\n        key = self._discretize_state(state)\n        next_key = self._discretize_state(next_state)\n        \n        if key not in self.q_table:\n            self.q_table[key] = np.zeros(self.n_actions)\n        if next_key not in self.q_table:\n            self.q_table[next_key] = np.zeros(self.n_actions)\n        \n        if done:\n            target = reward\n        else:\n            target = reward + self.gamma * np.max(self.q_table[next_key])\n        \n        self.q_table[key][action] += self.lr * (target - self.q_table[key][action])\n        \n        if done:\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n\nprint(\"Energy Storage Environment created!\")\nprint(\"\\nFeatures:\")\nprint(\"  - 100 kWh battery with 90% round-trip efficiency\")\nprint(\"  - Realistic price and demand profiles\")\nprint(\"  - 5 actions: full/half discharge, idle, half/full charge\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train energy management agent\nnp.random.seed(42)\n\nenv = EnergyStorageEnv()\nagent = EnergyAgent()\n\nn_episodes = 500\nepisode_costs = []\n\nprint(\"Training Energy Management Agent...\")\nprint(\"=\"*50)\n\nfor episode in range(n_episodes):\n    state = env.reset()\n    done = False\n    \n    while not done:\n        action = agent.get_action(state)\n        next_state, reward, done, info = env.step(action)\n        agent.update(state, action, reward, next_state, done)\n        state = next_state\n    \n    episode_costs.append(info['total_cost'])\n    \n    if (episode + 1) % 100 == 0:\n        avg_cost = np.mean(episode_costs[-100:])\n        print(f\"Episode {episode+1:3d} | Avg Daily Cost: ${avg_cost:.2f} | \"\n              f\"Epsilon: {agent.epsilon:.3f}\")\n\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize energy management results\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: Training progress\nax1 = axes[0, 0]\nax1.plot(episode_costs, alpha=0.3, color='blue')\nwindow = 30\nsmoothed = np.convolve(episode_costs, np.ones(window)/window, mode='valid')\nax1.plot(range(window-1, len(episode_costs)), smoothed, color='blue', linewidth=2)\nax1.set_xlabel('Episode')\nax1.set_ylabel('Daily Cost ($)')\nax1.set_title('Training Progress: Daily Electricity Cost', fontweight='bold')\nax1.grid(True, alpha=0.3)\n\n# Run a sample day with trained agent\nnp.random.seed(123)\nstate = env.reset()\ndone = False\n\nwhile not done:\n    action = agent.get_action(state, training=False)\n    state, _, done, info = env.step(action)\n\n# Plot 2: Price and demand profiles\nax2 = axes[0, 1]\nhours = np.arange(24)\nax2_twin = ax2.twinx()\n\nl1, = ax2.plot(hours, env.prices, 'b-', linewidth=2, label='Price')\nl2, = ax2_twin.plot(hours, env.demand, 'r-', linewidth=2, label='Demand')\n\nax2.set_xlabel('Hour')\nax2.set_ylabel('Price ($/kWh)', color='blue')\nax2_twin.set_ylabel('Demand (kW)', color='red')\nax2.set_title('Daily Price and Demand Profile', fontweight='bold')\nax2.legend([l1, l2], ['Price', 'Demand'], loc='upper left')\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Battery operation\nax3 = axes[1, 0]\nbattery_soc = np.array(env.battery_history) / env.battery_capacity * 100\n\nax3.fill_between(range(25), battery_soc, alpha=0.3, color='green')\nax3.plot(range(25), battery_soc, 'g-', linewidth=2)\nax3.axhline(y=20, color='red', linestyle='--', alpha=0.5, label='Min SOC')\nax3.axhline(y=80, color='red', linestyle='--', alpha=0.5, label='Max SOC')\n\nax3.set_xlabel('Hour')\nax3.set_ylabel('Battery SOC (%)')\nax3.set_title('Battery State of Charge', fontweight='bold')\nax3.set_ylim([0, 100])\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Plot 4: Compare with baseline (no storage)\nax4 = axes[1, 1]\n\n# Calculate baseline cost (no battery)\nbaseline_cost = sum(env.prices * env.demand)\n\n# Run multiple episodes for comparison\nrl_costs = []\nbaseline_costs = []\n\nfor _ in range(50):\n    state = env.reset()\n    done = False\n    while not done:\n        action = agent.get_action(state, training=False)\n        state, _, done, info = env.step(action)\n    rl_costs.append(info['total_cost'])\n    baseline_costs.append(sum(env.prices * env.demand))\n\nmethods = ['No Battery', 'RL-Optimized\\nBattery']\nmeans = [np.mean(baseline_costs), np.mean(rl_costs)]\nstds = [np.std(baseline_costs), np.std(rl_costs)]\n\nbars = ax4.bar(methods, means, yerr=stds, capsize=5, \n               color=['gray', 'green'], alpha=0.7)\nax4.set_ylabel('Daily Cost ($)')\nax4.set_title('Cost Comparison', fontweight='bold')\nax4.grid(True, alpha=0.3, axis='y')\n\nfor bar, mean in zip(bars, means):\n    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n             f'${mean:.2f}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nsavings = (np.mean(baseline_costs) - np.mean(rl_costs)) / np.mean(baseline_costs) * 100\nprint(f\"\\n\ud83d\udcca Results Summary:\")\nprint(f\"   Baseline Cost (no battery): ${np.mean(baseline_costs):.2f}\")\nprint(f\"   RL-Optimized Cost:          ${np.mean(rl_costs):.2f}\")\nprint(f\"   Cost Savings:               {savings:.1f}%\")\nprint(f\"\\n   The agent learns to charge during low-price hours and discharge during peaks!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='chess'></a>\n",
    "### Chess Environment\n",
    "\n",
    "**Chess as an RL Problem**\n",
    "\n",
    "Chess has been a benchmark for AI since the field's inception. While Deep Blue (1997) used primarily search-based methods, modern approaches like AlphaZero demonstrate the power of RL combined with deep learning.\n",
    "\n",
    "**Why Chess is Challenging:**\n",
    "\n",
    "1. **Enormous State Space**: ~10^44 legal positions\n",
    "2. **Large Action Space**: Average ~35 legal moves per position\n",
    "3. **Long-term Planning**: Games can last 40+ moves\n",
    "4. **Sparse Rewards**: Only win/lose/draw at game end\n",
    "5. **Perfect Information**: Both players see the full board\n",
    "\n",
    "**State Representation Challenges:**\n",
    "\n",
    "| Representation | Pros | Cons |\n",
    "|----------------|------|------|\n",
    "| FEN String | Compact, standard | Not neural-network friendly |\n",
    "| 8x8x12 Tensor | One-hot piece encoding | Large, sparse |\n",
    "| 8x8x119 (AlphaZero) | Includes history | Very large |\n",
    "| Graph Neural Network | Captures piece relationships | Complex |\n",
    "\n",
    "**AlphaZero's Approach:**\n",
    "\n",
    "- **Self-play**: Generates training data by playing against itself\n",
    "- **MCTS**: Monte Carlo Tree Search for move selection\n",
    "- **Neural Network**: Predicts move probabilities and position value\n",
    "- **No Human Knowledge**: Learns entirely from self-play (except rules)\n",
    "\n",
    "**Computational Challenges:**\n",
    "\n",
    "- AlphaZero used 5,000 TPUs for training\n",
    "- 44 million games of self-play\n",
    "- 9 hours of training to surpass all previous chess engines\n",
    "\n",
    "Let's set up a basic chess environment wrapper to understand the interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Chess Environment Setup\n# Note: This requires the python-chess library\n# Install with: pip install python-chess\n\nimport numpy as np\n\n# Try to import chess, provide fallback if not available\ntry:\n    import chess\n    CHESS_AVAILABLE = True\nexcept ImportError:\n    CHESS_AVAILABLE = False\n    print(\"python-chess not installed. Install with: pip install python-chess\")\n    print(\"Showing conceptual implementation instead.\\n\")\n\n\nclass ChessEnvWrapper:\n    \"\"\"\n    Wrapper around python-chess to create an RL-compatible interface.\n    \n    This demonstrates how to structure a chess environment for RL,\n    though training a competitive agent requires significant compute.\n    \"\"\"\n    \n    def __init__(self):\n        if not CHESS_AVAILABLE:\n            raise ImportError(\"python-chess required\")\n        \n        self.board = chess.Board()\n        self.move_history = []\n    \n    def reset(self):\n        \"\"\"Reset to starting position.\"\"\"\n        self.board = chess.Board()\n        self.move_history = []\n        return self._get_state()\n    \n    def _get_state(self):\n        \"\"\"\n        Convert board to neural network input.\n        \n        Returns 8x8x12 tensor:\n        - 6 planes for white pieces (P, N, B, R, Q, K)\n        - 6 planes for black pieces\n        \"\"\"\n        state = np.zeros((8, 8, 12), dtype=np.float32)\n        \n        piece_map = {\n            chess.PAWN: 0, chess.KNIGHT: 1, chess.BISHOP: 2,\n            chess.ROOK: 3, chess.QUEEN: 4, chess.KING: 5\n        }\n        \n        for square in chess.SQUARES:\n            piece = self.board.piece_at(square)\n            if piece:\n                row = square // 8\n                col = square % 8\n                plane = piece_map[piece.piece_type]\n                if piece.color == chess.BLACK:\n                    plane += 6\n                state[row, col, plane] = 1.0\n        \n        return state\n    \n    def get_legal_moves(self):\n        \"\"\"Return list of legal moves.\"\"\"\n        return list(self.board.legal_moves)\n    \n    def get_legal_move_mask(self):\n        \"\"\"\n        Return mask of legal moves for action space.\n        \n        Action space: 64 * 64 = 4096 (from_square, to_square)\n        Plus promotions, but simplified here.\n        \"\"\"\n        mask = np.zeros(4096, dtype=np.float32)\n        for move in self.board.legal_moves:\n            idx = move.from_square * 64 + move.to_square\n            mask[idx] = 1.0\n        return mask\n    \n    def step(self, action):\n        \"\"\"\n        Execute a move.\n        \n        Args:\n            action: Either a chess.Move object or (from_sq, to_sq) tuple\n        \"\"\"\n        if isinstance(action, tuple):\n            from_sq, to_sq = action\n            move = chess.Move(from_sq, to_sq)\n            # Handle promotion (default to queen)\n            if (self.board.piece_at(from_sq) == chess.PAWN and \n                (to_sq // 8 == 0 or to_sq // 8 == 7)):\n                move = chess.Move(from_sq, to_sq, promotion=chess.QUEEN)\n        else:\n            move = action\n        \n        # Check if move is legal\n        if move not in self.board.legal_moves:\n            return self._get_state(), -10, True, {'illegal_move': True}\n        \n        # Make move\n        self.board.push(move)\n        self.move_history.append(move)\n        \n        # Check game end\n        done = self.board.is_game_over()\n        \n        if done:\n            result = self.board.result()\n            if result == '1-0':\n                reward = 1 if len(self.move_history) % 2 == 1 else -1\n            elif result == '0-1':\n                reward = -1 if len(self.move_history) % 2 == 1 else 1\n            else:\n                reward = 0  # Draw\n        else:\n            reward = 0\n        \n        info = {\n            'result': self.board.result() if done else None,\n            'move': move.uci(),\n            'fen': self.board.fen()\n        }\n        \n        return self._get_state(), reward, done, info\n    \n    def render(self):\n        \"\"\"Print the board.\"\"\"\n        print(self.board)\n        print(f\"\\nFEN: {self.board.fen()}\")\n        print(f\"Legal moves: {len(list(self.board.legal_moves))}\")\n\n\nif CHESS_AVAILABLE:\n    print(\"Chess Environment Wrapper created!\")\n    print(\"\\nState representation: 8x8x12 tensor\")\n    print(\"  - 6 planes for white pieces (P, N, B, R, Q, K)\")\n    print(\"  - 6 planes for black pieces\")\n    print(\"\\nAction space: 4096 (64 from-squares \u00d7 64 to-squares)\")\nelse:\n    print(\"Chess environment requires python-chess library.\")\n    print(\"The code above shows the structure of a chess RL environment.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate chess environment (if available)\nif CHESS_AVAILABLE:\n    env = ChessEnvWrapper()\n    state = env.reset()\n    \n    print(\"Initial Position:\")\n    env.render()\n    \n    print(f\"\\nState shape: {state.shape}\")\n    print(f\"Number of legal moves: {len(env.get_legal_moves())}\")\n    \n    # Play a few random moves\n    print(\"\\n\" + \"=\"*50)\n    print(\"Playing 4 random moves...\")\n    print(\"=\"*50)\n    \n    for i in range(4):\n        legal_moves = env.get_legal_moves()\n        move = np.random.choice(legal_moves)\n        state, reward, done, info = env.step(move)\n        print(f\"\\nMove {i+1}: {info['move']}\")\n    \n    print(\"\\nPosition after 4 moves:\")\n    env.render()\nelse:\n    print(\"Skipping demo - python-chess not installed\")\n    print(\"\\nConceptual overview of chess RL:\")\n    print(\"1. State: Board position encoded as tensor\")\n    print(\"2. Action: Select from legal moves\")\n    print(\"3. Reward: +1 for win, -1 for loss, 0 for draw\")\n    print(\"4. Training: Self-play with MCTS + neural network\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computational Challenges and Practical Considerations**\n",
    "\n",
    "Training a competitive chess agent from scratch is computationally prohibitive for most practitioners:\n",
    "\n",
    "**AlphaZero Resources:**\n",
    "- 5,000 first-generation TPUs\n",
    "- 44 million games of self-play\n",
    "- 9 hours of training\n",
    "- Estimated cost: $25+ million in compute\n",
    "\n",
    "**Practical Alternatives:**\n",
    "\n",
    "1. **Use Pre-trained Models**\n",
    "   - Leela Chess Zero (Lc0): Open-source AlphaZero implementation\n",
    "   - Pre-trained weights available\n",
    "   - Can fine-tune for specific purposes\n",
    "\n",
    "2. **Simplified Variants**\n",
    "   - Mini-chess (5x5 or 6x6 boards)\n",
    "   - Reduced piece sets\n",
    "   - Faster training, same concepts\n",
    "\n",
    "3. **Transfer Learning**\n",
    "   - Start from strong engine evaluations\n",
    "   - Use supervised learning on grandmaster games\n",
    "   - Fine-tune with RL\n",
    "\n",
    "4. **Focus on Specific Aspects**\n",
    "   - Endgame tablebases\n",
    "   - Opening book learning\n",
    "   - Tactical pattern recognition\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- Chess demonstrates RL's potential for complex strategic reasoning\n",
    "- Self-play + MCTS + deep learning is a powerful combination\n",
    "- Computational requirements scale with problem complexity\n",
    "- Open-source implementations make experimentation accessible\n",
    "\n",
    "**Resources for Further Exploration:**\n",
    "- [Leela Chess Zero](https://lczero.org/) - Open-source neural network chess engine\n",
    "- [python-chess](https://python-chess.readthedocs.io/) - Chess library for Python\n",
    "- [AlphaZero Paper](https://arxiv.org/abs/1712.01815) - Original DeepMind paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## Section 6: Advanced Research & Deployment\n",
    "\n",
    "In this final section, we explore the cutting edge of reinforcement learning research and the practical challenges of deploying RL systems in production. We'll cover current research trends, ethical considerations, deployment challenges, and provide a complete end-to-end pipeline for taking RL from research to production.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Current research frontiers in multi-agent RL, meta-learning, and safe RL\n",
    "- Ethical and safety considerations for deploying RL systems\n",
    "- Practical challenges in production deployment\n",
    "- End-to-end pipeline for training, validating, and deploying RL models\n",
    "- Recent research highlights from top conferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research-trends'></a>\n",
    "### Current Research Trends\n",
    "\n",
    "The field of reinforcement learning is rapidly evolving. Here we explore the most exciting research directions that are shaping the future of RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Agent Reinforcement Learning (MARL)\n",
    "\n",
    "**What is Multi-Agent RL?**\n",
    "\n",
    "Multi-Agent Reinforcement Learning extends traditional RL to scenarios where multiple agents interact within a shared environment. Each agent learns its own policy while considering the actions and strategies of other agents.\n",
    "\n",
    "**Key Challenges:**\n",
    "\n",
    "1. **Non-Stationarity**: From each agent's perspective, the environment appears non-stationary because other agents are also learning and changing their policies.\n",
    "\n",
    "2. **Credit Assignment**: In cooperative settings, determining which agent's actions contributed to team success is difficult.\n",
    "\n",
    "3. **Scalability**: The joint action space grows exponentially with the number of agents.\n",
    "\n",
    "4. **Equilibrium Selection**: In competitive settings, multiple Nash equilibria may exist.\n",
    "\n",
    "**Recent Advancements:**\n",
    "\n",
    "- **QMIX (2018)**: Factorizes the joint action-value function into individual agent utilities while maintaining monotonicity constraints.\n",
    "\n",
    "- **MAPPO (2021)**: Adapts PPO for multi-agent settings with centralized training and decentralized execution.\n",
    "\n",
    "- **OpenAI Five (2019)**: Demonstrated superhuman performance in Dota 2 using distributed multi-agent training.\n",
    "\n",
    "- **AlphaStar (2019)**: Achieved Grandmaster level in StarCraft II using population-based training with multiple agents.\n",
    "\n",
    "**Mathematical Framework:**\n",
    "\n",
    "In a Markov Game (multi-agent MDP), we have:\n",
    "- $N$ agents with individual action spaces $\\mathcal{A}_1, ..., \\mathcal{A}_N$\n",
    "- Joint action $\\mathbf{a} = (a_1, ..., a_N)$\n",
    "- Individual reward functions $r_i(s, \\mathbf{a})$\n",
    "- Transition function $P(s' | s, \\mathbf{a})$\n",
    "\n",
    "Each agent $i$ aims to maximize:\n",
    "\n",
    "$J_i(\\pi_i) = \\mathbb{E}_{\\pi_1,...,\\pi_N}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_i(s_t, \\mathbf{a}_t)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curriculum Learning in RL\n",
    "\n",
    "**What is Curriculum Learning?**\n",
    "\n",
    "Curriculum learning is a training strategy where an agent learns from a sequence of progressively more difficult tasks, similar to how humans learn. Instead of immediately tackling the hardest version of a problem, the agent starts with simpler versions and gradually advances.\n",
    "\n",
    "**Why It Matters:**\n",
    "\n",
    "- **Faster Learning**: Starting with easier tasks provides more frequent rewards, accelerating initial learning.\n",
    "- **Better Exploration**: Simpler tasks help the agent discover useful behaviors before facing complex challenges.\n",
    "- **Improved Final Performance**: Gradual difficulty increase often leads to better policies than training on hard tasks directly.\n",
    "\n",
    "**Types of Curricula:**\n",
    "\n",
    "1. **Hand-Designed Curricula**: Domain experts manually design the progression of tasks.\n",
    "\n",
    "2. **Automatic Curriculum Learning (ACL)**: The curriculum is generated automatically based on agent performance.\n",
    "\n",
    "3. **Self-Play Curricula**: Agents create their own curriculum by playing against past versions of themselves.\n",
    "\n",
    "**Key Methods:**\n",
    "\n",
    "- **POET (2019)**: Paired Open-Ended Trailblazer - co-evolves environments and agents.\n",
    "- **PLR (2020)**: Prioritized Level Replay - prioritizes training on levels where the agent struggles.\n",
    "- **ACCEL (2022)**: Automatic Curriculum via Evolution of Learnable environments.\n",
    "\n",
    "**Example: Learning to Walk**\n",
    "\n",
    "A curriculum for teaching a robot to walk might progress:\n",
    "1. Standing balance on flat ground\n",
    "2. Taking single steps\n",
    "3. Walking on flat terrain\n",
    "4. Walking on uneven terrain\n",
    "5. Walking while carrying loads\n",
    "6. Running and jumping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta-Reinforcement Learning\n",
    "\n",
    "**What is Meta-RL?**\n",
    "\n",
    "Meta-reinforcement learning, or \"learning to learn,\" trains agents that can quickly adapt to new tasks with minimal experience. Instead of learning a single policy, meta-RL learns a learning algorithm or adaptation mechanism.\n",
    "\n",
    "**The Meta-Learning Objective:**\n",
    "\n",
    "Given a distribution of tasks $p(\\mathcal{T})$, meta-RL optimizes:\n",
    "\n",
    "$\\theta^* = \\arg\\max_\\theta \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})}\\left[J(\\theta, \\mathcal{T})\\right]$\n",
    "\n",
    "where $J(\\theta, \\mathcal{T})$ is the expected return on task $\\mathcal{T}$ after adaptation.\n",
    "\n",
    "**Key Approaches:**\n",
    "\n",
    "1. **Recurrent Meta-RL**: Uses RNNs to encode task information in hidden states.\n",
    "   - RL\u00b2 (2016): Treats the entire learning process as an RNN rollout.\n",
    "\n",
    "2. **Gradient-Based Meta-RL**: Learns initial parameters that can be quickly fine-tuned.\n",
    "   - MAML (2017): Model-Agnostic Meta-Learning - learns initialization for fast adaptation.\n",
    "\n",
    "3. **Context-Based Meta-RL**: Learns to infer task context from experience.\n",
    "   - PEARL (2019): Probabilistic Embeddings for Actor-critic RL.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "- **Robotics**: Quickly adapting to new objects or environments\n",
    "- **Personalization**: Adapting to individual user preferences\n",
    "- **Sim-to-Real Transfer**: Adapting simulation-trained policies to real-world dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Safe Reinforcement Learning\n",
    "\n",
    "**The Safety Challenge:**\n",
    "\n",
    "Traditional RL optimizes for reward without considering safety constraints. In real-world applications (autonomous vehicles, medical treatment, industrial control), unsafe exploration can have catastrophic consequences.\n",
    "\n",
    "**Key Challenges in Sensitive Areas:**\n",
    "\n",
    "1. **Healthcare**: Wrong treatment decisions can harm patients\n",
    "2. **Autonomous Vehicles**: Unsafe actions can cause accidents\n",
    "3. **Financial Systems**: Poor decisions can cause significant losses\n",
    "4. **Industrial Control**: Unsafe operations can damage equipment or harm workers\n",
    "\n",
    "**Approaches to Safe RL:**\n",
    "\n",
    "1. **Constrained MDPs (CMDPs)**:\n",
    "   - Add constraints to the optimization problem\n",
    "   - $\\max_\\pi J(\\pi)$ subject to $C_i(\\pi) \\leq d_i$ for safety constraints $C_i$\n",
    "\n",
    "2. **Risk-Sensitive RL**:\n",
    "   - Optimize risk-adjusted returns (CVaR, variance-penalized)\n",
    "   - $\\max_\\pi \\mathbb{E}[R] - \\lambda \\cdot \\text{Var}[R]$\n",
    "\n",
    "3. **Shielding**:\n",
    "   - Use a safety layer that overrides unsafe actions\n",
    "   - Formal verification of safety properties\n",
    "\n",
    "4. **Safe Exploration**:\n",
    "   - Constrained Policy Optimization (CPO)\n",
    "   - Lyapunov-based methods for stability guarantees\n",
    "\n",
    "**Recent Methods:**\n",
    "\n",
    "- **CPO (2017)**: Constrained Policy Optimization with trust region constraints\n",
    "- **LAMBDA (2020)**: Lagrangian-based safe RL with automatic constraint satisfaction\n",
    "- **Recovery RL (2021)**: Learns a recovery policy to return to safe states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretability in Reinforcement Learning\n",
    "\n",
    "**Why Interpretability Matters:**\n",
    "\n",
    "As RL systems are deployed in high-stakes domains, understanding *why* an agent makes certain decisions becomes crucial for:\n",
    "\n",
    "- **Trust**: Users need to understand and trust the system's decisions\n",
    "- **Debugging**: Identifying and fixing problematic behaviors\n",
    "- **Compliance**: Meeting regulatory requirements (e.g., GDPR's right to explanation)\n",
    "- **Safety**: Verifying that the agent behaves safely in edge cases\n",
    "\n",
    "**Approaches to Interpretable RL:**\n",
    "\n",
    "1. **Attention Mechanisms**:\n",
    "   - Visualize what parts of the state the agent focuses on\n",
    "   - Saliency maps for visual inputs\n",
    "\n",
    "2. **Policy Distillation**:\n",
    "   - Train interpretable models (decision trees, rule lists) to mimic complex policies\n",
    "   - Trade-off between interpretability and performance\n",
    "\n",
    "3. **Reward Decomposition**:\n",
    "   - Break down the reward into interpretable components\n",
    "   - Understand which factors drive agent behavior\n",
    "\n",
    "4. **Concept-Based Explanations**:\n",
    "   - Map agent decisions to human-understandable concepts\n",
    "   - \"The agent chose action X because it detected concept Y\"\n",
    "\n",
    "5. **Counterfactual Explanations**:\n",
    "   - \"The agent would have chosen differently if...\"\n",
    "   - Identify minimal changes that would alter decisions\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "- Deep RL policies are inherently complex and non-linear\n",
    "- Temporal credit assignment makes explanations difficult\n",
    "- Trade-off between model complexity and interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning in Natural Language Processing\n",
    "\n",
    "**The Role of RL in NLP:**\n",
    "\n",
    "RL has become increasingly important in NLP, particularly for:\n",
    "\n",
    "1. **Dialogue Systems**: Training conversational agents to maximize user satisfaction\n",
    "\n",
    "2. **Text Generation**: Optimizing for non-differentiable metrics (BLEU, ROUGE)\n",
    "\n",
    "3. **Machine Translation**: Fine-tuning translation quality beyond cross-entropy loss\n",
    "\n",
    "4. **Large Language Model Alignment**: RLHF (Reinforcement Learning from Human Feedback)\n",
    "\n",
    "**RLHF: Aligning Language Models**\n",
    "\n",
    "RLHF has become the dominant paradigm for aligning large language models with human preferences:\n",
    "\n",
    "1. **Supervised Fine-Tuning (SFT)**: Train on high-quality demonstrations\n",
    "\n",
    "2. **Reward Model Training**: Train a model to predict human preferences\n",
    "   - Given pairs of responses, predict which humans prefer\n",
    "\n",
    "3. **RL Fine-Tuning**: Use PPO to optimize the language model against the reward model\n",
    "   - $\\max_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi}[r(x, y)] - \\beta \\cdot D_{KL}[\\pi || \\pi_{ref}]$\n",
    "\n",
    "**Key Applications:**\n",
    "\n",
    "- **ChatGPT/GPT-4**: Uses RLHF for helpful, harmless, and honest responses\n",
    "- **Claude**: Constitutional AI with RL from AI feedback\n",
    "- **Gemini**: Multi-modal alignment using RL techniques\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "- Reward hacking: Models may exploit reward model weaknesses\n",
    "- Distribution shift: Generated text may differ from training distribution\n",
    "- Scalability: RLHF is computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Research References\n",
    "\n",
    "**Multi-Agent RL:**\n",
    "- Rashid et al. (2018). \"QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.\" ICML.\n",
    "- Yu et al. (2021). \"The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games.\" NeurIPS.\n",
    "\n",
    "**Curriculum Learning:**\n",
    "- Wang et al. (2019). \"POET: Open-Ended Coevolution of Environments and their Optimized Solutions.\" GECCO.\n",
    "- Jiang et al. (2021). \"Prioritized Level Replay.\" ICML.\n",
    "\n",
    "**Meta-RL:**\n",
    "- Finn et al. (2017). \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" ICML.\n",
    "- Rakelly et al. (2019). \"Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables.\" ICML.\n",
    "\n",
    "**Safe RL:**\n",
    "- Achiam et al. (2017). \"Constrained Policy Optimization.\" ICML.\n",
    "- Ray et al. (2019). \"Benchmarking Safe Exploration in Deep Reinforcement Learning.\" OpenAI.\n",
    "\n",
    "**RLHF:**\n",
    "- Ouyang et al. (2022). \"Training language models to follow instructions with human feedback.\" NeurIPS.\n",
    "- Bai et al. (2022). \"Constitutional AI: Harmlessness from AI Feedback.\" Anthropic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ethics'></a>\n",
    "### Ethical and Safety Considerations\n",
    "\n",
    "As RL systems become more powerful and widely deployed, ethical considerations become paramount. This section explores the key ethical challenges and frameworks for responsible RL development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethical Concerns in RL Deployment\n",
    "\n",
    "**Key Ethical Issues:**\n",
    "\n",
    "1. **Autonomy and Control**\n",
    "   - RL agents make decisions without human oversight\n",
    "   - Who is responsible when an autonomous system causes harm?\n",
    "   - How much autonomy should we grant to RL systems?\n",
    "\n",
    "2. **Transparency and Explainability**\n",
    "   - Deep RL policies are often \"black boxes\"\n",
    "   - Users may not understand why decisions are made\n",
    "   - Regulatory requirements for explainability (GDPR Article 22)\n",
    "\n",
    "3. **Privacy Concerns**\n",
    "   - RL systems often require extensive data collection\n",
    "   - Personalization systems learn detailed user profiles\n",
    "   - Data retention and usage policies\n",
    "\n",
    "4. **Dual Use Concerns**\n",
    "   - RL techniques can be used for beneficial or harmful purposes\n",
    "   - Autonomous weapons systems\n",
    "   - Manipulation and persuasion systems\n",
    "\n",
    "5. **Environmental Impact**\n",
    "   - Training large RL models requires significant compute\n",
    "   - Carbon footprint of RL research and deployment\n",
    "   - Need for more sample-efficient algorithms\n",
    "\n",
    "**Case Study: Recommendation Systems**\n",
    "\n",
    "RL-powered recommendation systems raise several ethical concerns:\n",
    "- **Filter Bubbles**: Optimizing for engagement may limit exposure to diverse viewpoints\n",
    "- **Addiction**: Systems may exploit psychological vulnerabilities to maximize engagement\n",
    "- **Misinformation**: Engagement-optimized systems may promote sensational but false content\n",
    "- **Privacy**: Detailed user modeling for personalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Alignment Problem\n",
    "\n",
    "**What is the Alignment Problem?**\n",
    "\n",
    "The alignment problem refers to the challenge of ensuring that AI systems' objectives and behaviors align with human values and intentions. In RL, this manifests as the difficulty of specifying reward functions that capture what we truly want.\n",
    "\n",
    "**Why Alignment is Difficult:**\n",
    "\n",
    "1. **Reward Specification**\n",
    "   - Human values are complex and context-dependent\n",
    "   - Difficult to encode all relevant considerations in a reward function\n",
    "   - Goodhart's Law: \"When a measure becomes a target, it ceases to be a good measure\"\n",
    "\n",
    "2. **Reward Hacking**\n",
    "   - Agents find unintended ways to maximize reward\n",
    "   - Example: A cleaning robot might hide dirt instead of cleaning it\n",
    "   - Example: A game-playing agent might exploit bugs rather than play skillfully\n",
    "\n",
    "3. **Distributional Shift**\n",
    "   - Agents may behave unexpectedly in situations not covered by training\n",
    "   - Real-world deployment differs from training environments\n",
    "\n",
    "4. **Mesa-Optimization**\n",
    "   - Learned policies may develop internal objectives that differ from the training objective\n",
    "   - These internal objectives may be misaligned with human values\n",
    "\n",
    "**Approaches to Alignment:**\n",
    "\n",
    "1. **Inverse Reinforcement Learning (IRL)**\n",
    "   - Learn reward functions from human demonstrations\n",
    "   - Infer what humans value from their behavior\n",
    "\n",
    "2. **Reward Modeling**\n",
    "   - Train models to predict human preferences\n",
    "   - RLHF uses this approach for language model alignment\n",
    "\n",
    "3. **Constitutional AI**\n",
    "   - Define principles that the AI should follow\n",
    "   - Use AI feedback to enforce these principles\n",
    "\n",
    "4. **Debate and Amplification**\n",
    "   - Use AI systems to help humans evaluate AI behavior\n",
    "   - Scale human oversight through AI assistance\n",
    "\n",
    "5. **Corrigibility**\n",
    "   - Design agents that allow themselves to be corrected\n",
    "   - Avoid agents that resist shutdown or modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness and Bias Considerations\n",
    "\n",
    "**Sources of Bias in RL:**\n",
    "\n",
    "1. **Training Data Bias**\n",
    "   - Historical data may reflect past discrimination\n",
    "   - Underrepresentation of certain groups\n",
    "   - Example: Healthcare RL trained on biased treatment data\n",
    "\n",
    "2. **Reward Function Bias**\n",
    "   - Reward functions may implicitly favor certain groups\n",
    "   - Proxy metrics may correlate with protected attributes\n",
    "   - Example: Optimizing for \"engagement\" may disadvantage certain demographics\n",
    "\n",
    "3. **Simulation Bias**\n",
    "   - Simulated environments may not accurately represent all populations\n",
    "   - Sim-to-real transfer may work better for some groups than others\n",
    "\n",
    "4. **Feedback Loop Bias**\n",
    "   - RL systems can amplify existing biases through feedback loops\n",
    "   - Example: Predictive policing concentrating resources in already over-policed areas\n",
    "\n",
    "**Fairness Definitions:**\n",
    "\n",
    "Several mathematical definitions of fairness exist, often in tension:\n",
    "\n",
    "1. **Demographic Parity**: Equal positive outcome rates across groups\n",
    "   - $P(\\hat{Y}=1|A=0) = P(\\hat{Y}=1|A=1)$\n",
    "\n",
    "2. **Equalized Odds**: Equal true positive and false positive rates\n",
    "   - $P(\\hat{Y}=1|Y=y,A=0) = P(\\hat{Y}=1|Y=y,A=1)$ for $y \\in \\{0,1\\}$\n",
    "\n",
    "3. **Individual Fairness**: Similar individuals should be treated similarly\n",
    "   - $d(f(x_1), f(x_2)) \\leq L \\cdot d(x_1, x_2)$\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "\n",
    "1. **Pre-processing**: Remove or transform biased features\n",
    "2. **In-processing**: Add fairness constraints to the RL objective\n",
    "3. **Post-processing**: Adjust outputs to satisfy fairness criteria\n",
    "4. **Auditing**: Regular testing for disparate impact across groups\n",
    "\n",
    "**Case Study: Healthcare RL**\n",
    "\n",
    "An RL system for treatment recommendations must consider:\n",
    "- Historical treatment disparities in training data\n",
    "- Different baseline health outcomes across demographics\n",
    "- Access to healthcare affecting data availability\n",
    "- Ensuring equitable treatment recommendations across all patient groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Studies in RL Ethics\n",
    "\n",
    "**Case Study 1: Autonomous Vehicles**\n",
    "\n",
    "RL-trained autonomous vehicles face ethical dilemmas:\n",
    "- **Trolley Problem Scenarios**: How should the car behave in unavoidable accident situations?\n",
    "- **Risk Distribution**: Should the car prioritize passenger safety over pedestrian safety?\n",
    "- **Transparency**: Should passengers know how the car makes decisions?\n",
    "- **Liability**: Who is responsible for accidents - manufacturer, owner, or algorithm?\n",
    "\n",
    "**Case Study 2: Content Moderation**\n",
    "\n",
    "RL systems for content moderation must balance:\n",
    "- **Free Speech**: Avoiding over-censorship of legitimate content\n",
    "- **Harm Prevention**: Removing genuinely harmful content\n",
    "- **Cultural Context**: Different norms across regions and communities\n",
    "- **Consistency**: Applying rules fairly across all users\n",
    "\n",
    "**Case Study 3: Financial Trading**\n",
    "\n",
    "RL trading systems raise concerns about:\n",
    "- **Market Stability**: Could RL agents cause flash crashes?\n",
    "- **Fairness**: Do RL systems give unfair advantages to wealthy institutions?\n",
    "- **Manipulation**: Could RL agents learn to manipulate markets?\n",
    "- **Systemic Risk**: What happens when many RL agents interact?\n",
    "\n",
    "**Best Practices for Ethical RL Development:**\n",
    "\n",
    "1. **Diverse Teams**: Include ethicists, domain experts, and affected communities\n",
    "2. **Impact Assessment**: Evaluate potential harms before deployment\n",
    "3. **Monitoring**: Continuously monitor for unintended consequences\n",
    "4. **Feedback Mechanisms**: Allow users to report problems\n",
    "5. **Graceful Degradation**: Design systems that fail safely\n",
    "6. **Documentation**: Maintain clear records of design decisions and their rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deployment'></a>\n",
    "### Deployment Challenges\n",
    "\n",
    "Deploying RL systems in production presents unique challenges beyond traditional ML. This section covers the key obstacles and best practices for successful RL deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges of Deploying RL in Production\n",
    "\n",
    "**Why RL Deployment is Harder Than Supervised Learning:**\n",
    "\n",
    "1. **Online Learning Requirements**\n",
    "   - RL agents often need to continue learning in production\n",
    "   - Balancing exploration with production stability\n",
    "   - Managing the exploration-exploitation trade-off in live systems\n",
    "\n",
    "2. **Safety Constraints**\n",
    "   - Exploration can lead to dangerous or costly actions\n",
    "   - Need for safety bounds and fallback policies\n",
    "   - Human oversight requirements\n",
    "\n",
    "3. **Non-Stationarity**\n",
    "   - Real-world environments change over time\n",
    "   - User behavior evolves\n",
    "   - Concept drift affects policy performance\n",
    "\n",
    "4. **Delayed Feedback**\n",
    "   - Rewards may arrive hours, days, or weeks after actions\n",
    "   - Attribution becomes difficult\n",
    "   - Example: Ad recommendation effects on long-term user engagement\n",
    "\n",
    "5. **Simulation-to-Reality Gap**\n",
    "   - Policies trained in simulation may fail in the real world\n",
    "   - Domain randomization and adaptation techniques needed\n",
    "   - Continuous calibration of simulators\n",
    "\n",
    "**Key Production Requirements:**\n",
    "\n",
    "- **Latency**: Real-time decision making (often <100ms)\n",
    "- **Reliability**: 99.9%+ uptime requirements\n",
    "- **Scalability**: Handling millions of requests\n",
    "- **Reproducibility**: Consistent behavior across deployments\n",
    "- **Auditability**: Logging and explaining decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Pitfalls When Scaling RL Applications\n",
    "\n",
    "**Pitfall 1: Reward Hacking at Scale**\n",
    "\n",
    "As systems scale, reward hacking becomes more likely:\n",
    "- More edge cases encountered\n",
    "- More opportunities to exploit reward function weaknesses\n",
    "- Example: YouTube's recommendation system optimizing for watch time led to promoting conspiracy theories\n",
    "\n",
    "**Pitfall 2: Feedback Loop Amplification**\n",
    "\n",
    "RL systems can create self-reinforcing feedback loops:\n",
    "- Actions influence future data distribution\n",
    "- Biases get amplified over time\n",
    "- Example: Predictive policing creating more arrests in already over-policed areas\n",
    "\n",
    "**Pitfall 3: Catastrophic Forgetting**\n",
    "\n",
    "When updating policies online:\n",
    "- New experiences can overwrite important learned behaviors\n",
    "- Performance on rare but important scenarios may degrade\n",
    "- Need for experience replay and regularization\n",
    "\n",
    "**Pitfall 4: Coordination Failures**\n",
    "\n",
    "Multiple RL agents or systems interacting:\n",
    "- Race conditions in action selection\n",
    "- Emergent behaviors from agent interactions\n",
    "- Example: Multiple trading bots causing flash crashes\n",
    "\n",
    "**Pitfall 5: Infrastructure Complexity**\n",
    "\n",
    "RL systems require complex infrastructure:\n",
    "- Experience collection and storage\n",
    "- Distributed training\n",
    "- Model serving with low latency\n",
    "- A/B testing frameworks\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "\n",
    "1. **Staged Rollouts**: Gradually increase traffic to new policies\n",
    "2. **Canary Deployments**: Test on small user segments first\n",
    "3. **Automatic Rollback**: Revert to previous policy if metrics degrade\n",
    "4. **Diversity Constraints**: Prevent over-optimization on single metrics\n",
    "5. **Regular Audits**: Periodic review of system behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring and Managing RL System Performance\n",
    "\n",
    "**Key Metrics to Monitor:**\n",
    "\n",
    "1. **Reward Metrics**\n",
    "   - Average reward per episode/interaction\n",
    "   - Reward distribution (not just mean)\n",
    "   - Reward trends over time\n",
    "\n",
    "2. **Policy Metrics**\n",
    "   - Action distribution entropy (exploration level)\n",
    "   - Policy divergence from baseline\n",
    "   - Value function estimates\n",
    "\n",
    "3. **Business Metrics**\n",
    "   - Conversion rates, engagement, revenue\n",
    "   - User satisfaction scores\n",
    "   - Long-term user retention\n",
    "\n",
    "4. **Safety Metrics**\n",
    "   - Constraint violation rates\n",
    "   - Fallback policy activation frequency\n",
    "   - Human override frequency\n",
    "\n",
    "5. **System Metrics**\n",
    "   - Inference latency\n",
    "   - Model serving errors\n",
    "   - Data pipeline health\n",
    "\n",
    "**Monitoring Best Practices:**\n",
    "\n",
    "```\n",
    "Monitoring Architecture:\n",
    "\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502   RL Agent      \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Logging Layer  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Metrics DB    \u2502\n",
    "\u2502   (Production)  \u2502     \u2502  (Actions, Obs) \u2502     \u2502  (Time Series)  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                                        \u2502\n",
    "                                                        \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502   Alert System  \u2502\u25c0\u2500\u2500\u2500\u2500\u2502   Dashboards    \u2502\u25c0\u2500\u2500\u2500\u2500\u2502   Analytics     \u2502\n",
    "\u2502   (PagerDuty)   \u2502     \u2502   (Grafana)     \u2502     \u2502   (Anomaly Det) \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Alerting Strategies:**\n",
    "\n",
    "- **Threshold Alerts**: Trigger when metrics exceed bounds\n",
    "- **Anomaly Detection**: ML-based detection of unusual patterns\n",
    "- **Trend Alerts**: Detect gradual degradation\n",
    "- **Comparative Alerts**: Compare against baseline or control group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Robustness in RL\n",
    "\n",
    "**The Adversarial Threat:**\n",
    "\n",
    "RL systems can be vulnerable to adversarial attacks:\n",
    "\n",
    "1. **Observation Perturbations**\n",
    "   - Small changes to inputs cause wrong actions\n",
    "   - Similar to adversarial examples in image classification\n",
    "   - Example: Stickers on road signs fooling autonomous vehicles\n",
    "\n",
    "2. **Reward Poisoning**\n",
    "   - Attackers manipulate the reward signal\n",
    "   - Can cause agent to learn harmful behaviors\n",
    "   - Example: Fake reviews manipulating recommendation systems\n",
    "\n",
    "3. **Environment Manipulation**\n",
    "   - Adversaries modify the environment dynamics\n",
    "   - Exploit agent's learned assumptions\n",
    "   - Example: Market manipulation against trading bots\n",
    "\n",
    "4. **Policy Extraction**\n",
    "   - Attackers reverse-engineer the policy\n",
    "   - Can then find exploits or create competing systems\n",
    "   - Intellectual property concerns\n",
    "\n",
    "**Defense Strategies:**\n",
    "\n",
    "1. **Adversarial Training**\n",
    "   - Train against adversarial perturbations\n",
    "   - Include adversarial examples in training data\n",
    "\n",
    "2. **Robust Optimization**\n",
    "   - Optimize for worst-case performance\n",
    "   - $\\max_\\pi \\min_{\\delta} J(\\pi, \\delta)$ where $\\delta$ is adversarial perturbation\n",
    "\n",
    "3. **Input Validation**\n",
    "   - Detect and reject anomalous inputs\n",
    "   - Sanity checks on observations\n",
    "\n",
    "4. **Ensemble Methods**\n",
    "   - Use multiple policies and aggregate decisions\n",
    "   - Harder to fool all policies simultaneously\n",
    "\n",
    "5. **Certified Defenses**\n",
    "   - Provable bounds on robustness\n",
    "   - Guarantee correct behavior within perturbation bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RL for Data Center Energy Efficiency\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "Data centers consume approximately 1-2% of global electricity. Cooling systems alone can account for 30-40% of a data center's energy consumption. RL offers a promising approach to optimize this.\n",
    "\n",
    "**Google DeepMind's Success:**\n",
    "\n",
    "In 2016, DeepMind applied RL to Google's data center cooling:\n",
    "- **40% reduction** in cooling energy consumption\n",
    "- **15% reduction** in overall PUE (Power Usage Effectiveness)\n",
    "- Saved hundreds of millions of dollars\n",
    "\n",
    "**RL Formulation:**\n",
    "\n",
    "- **State**: Temperature sensors, power consumption, weather, workload\n",
    "- **Actions**: Cooling system settings, airflow adjustments\n",
    "- **Reward**: Negative energy consumption while maintaining safe temperatures\n",
    "\n",
    "**Key Challenges:**\n",
    "\n",
    "1. **Safety Constraints**: Cannot allow temperatures to exceed safe limits\n",
    "2. **Delayed Effects**: Cooling changes take time to propagate\n",
    "3. **Complex Dynamics**: Interactions between many systems\n",
    "4. **Rare Events**: Must handle unusual situations (heat waves, equipment failures)\n",
    "\n",
    "**Implementation Approach:**\n",
    "\n",
    "1. Build accurate simulator from historical data\n",
    "2. Train RL agent in simulation with safety constraints\n",
    "3. Deploy with human oversight and safety bounds\n",
    "4. Continuously improve with real-world data\n",
    "\n",
    "**Broader Applications:**\n",
    "\n",
    "- HVAC optimization in commercial buildings\n",
    "- Smart grid load balancing\n",
    "- Industrial process optimization\n",
    "- Renewable energy integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emerging Trends in RL for Financial Technology\n",
    "\n",
    "**Current Applications:**\n",
    "\n",
    "1. **Algorithmic Trading**\n",
    "   - Portfolio optimization and rebalancing\n",
    "   - Market making and liquidity provision\n",
    "   - Execution optimization (minimizing market impact)\n",
    "\n",
    "2. **Risk Management**\n",
    "   - Dynamic hedging strategies\n",
    "   - Credit risk assessment\n",
    "   - Fraud detection and prevention\n",
    "\n",
    "3. **Personalized Finance**\n",
    "   - Robo-advisors with personalized strategies\n",
    "   - Dynamic pricing and offers\n",
    "   - Customer lifetime value optimization\n",
    "\n",
    "**Emerging Trends:**\n",
    "\n",
    "1. **Multi-Agent Market Simulation**\n",
    "   - Simulating market dynamics with RL agents\n",
    "   - Testing strategies before deployment\n",
    "   - Understanding emergent market behaviors\n",
    "\n",
    "2. **Explainable Financial RL**\n",
    "   - Regulatory requirements for explainability\n",
    "   - Building trust with clients and regulators\n",
    "   - Audit trails for trading decisions\n",
    "\n",
    "3. **Safe Exploration in Finance**\n",
    "   - Constrained RL for risk limits\n",
    "   - Conservative exploration strategies\n",
    "   - Worst-case optimization\n",
    "\n",
    "4. **Transfer Learning Across Markets**\n",
    "   - Leveraging knowledge from one market to another\n",
    "   - Adapting to new financial instruments\n",
    "   - Handling regime changes\n",
    "\n",
    "**Challenges Specific to Finance:**\n",
    "\n",
    "- **Non-Stationarity**: Markets constantly evolve\n",
    "- **Low Signal-to-Noise**: Financial data is noisy\n",
    "- **Adversarial Environment**: Other agents actively compete\n",
    "- **Regulatory Constraints**: Must comply with financial regulations\n",
    "- **Tail Risks**: Must handle rare but catastrophic events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipeline'></a>\n",
    "### End-to-End Deployment Pipeline\n",
    "\n",
    "This section provides a complete pipeline for training, validating, and deploying RL models in production. We'll cover each stage with practical code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Pipeline Overview\n",
    "\n",
    "A production RL pipeline consists of several interconnected stages:\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                        RL Production Pipeline                                \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                              \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n",
    "\u2502  \u2502  Data    \u2502\u2500\u2500\u2500\u25b6\u2502 Training \u2502\u2500\u2500\u2500\u25b6\u2502Validation\u2502\u2500\u2500\u2500\u25b6\u2502Deployment\u2502              \u2502\n",
    "\u2502  \u2502Collection\u2502    \u2502 Pipeline \u2502    \u2502 & Testing\u2502    \u2502 & Serving\u2502              \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n",
    "\u2502       \u2502                                               \u2502                     \u2502\n",
    "\u2502       \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502                     \u2502\n",
    "\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Monitoring & Logging   \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n",
    "\u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                \u2502\n",
    "\u2502                              \u2502                                              \u2502\n",
    "\u2502                              \u25bc                                              \u2502\n",
    "\u2502                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                \u2502\n",
    "\u2502                 \u2502   Feedback & Retraining  \u2502                                \u2502\n",
    "\u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                \u2502\n",
    "\u2502                                                                              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Data Collection**: Gathering experiences from environments or simulations\n",
    "2. **Training Pipeline**: Distributed training with hyperparameter optimization\n",
    "3. **Validation & Testing**: Ensuring policy quality before deployment\n",
    "4. **Deployment & Serving**: Low-latency model serving infrastructure\n",
    "5. **Monitoring & Logging**: Tracking performance and detecting issues\n",
    "6. **Feedback & Retraining**: Continuous improvement from production data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Pipeline Setup\n",
    "\n",
    "A robust training pipeline handles data collection, model training, and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training Pipeline for Production RL\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom collections import deque\nfrom datetime import datetime\nimport json\nimport os\n\nclass TrainingPipeline:\n    \"\"\"\n    A production-ready training pipeline for RL agents.\n    \n    Features:\n    - Checkpointing and model versioning\n    - Metrics logging\n    - Hyperparameter tracking\n    - Early stopping\n    \"\"\"\n    \n    def __init__(self, agent, env, config):\n        \"\"\"\n        Initialize the training pipeline.\n        \n        Args:\n            agent: The RL agent to train\n            env: The training environment\n            config: Training configuration dictionary\n        \"\"\"\n        self.agent = agent\n        self.env = env\n        self.config = config\n        \n        # Training state\n        self.episode = 0\n        self.total_steps = 0\n        self.best_reward = float('-inf')\n        \n        # Metrics tracking\n        self.metrics_history = {\n            'episode_rewards': [],\n            'episode_lengths': [],\n            'losses': [],\n            'timestamps': []\n        }\n        \n        # Setup directories\n        self.checkpoint_dir = config.get('checkpoint_dir', './checkpoints')\n        self.log_dir = config.get('log_dir', './logs')\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        os.makedirs(self.log_dir, exist_ok=True)\n        \n    def train(self, num_episodes, eval_interval=100):\n        \"\"\"\n        Main training loop.\n        \n        Args:\n            num_episodes: Number of episodes to train\n            eval_interval: Episodes between evaluations\n        \"\"\"\n        print(f\"Starting training for {num_episodes} episodes...\")\n        print(f\"Config: {self.config}\")\n        \n        for ep in range(num_episodes):\n            self.episode = ep\n            \n            # Collect episode\n            episode_reward, episode_length = self._run_episode()\n            \n            # Log metrics\n            self._log_metrics(episode_reward, episode_length)\n            \n            # Periodic evaluation and checkpointing\n            if (ep + 1) % eval_interval == 0:\n                eval_reward = self._evaluate()\n                print(f\"Episode {ep+1}/{num_episodes} | \"\n                      f\"Train Reward: {episode_reward:.2f} | \"\n                      f\"Eval Reward: {eval_reward:.2f}\")\n                \n                # Save best model\n                if eval_reward > self.best_reward:\n                    self.best_reward = eval_reward\n                    self._save_checkpoint('best')\n                    \n            # Regular checkpointing\n            if (ep + 1) % (eval_interval * 5) == 0:\n                self._save_checkpoint(f'episode_{ep+1}')\n                \n        # Final save\n        self._save_checkpoint('final')\n        self._save_metrics()\n        print(\"Training complete!\")\n        \n    def _run_episode(self):\n        \"\"\"Run a single training episode.\"\"\"\n        state = self.env.reset()\n        if isinstance(state, tuple):\n            state = state[0]\n        \n        episode_reward = 0\n        episode_length = 0\n        done = False\n        \n        while not done:\n            # Select action\n            action = self.agent.select_action(state)\n            \n            # Take step\n            result = self.env.step(action)\n            if len(result) == 5:\n                next_state, reward, terminated, truncated, _ = result\n                done = terminated or truncated\n            else:\n                next_state, reward, done, _ = result\n            \n            # Update agent\n            if hasattr(self.agent, 'update'):\n                self.agent.update(state, action, reward, next_state, done)\n            \n            state = next_state\n            episode_reward += reward\n            episode_length += 1\n            self.total_steps += 1\n            \n        return episode_reward, episode_length\n    \n    def _evaluate(self, num_episodes=10):\n        \"\"\"Evaluate current policy.\"\"\"\n        rewards = []\n        for _ in range(num_episodes):\n            state = self.env.reset()\n            if isinstance(state, tuple):\n                state = state[0]\n            episode_reward = 0\n            done = False\n            \n            while not done:\n                action = self.agent.select_action(state, explore=False)\n                result = self.env.step(action)\n                if len(result) == 5:\n                    state, reward, terminated, truncated, _ = result\n                    done = terminated or truncated\n                else:\n                    state, reward, done, _ = result\n                episode_reward += reward\n                \n            rewards.append(episode_reward)\n        return np.mean(rewards)\n    \n    def _log_metrics(self, reward, length):\n        \"\"\"Log training metrics.\"\"\"\n        self.metrics_history['episode_rewards'].append(reward)\n        self.metrics_history['episode_lengths'].append(length)\n        self.metrics_history['timestamps'].append(datetime.now().isoformat())\n        \n    def _save_checkpoint(self, name):\n        \"\"\"Save model checkpoint.\"\"\"\n        checkpoint = {\n            'episode': self.episode,\n            'total_steps': self.total_steps,\n            'best_reward': self.best_reward,\n            'config': self.config,\n            'agent_state': self.agent.get_state() if hasattr(self.agent, 'get_state') else None\n        }\n        \n        path = os.path.join(self.checkpoint_dir, f'{name}.pt')\n        torch.save(checkpoint, path)\n        print(f\"Saved checkpoint: {path}\")\n        \n    def _save_metrics(self):\n        \"\"\"Save training metrics to JSON.\"\"\"\n        path = os.path.join(self.log_dir, 'metrics.json')\n        with open(path, 'w') as f:\n            json.dump(self.metrics_history, f, indent=2)\n        print(f\"Saved metrics: {path}\")\n\n\n# Example usage\nprint(\"Training Pipeline class defined!\")\nprint(\"\\nKey features:\")\nprint(\"  - Automatic checkpointing of best and periodic models\")\nprint(\"  - Metrics logging with timestamps\")\nprint(\"  - Configurable evaluation intervals\")\nprint(\"  - Support for both old and new Gym API\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Strategies\n",
    "\n",
    "Before deploying an RL model, thorough validation is essential to ensure safe and effective behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Validation Strategies for RL Models\nimport numpy as np\nfrom typing import List, Dict, Callable\n\nclass RLValidator:\n    \"\"\"\n    Comprehensive validation suite for RL policies.\n    \n    Validates:\n    - Performance metrics\n    - Safety constraints\n    - Robustness to perturbations\n    - Edge case handling\n    \"\"\"\n    \n    def __init__(self, env, policy):\n        self.env = env\n        self.policy = policy\n        self.results = {}\n        \n    def validate_performance(self, num_episodes=100, min_reward=None):\n        \"\"\"\n        Validate policy performance meets minimum requirements.\n        \n        Args:\n            num_episodes: Number of evaluation episodes\n            min_reward: Minimum acceptable average reward\n        \"\"\"\n        rewards = []\n        for _ in range(num_episodes):\n            state = self.env.reset()\n            if isinstance(state, tuple):\n                state = state[0]\n            episode_reward = 0\n            done = False\n            \n            while not done:\n                action = self.policy(state)\n                result = self.env.step(action)\n                if len(result) == 5:\n                    state, reward, terminated, truncated, _ = result\n                    done = terminated or truncated\n                else:\n                    state, reward, done, _ = result\n                episode_reward += reward\n                \n            rewards.append(episode_reward)\n        \n        avg_reward = np.mean(rewards)\n        std_reward = np.std(rewards)\n        \n        self.results['performance'] = {\n            'mean_reward': avg_reward,\n            'std_reward': std_reward,\n            'min_reward': np.min(rewards),\n            'max_reward': np.max(rewards),\n            'passed': min_reward is None or avg_reward >= min_reward\n        }\n        \n        return self.results['performance']\n    \n    def validate_safety(self, constraint_fn: Callable, num_episodes=100, max_violations=0):\n        \"\"\"\n        Validate policy satisfies safety constraints.\n        \n        Args:\n            constraint_fn: Function(state, action) -> bool (True if safe)\n            num_episodes: Number of episodes to test\n            max_violations: Maximum allowed constraint violations\n        \"\"\"\n        total_violations = 0\n        total_steps = 0\n        \n        for _ in range(num_episodes):\n            state = self.env.reset()\n            if isinstance(state, tuple):\n                state = state[0]\n            done = False\n            \n            while not done:\n                action = self.policy(state)\n                \n                # Check constraint\n                if not constraint_fn(state, action):\n                    total_violations += 1\n                \n                result = self.env.step(action)\n                if len(result) == 5:\n                    state, _, terminated, truncated, _ = result\n                    done = terminated or truncated\n                else:\n                    state, _, done, _ = result\n                total_steps += 1\n        \n        violation_rate = total_violations / total_steps if total_steps > 0 else 0\n        \n        self.results['safety'] = {\n            'total_violations': total_violations,\n            'total_steps': total_steps,\n            'violation_rate': violation_rate,\n            'passed': total_violations <= max_violations\n        }\n        \n        return self.results['safety']\n    \n    def validate_robustness(self, noise_levels=[0.01, 0.05, 0.1], num_episodes=50):\n        \"\"\"\n        Test policy robustness to observation noise.\n        \n        Args:\n            noise_levels: List of noise standard deviations to test\n            num_episodes: Episodes per noise level\n        \"\"\"\n        robustness_results = {}\n        \n        for noise in noise_levels:\n            rewards = []\n            for _ in range(num_episodes):\n                state = self.env.reset()\n                if isinstance(state, tuple):\n                    state = state[0]\n                episode_reward = 0\n                done = False\n                \n                while not done:\n                    # Add noise to observation\n                    noisy_state = state + np.random.normal(0, noise, state.shape)\n                    action = self.policy(noisy_state)\n                    \n                    result = self.env.step(action)\n                    if len(result) == 5:\n                        state, reward, terminated, truncated, _ = result\n                        done = terminated or truncated\n                    else:\n                        state, reward, done, _ = result\n                    episode_reward += reward\n                    \n                rewards.append(episode_reward)\n            \n            robustness_results[f'noise_{noise}'] = {\n                'mean_reward': np.mean(rewards),\n                'std_reward': np.std(rewards)\n            }\n        \n        self.results['robustness'] = robustness_results\n        return robustness_results\n    \n    def generate_report(self):\n        \"\"\"Generate validation report.\"\"\"\n        report = \"=\" * 60 + \"\\n\"\n        report += \"RL Policy Validation Report\\n\"\n        report += \"=\" * 60 + \"\\n\\n\"\n        \n        for test_name, results in self.results.items():\n            report += f\"### {test_name.upper()} ###\\n\"\n            if isinstance(results, dict):\n                for key, value in results.items():\n                    report += f\"  {key}: {value}\\n\"\n            report += \"\\n\"\n        \n        return report\n\n\n# Example usage demonstration\nprint(\"RLValidator class defined!\")\nprint(\"\\nValidation capabilities:\")\nprint(\"  - Performance validation against minimum thresholds\")\nprint(\"  - Safety constraint checking\")\nprint(\"  - Robustness testing with observation noise\")\nprint(\"  - Comprehensive report generation\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Serialization and Loading\n",
    "\n",
    "Proper model serialization is crucial for reproducibility and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model Serialization for RL Policies\nimport torch\nimport torch.nn as nn\nimport json\nimport hashlib\nfrom datetime import datetime\n\nclass PolicySerializer:\n    \"\"\"\n    Handles serialization and loading of RL policies with metadata.\n    \n    Features:\n    - Version tracking\n    - Metadata preservation\n    - Integrity verification\n    - Format compatibility\n    \"\"\"\n    \n    @staticmethod\n    def save_policy(policy_network, path, metadata=None):\n        \"\"\"\n        Save a policy network with metadata.\n        \n        Args:\n            policy_network: PyTorch nn.Module\n            path: Save path\n            metadata: Optional metadata dictionary\n        \"\"\"\n        # Prepare metadata\n        save_metadata = {\n            'timestamp': datetime.now().isoformat(),\n            'pytorch_version': torch.__version__,\n            'architecture': str(policy_network),\n            'num_parameters': sum(p.numel() for p in policy_network.parameters()),\n        }\n        \n        if metadata:\n            save_metadata.update(metadata)\n        \n        # Create checkpoint\n        checkpoint = {\n            'model_state_dict': policy_network.state_dict(),\n            'metadata': save_metadata\n        }\n        \n        # Save\n        torch.save(checkpoint, path)\n        \n        # Calculate and store hash for integrity\n        with open(path, 'rb') as f:\n            file_hash = hashlib.md5(f.read()).hexdigest()\n        \n        # Save metadata separately for quick access\n        meta_path = path.replace('.pt', '_meta.json')\n        save_metadata['file_hash'] = file_hash\n        with open(meta_path, 'w') as f:\n            json.dump(save_metadata, f, indent=2)\n        \n        print(f\"Saved policy to {path}\")\n        print(f\"  Parameters: {save_metadata['num_parameters']:,}\")\n        print(f\"  Hash: {file_hash}\")\n        \n        return file_hash\n    \n    @staticmethod\n    def load_policy(policy_network, path, verify_hash=True):\n        \"\"\"\n        Load a policy network with optional integrity verification.\n        \n        Args:\n            policy_network: PyTorch nn.Module (architecture must match)\n            path: Load path\n            verify_hash: Whether to verify file integrity\n        \"\"\"\n        # Load checkpoint\n        checkpoint = torch.load(path, map_location='cpu')\n        \n        # Verify integrity if requested\n        if verify_hash:\n            meta_path = path.replace('.pt', '_meta.json')\n            try:\n                with open(meta_path, 'r') as f:\n                    saved_meta = json.load(f)\n                \n                with open(path, 'rb') as f:\n                    current_hash = hashlib.md5(f.read()).hexdigest()\n                \n                if current_hash != saved_meta.get('file_hash'):\n                    raise ValueError(\"File integrity check failed!\")\n                print(\"Integrity verified \u2713\")\n            except FileNotFoundError:\n                print(\"Warning: Metadata file not found, skipping integrity check\")\n        \n        # Load state dict\n        policy_network.load_state_dict(checkpoint['model_state_dict'])\n        \n        print(f\"Loaded policy from {path}\")\n        print(f\"  Saved: {checkpoint['metadata'].get('timestamp', 'Unknown')}\")\n        \n        return checkpoint['metadata']\n    \n    @staticmethod\n    def export_for_inference(policy_network, path, example_input):\n        \"\"\"\n        Export policy for optimized inference (TorchScript).\n        \n        Args:\n            policy_network: PyTorch nn.Module\n            path: Export path\n            example_input: Example input tensor for tracing\n        \"\"\"\n        policy_network.eval()\n        \n        # Trace the model\n        traced = torch.jit.trace(policy_network, example_input)\n        \n        # Save traced model\n        traced.save(path)\n        print(f\"Exported TorchScript model to {path}\")\n        \n        return traced\n\n\n# Example policy network for demonstration\nclass SimplePolicyNetwork(nn.Module):\n    def __init__(self, state_dim=4, action_dim=2, hidden_dim=64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Softmax(dim=-1)\n        )\n    \n    def forward(self, x):\n        return self.network(x)\n\n\n# Demonstrate serialization\nprint(\"PolicySerializer class defined!\")\nprint(\"\\nSerialization features:\")\nprint(\"  - Metadata preservation (timestamp, architecture, parameters)\")\nprint(\"  - Integrity verification via MD5 hash\")\nprint(\"  - TorchScript export for optimized inference\")\nprint(\"\\nExample usage:\")\nprint(\"  PolicySerializer.save_policy(model, 'policy.pt', {'version': '1.0'})\")\nprint(\"  PolicySerializer.load_policy(model, 'policy.pt')\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment Architecture Considerations\n",
    "\n",
    "A production RL deployment requires careful architectural planning for reliability and scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Deployment Architecture for RL Systems\nimport torch\nimport numpy as np\nfrom typing import Dict, Any\nfrom collections import deque\nimport time\n\nclass RLServingSystem:\n    \"\"\"\n    Production serving system for RL policies.\n    \n    Features:\n    - Low-latency inference\n    - Request batching\n    - Fallback policies\n    - A/B testing support\n    \"\"\"\n    \n    def __init__(self, primary_policy, fallback_policy=None):\n        \"\"\"\n        Initialize serving system.\n        \n        Args:\n            primary_policy: Main policy model\n            fallback_policy: Backup policy for failures\n        \"\"\"\n        self.primary_policy = primary_policy\n        self.fallback_policy = fallback_policy\n        self.primary_policy.eval()\n        \n        # Metrics\n        self.request_count = 0\n        self.fallback_count = 0\n        self.latencies = deque(maxlen=1000)\n        \n        # A/B testing\n        self.ab_policies = {}\n        self.ab_traffic_split = {}\n        \n    def predict(self, state: np.ndarray, experiment_id: str = None) -> Dict[str, Any]:\n        \"\"\"\n        Get action prediction for a state.\n        \n        Args:\n            state: Environment state\n            experiment_id: Optional A/B test experiment ID\n            \n        Returns:\n            Dictionary with action and metadata\n        \"\"\"\n        start_time = time.time()\n        self.request_count += 1\n        \n        try:\n            # Select policy (A/B testing)\n            if experiment_id and experiment_id in self.ab_policies:\n                policy = self._select_ab_policy(experiment_id)\n                policy_name = f\"ab_{experiment_id}\"\n            else:\n                policy = self.primary_policy\n                policy_name = \"primary\"\n            \n            # Convert to tensor\n            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n            \n            # Inference\n            with torch.no_grad():\n                action_probs = policy(state_tensor)\n                action = torch.argmax(action_probs, dim=-1).item()\n            \n            latency = time.time() - start_time\n            self.latencies.append(latency)\n            \n            return {\n                'action': action,\n                'action_probs': action_probs.numpy().tolist(),\n                'policy': policy_name,\n                'latency_ms': latency * 1000,\n                'status': 'success'\n            }\n            \n        except Exception as e:\n            # Fallback to backup policy\n            self.fallback_count += 1\n            \n            if self.fallback_policy is not None:\n                action = self.fallback_policy(state)\n                return {\n                    'action': action,\n                    'policy': 'fallback',\n                    'status': 'fallback',\n                    'error': str(e)\n                }\n            else:\n                return {\n                    'action': 0,  # Default action\n                    'policy': 'default',\n                    'status': 'error',\n                    'error': str(e)\n                }\n    \n    def register_ab_policy(self, experiment_id: str, policy, traffic_fraction: float):\n        \"\"\"Register a policy for A/B testing.\"\"\"\n        self.ab_policies[experiment_id] = policy\n        self.ab_traffic_split[experiment_id] = traffic_fraction\n        policy.eval()\n        print(f\"Registered A/B policy: {experiment_id} ({traffic_fraction*100}% traffic)\")\n    \n    def _select_ab_policy(self, experiment_id: str):\n        \"\"\"Select policy based on traffic split.\"\"\"\n        if np.random.random() < self.ab_traffic_split[experiment_id]:\n            return self.ab_policies[experiment_id]\n        return self.primary_policy\n    \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get serving metrics.\"\"\"\n        return {\n            'total_requests': self.request_count,\n            'fallback_rate': self.fallback_count / max(1, self.request_count),\n            'avg_latency_ms': np.mean(self.latencies) * 1000 if self.latencies else 0,\n            'p99_latency_ms': np.percentile(self.latencies, 99) * 1000 if self.latencies else 0,\n        }\n\n\nprint(\"RLServingSystem class defined!\")\nprint(\"\\nDeployment features:\")\nprint(\"  - Low-latency inference with batching support\")\nprint(\"  - Automatic fallback to backup policy on errors\")\nprint(\"  - A/B testing with configurable traffic splits\")\nprint(\"  - Real-time latency and error tracking\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring Setup\n",
    "\n",
    "Comprehensive monitoring is essential for maintaining RL system health in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Monitoring System for Production RL\nimport numpy as np\nfrom collections import deque\nfrom datetime import datetime\nimport json\n\nclass RLMonitor:\n    \"\"\"\n    Monitoring system for production RL deployments.\n    \n    Tracks:\n    - Reward metrics\n    - Policy behavior\n    - System health\n    - Anomaly detection\n    \"\"\"\n    \n    def __init__(self, window_size=1000, alert_thresholds=None):\n        \"\"\"\n        Initialize monitoring system.\n        \n        Args:\n            window_size: Size of rolling window for metrics\n            alert_thresholds: Dictionary of metric thresholds for alerts\n        \"\"\"\n        self.window_size = window_size\n        self.alert_thresholds = alert_thresholds or {}\n        \n        # Metric buffers\n        self.rewards = deque(maxlen=window_size)\n        self.actions = deque(maxlen=window_size)\n        self.latencies = deque(maxlen=window_size)\n        self.errors = deque(maxlen=window_size)\n        \n        # Baseline statistics (set during calibration)\n        self.baseline_reward_mean = None\n        self.baseline_reward_std = None\n        \n        # Alert history\n        self.alerts = []\n        \n    def log_interaction(self, state, action, reward, latency_ms, error=None):\n        \"\"\"Log a single interaction.\"\"\"\n        self.rewards.append(reward)\n        self.actions.append(action)\n        self.latencies.append(latency_ms)\n        self.errors.append(1 if error else 0)\n        \n        # Check for anomalies\n        self._check_alerts()\n    \n    def calibrate_baseline(self, rewards):\n        \"\"\"Set baseline statistics from historical data.\"\"\"\n        self.baseline_reward_mean = np.mean(rewards)\n        self.baseline_reward_std = np.std(rewards)\n        print(f\"Baseline calibrated: mean={self.baseline_reward_mean:.3f}, std={self.baseline_reward_std:.3f}\")\n    \n    def _check_alerts(self):\n        \"\"\"Check for alert conditions.\"\"\"\n        if len(self.rewards) < 100:\n            return\n        \n        # Reward degradation alert\n        if self.baseline_reward_mean is not None:\n            recent_mean = np.mean(list(self.rewards)[-100:])\n            threshold = self.alert_thresholds.get('reward_degradation', 2.0)\n            \n            if recent_mean < self.baseline_reward_mean - threshold * self.baseline_reward_std:\n                self._raise_alert('reward_degradation', \n                    f'Reward dropped to {recent_mean:.3f} (baseline: {self.baseline_reward_mean:.3f})')\n        \n        # High error rate alert\n        error_rate = np.mean(list(self.errors)[-100:])\n        threshold = self.alert_thresholds.get('error_rate', 0.05)\n        if error_rate > threshold:\n            self._raise_alert('high_error_rate', f'Error rate: {error_rate:.2%}')\n        \n        # High latency alert\n        p99_latency = np.percentile(list(self.latencies)[-100:], 99)\n        threshold = self.alert_thresholds.get('latency_p99_ms', 100)\n        if p99_latency > threshold:\n            self._raise_alert('high_latency', f'P99 latency: {p99_latency:.1f}ms')\n    \n    def _raise_alert(self, alert_type, message):\n        \"\"\"Raise an alert.\"\"\"\n        alert = {\n            'type': alert_type,\n            'message': message,\n            'timestamp': datetime.now().isoformat()\n        }\n        self.alerts.append(alert)\n        print(f\"\u26a0\ufe0f  ALERT [{alert_type}]: {message}\")\n    \n    def get_dashboard_metrics(self):\n        \"\"\"Get metrics for dashboard display.\"\"\"\n        if not self.rewards:\n            return {}\n        \n        return {\n            'reward': {\n                'current': self.rewards[-1] if self.rewards else None,\n                'mean': np.mean(self.rewards),\n                'std': np.std(self.rewards),\n                'min': np.min(self.rewards),\n                'max': np.max(self.rewards)\n            },\n            'latency': {\n                'mean_ms': np.mean(self.latencies),\n                'p50_ms': np.percentile(self.latencies, 50),\n                'p99_ms': np.percentile(self.latencies, 99)\n            },\n            'errors': {\n                'rate': np.mean(self.errors),\n                'total': sum(self.errors)\n            },\n            'actions': {\n                'distribution': dict(zip(*np.unique(self.actions, return_counts=True)))\n            },\n            'alerts': {\n                'recent': self.alerts[-5:] if self.alerts else [],\n                'total': len(self.alerts)\n            }\n        }\n    \n    def export_report(self, path):\n        \"\"\"Export monitoring report to JSON.\"\"\"\n        report = {\n            'generated_at': datetime.now().isoformat(),\n            'metrics': self.get_dashboard_metrics(),\n            'baseline': {\n                'reward_mean': self.baseline_reward_mean,\n                'reward_std': self.baseline_reward_std\n            },\n            'all_alerts': self.alerts\n        }\n        \n        with open(path, 'w') as f:\n            json.dump(report, f, indent=2, default=str)\n        print(f\"Exported report to {path}\")\n\n\nprint(\"RLMonitor class defined!\")\nprint(\"\\nMonitoring capabilities:\")\nprint(\"  - Rolling window metrics for rewards, latency, errors\")\nprint(\"  - Baseline calibration for anomaly detection\")\nprint(\"  - Automatic alerting on degradation\")\nprint(\"  - Dashboard-ready metrics export\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maintenance and Updates\n",
    "\n",
    "**Continuous Improvement Cycle:**\n",
    "\n",
    "1. **Data Collection**: Continuously gather production experiences\n",
    "2. **Offline Evaluation**: Test new policies on historical data\n",
    "3. **A/B Testing**: Gradually roll out improvements\n",
    "4. **Monitoring**: Track performance and detect regressions\n",
    "5. **Rollback**: Quickly revert if issues arise\n",
    "\n",
    "**Best Practices for Updates:**\n",
    "\n",
    "- **Version Control**: Track all model versions with metadata\n",
    "- **Staged Rollouts**: Start with small traffic percentage\n",
    "- **Canary Deployments**: Test on subset of users first\n",
    "- **Feature Flags**: Enable quick rollback without redeployment\n",
    "- **Shadow Mode**: Run new policy alongside old one without affecting users\n",
    "\n",
    "**Retraining Triggers:**\n",
    "\n",
    "- Performance degradation below threshold\n",
    "- Significant distribution shift in inputs\n",
    "- New data availability\n",
    "- Scheduled periodic retraining\n",
    "- Business requirement changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recent-research'></a>\n",
    "### Recent Research Highlights\n",
    "\n",
    "This section highlights significant recent advances in reinforcement learning from top conferences like NeurIPS, ICML, and ICLR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notable Papers from NeurIPS and ICML\n",
    "\n",
    "**Foundation Models for Decision Making (2023-2024)**\n",
    "\n",
    "The intersection of large language models and RL has produced exciting results:\n",
    "\n",
    "- **Decision Transformer** (Chen et al., NeurIPS 2021)\n",
    "  - Frames RL as sequence modeling\n",
    "  - Uses transformer architecture to predict actions\n",
    "  - Achieves strong results without traditional RL training\n",
    "\n",
    "- **Gato** (Reed et al., 2022)\n",
    "  - Single generalist agent for multiple tasks\n",
    "  - Plays games, controls robots, chats\n",
    "  - Demonstrates potential of multi-task RL\n",
    "\n",
    "- **RT-2** (Brohan et al., 2023)\n",
    "  - Vision-Language-Action model for robotics\n",
    "  - Transfers web knowledge to robot control\n",
    "  - Enables zero-shot generalization to new tasks\n",
    "\n",
    "**Offline Reinforcement Learning**\n",
    "\n",
    "Learning from fixed datasets without environment interaction:\n",
    "\n",
    "- **Conservative Q-Learning (CQL)** (Kumar et al., NeurIPS 2020)\n",
    "  - Addresses overestimation in offline RL\n",
    "  - Learns conservative value estimates\n",
    "  - Widely adopted baseline for offline RL\n",
    "\n",
    "- **Implicit Q-Learning (IQL)** (Kostrikov et al., ICLR 2022)\n",
    "  - Avoids querying out-of-distribution actions\n",
    "  - Simple and effective approach\n",
    "  - Strong performance across benchmarks\n",
    "\n",
    "- **Decision Diffuser** (Ajay et al., ICML 2023)\n",
    "  - Uses diffusion models for trajectory generation\n",
    "  - Flexible conditioning on rewards and constraints\n",
    "  - State-of-the-art on several benchmarks\n",
    "\n",
    "**Sample Efficiency Improvements**\n",
    "\n",
    "- **DreamerV3** (Hafner et al., 2023)\n",
    "  - World model-based RL\n",
    "  - Masters diverse domains with single algorithm\n",
    "  - Achieves human-level Minecraft diamond collection\n",
    "\n",
    "- **IRIS** (Micheli et al., ICML 2023)\n",
    "  - Combines transformers with world models\n",
    "  - Efficient imagination-based planning\n",
    "  - Strong Atari performance with limited data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Innovations and Techniques\n",
    "\n",
    "**1. Representation Learning for RL**\n",
    "\n",
    "Learning good state representations is crucial for sample efficiency:\n",
    "\n",
    "- **Contrastive Learning**: CURL, DrQ use data augmentation and contrastive objectives\n",
    "- **Masked Prediction**: MLR, MWM predict masked portions of observations\n",
    "- **Self-Supervised Objectives**: Auxiliary tasks improve representation quality\n",
    "\n",
    "**2. Exploration Advances**\n",
    "\n",
    "Better exploration strategies for sparse reward environments:\n",
    "\n",
    "- **Intrinsic Motivation**: Curiosity-driven exploration (ICM, RND)\n",
    "- **Count-Based Methods**: Pseudo-counts for novelty estimation\n",
    "- **Information Gain**: Maximize information about environment dynamics\n",
    "- **Go-Explore**: First return, then explore paradigm\n",
    "\n",
    "**3. Hierarchical and Goal-Conditioned RL**\n",
    "\n",
    "Decomposing complex tasks into manageable subproblems:\n",
    "\n",
    "- **Goal-Conditioned Policies**: Learn to reach arbitrary goals\n",
    "- **Hindsight Experience Replay (HER)**: Learn from failures by relabeling goals\n",
    "- **Skill Discovery**: Automatically discover reusable skills (DIAYN, VIC)\n",
    "\n",
    "**4. Multi-Task and Transfer Learning**\n",
    "\n",
    "Leveraging knowledge across tasks:\n",
    "\n",
    "- **Distillation**: Compress multiple policies into one\n",
    "- **Progressive Networks**: Prevent catastrophic forgetting\n",
    "- **Successor Features**: Generalize across reward functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources for Staying Current\n",
    "\n",
    "**Top Conferences:**\n",
    "- NeurIPS (Neural Information Processing Systems)\n",
    "- ICML (International Conference on Machine Learning)\n",
    "- ICLR (International Conference on Learning Representations)\n",
    "- AAAI (Association for the Advancement of Artificial Intelligence)\n",
    "- CoRL (Conference on Robot Learning)\n",
    "\n",
    "**Key Research Groups:**\n",
    "- DeepMind (AlphaGo, AlphaStar, Gato)\n",
    "- OpenAI (GPT, DALL-E, RLHF)\n",
    "- Google Brain / Google DeepMind\n",
    "- Meta AI (FAIR)\n",
    "- UC Berkeley (BAIR)\n",
    "- Stanford AI Lab\n",
    "\n",
    "**Useful Resources:**\n",
    "- [Papers With Code - RL](https://paperswithcode.com/area/reinforcement-learning) - Benchmarks and implementations\n",
    "- [Spinning Up in Deep RL](https://spinningup.openai.com/) - OpenAI's educational resource\n",
    "- [RL Weekly](https://www.endtoend.ai/rl-weekly/) - Weekly newsletter on RL research\n",
    "- [The RL Discord](https://discord.gg/xhfNqQv) - Community discussions\n",
    "- [arXiv cs.LG](https://arxiv.org/list/cs.LG/recent) - Latest preprints\n",
    "\n",
    "**Benchmark Environments:**\n",
    "- OpenAI Gym / Gymnasium - Standard RL benchmarks\n",
    "- MuJoCo - Physics simulation for robotics\n",
    "- Atari - Classic game benchmarks\n",
    "- ProcGen - Procedurally generated environments\n",
    "- Meta-World - Multi-task robotics benchmark\n",
    "- D4RL - Offline RL datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Directions\n",
    "\n",
    "**Emerging Research Areas:**\n",
    "\n",
    "1. **Foundation Models for RL**\n",
    "   - Pre-trained models that transfer across tasks\n",
    "   - Language-conditioned policies\n",
    "   - Vision-language-action models\n",
    "\n",
    "2. **Real-World Robotics**\n",
    "   - Sim-to-real transfer at scale\n",
    "   - Learning from human demonstrations\n",
    "   - Safe exploration in physical systems\n",
    "\n",
    "3. **Human-AI Collaboration**\n",
    "   - Learning from human feedback (RLHF)\n",
    "   - Interactive learning and teaching\n",
    "   - Shared autonomy systems\n",
    "\n",
    "4. **Scalable Multi-Agent Systems**\n",
    "   - Population-based training\n",
    "   - Emergent communication\n",
    "   - Large-scale coordination\n",
    "\n",
    "5. **Theoretical Foundations**\n",
    "   - Sample complexity bounds\n",
    "   - Generalization theory for RL\n",
    "   - Provably efficient algorithms\n",
    "\n",
    "**Open Challenges:**\n",
    "\n",
    "- **Sample Efficiency**: Still require millions of interactions for complex tasks\n",
    "- **Generalization**: Policies often fail on slight environment changes\n",
    "- **Safety**: Ensuring safe behavior during learning and deployment\n",
    "- **Interpretability**: Understanding why agents make decisions\n",
    "- **Reward Specification**: Defining rewards that capture true objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## Conclusion and Next Steps\n",
    "\n",
    "Congratulations on completing this comprehensive journey through Reinforcement Learning! You've covered an extensive range of topics, from foundational concepts to cutting-edge research and real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Key Concepts\n",
    "\n",
    "**Section 1: Foundational Concepts**\n",
    "- The RL paradigm: agents learning through interaction with environments\n",
    "- Multi-Armed Bandits and the exploration-exploitation trade-off\n",
    "- Markov Decision Processes (MDPs) as the mathematical framework for RL\n",
    "- Value functions $V(s)$ and $Q(s,a)$ for evaluating states and actions\n",
    "- The Bellman equations as the foundation for value-based methods\n",
    "- Dynamic Programming: Policy Evaluation, Policy Improvement, and Value Iteration\n",
    "\n",
    "**Section 2: Core Algorithms**\n",
    "- Monte Carlo methods: learning from complete episodes\n",
    "- Temporal Difference learning: bootstrapping for faster learning\n",
    "- Q-Learning: the foundational off-policy algorithm\n",
    "- Deep Q-Networks (DQN): combining neural networks with Q-learning\n",
    "- Policy Gradient methods: directly optimizing policies\n",
    "- Actor-Critic architectures: combining value and policy methods\n",
    "\n",
    "**Section 3: Advanced Topics**\n",
    "- Reward engineering and shaping\n",
    "- Function approximation for large state spaces\n",
    "- Transfer learning and generalization\n",
    "- Eligibility traces and TRPO\n",
    "- Hierarchical RL and inverse RL\n",
    "\n",
    "**Section 5: Real-World Applications**\n",
    "- Traffic signal optimization\n",
    "- Robotics and sim-to-real transfer\n",
    "- Autonomous trading systems\n",
    "- Recommendation engines\n",
    "- Healthcare treatment optimization\n",
    "- Game playing AI\n",
    "\n",
    "**Section 6: Research & Deployment**\n",
    "- Current research trends in multi-agent RL, meta-learning, and safe RL\n",
    "- Ethical considerations and alignment challenges\n",
    "- Production deployment pipelines and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Next Steps\n",
    "\n",
    "**For Beginners:**\n",
    "1. **Practice with OpenAI Gym**: Implement the algorithms from this notebook on different environments\n",
    "2. **Experiment with hyperparameters**: Understand how learning rate, discount factor, and exploration affect learning\n",
    "3. **Read Sutton & Barto**: The textbook \"Reinforcement Learning: An Introduction\" provides deeper theoretical foundations\n",
    "4. **Join the community**: Participate in RL Discord, Reddit r/reinforcementlearning, and Stack Overflow\n",
    "\n",
    "**For Intermediate Learners:**\n",
    "1. **Implement PPO and SAC**: These are the most widely-used algorithms in practice\n",
    "2. **Try MuJoCo environments**: Continuous control tasks provide new challenges\n",
    "3. **Explore offline RL**: Learn from fixed datasets without environment interaction\n",
    "4. **Study multi-agent RL**: Extend your knowledge to competitive and cooperative settings\n",
    "\n",
    "**For Advanced Practitioners:**\n",
    "1. **Read recent papers**: Follow NeurIPS, ICML, and ICLR proceedings\n",
    "2. **Contribute to open-source**: Libraries like Stable-Baselines3, RLlib, and CleanRL welcome contributions\n",
    "3. **Apply RL to real problems**: Identify opportunities in your domain\n",
    "4. **Explore research frontiers**: Foundation models, world models, and sample-efficient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources and References\n",
    "\n",
    "**Essential Textbooks:**\n",
    "- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.) - [Free online](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "- Bertsekas, D. P. (2019). *Reinforcement Learning and Optimal Control*\n",
    "- Szepesv\u00e1ri, C. (2010). *Algorithms for Reinforcement Learning*\n",
    "\n",
    "**Online Courses:**\n",
    "- [David Silver's RL Course](https://www.davidsilver.uk/teaching/) - DeepMind's foundational course\n",
    "- [Berkeley CS285](http://rail.eecs.berkeley.edu/deeprlcourse/) - Deep RL course by Sergey Levine\n",
    "- [Stanford CS234](https://web.stanford.edu/class/cs234/) - Reinforcement Learning course\n",
    "- [Spinning Up in Deep RL](https://spinningup.openai.com/) - OpenAI's practical guide\n",
    "\n",
    "**Libraries and Frameworks:**\n",
    "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/) - Reliable implementations of RL algorithms\n",
    "- [RLlib](https://docs.ray.io/en/latest/rllib/) - Scalable RL library from Ray\n",
    "- [CleanRL](https://github.com/vwxyzjn/cleanrl) - Single-file implementations for learning\n",
    "- [Gymnasium](https://gymnasium.farama.org/) - Standard RL environments (successor to OpenAI Gym)\n",
    "- [TorchRL](https://pytorch.org/rl/) - PyTorch's official RL library\n",
    "\n",
    "**Research Resources:**\n",
    "- [Papers With Code - RL](https://paperswithcode.com/area/reinforcement-learning) - Benchmarks and implementations\n",
    "- [arXiv cs.LG](https://arxiv.org/list/cs.LG/recent) - Latest preprints\n",
    "- [OpenReview](https://openreview.net/) - Conference paper reviews and discussions\n",
    "\n",
    "**Community:**\n",
    "- [RL Discord](https://discord.gg/xhfNqQv) - Active community discussions\n",
    "- [Reddit r/reinforcementlearning](https://www.reddit.com/r/reinforcementlearning/) - News and discussions\n",
    "- [RL Weekly Newsletter](https://www.endtoend.ai/rl-weekly/) - Curated research updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "Reinforcement Learning represents one of the most exciting frontiers in artificial intelligence. From mastering complex games to optimizing real-world systems, RL continues to push the boundaries of what machines can learn to do.\n",
    "\n",
    "The field is evolving rapidly, with new algorithms, applications, and theoretical insights emerging regularly. The foundations you've built in this notebook will serve you well as you continue to explore and contribute to this dynamic field.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- RL is about learning optimal behavior through interaction\n",
    "- The exploration-exploitation trade-off is fundamental\n",
    "- Value-based and policy-based methods each have their strengths\n",
    "- Deep learning has dramatically expanded RL's capabilities\n",
    "- Real-world deployment requires careful consideration of safety and ethics\n",
    "\n",
    "Thank you for completing this notebook. Happy learning, and may your agents always find the optimal policy! \ud83c\udfaf\ud83e\udd16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}