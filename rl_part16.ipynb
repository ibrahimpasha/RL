{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Zero to Hero - Part 16/17\n",
    "\n",
    "**Cells 261-280 of 291**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges of Deploying RL in Production\n",
    "\n",
    "**Why RL Deployment is Harder Than Supervised Learning:**\n",
    "\n",
    "1. **Online Learning Requirements**\n",
    "   - RL agents often need to continue learning in production\n",
    "   - Balancing exploration with production stability\n",
    "   - Managing the exploration-exploitation trade-off in live systems\n",
    "\n",
    "2. **Safety Constraints**\n",
    "   - Exploration can lead to dangerous or costly actions\n",
    "   - Need for safety bounds and fallback policies\n",
    "   - Human oversight requirements\n",
    "\n",
    "3. **Non-Stationarity**\n",
    "   - Real-world environments change over time\n",
    "   - User behavior evolves\n",
    "   - Concept drift affects policy performance\n",
    "\n",
    "4. **Delayed Feedback**\n",
    "   - Rewards may arrive hours, days, or weeks after actions\n",
    "   - Attribution becomes difficult\n",
    "   - Example: Ad recommendation effects on long-term user engagement\n",
    "\n",
    "5. **Simulation-to-Reality Gap**\n",
    "   - Policies trained in simulation may fail in the real world\n",
    "   - Domain randomization and adaptation techniques needed\n",
    "   - Continuous calibration of simulators\n",
    "\n",
    "**Key Production Requirements:**\n",
    "\n",
    "- **Latency**: Real-time decision making (often <100ms)\n",
    "- **Reliability**: 99.9%+ uptime requirements\n",
    "- **Scalability**: Handling millions of requests\n",
    "- **Reproducibility**: Consistent behavior across deployments\n",
    "- **Auditability**: Logging and explaining decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Pitfalls When Scaling RL Applications\n",
    "\n",
    "**Pitfall 1: Reward Hacking at Scale**\n",
    "\n",
    "As systems scale, reward hacking becomes more likely:\n",
    "- More edge cases encountered\n",
    "- More opportunities to exploit reward function weaknesses\n",
    "- Example: YouTube's recommendation system optimizing for watch time led to promoting conspiracy theories\n",
    "\n",
    "**Pitfall 2: Feedback Loop Amplification**\n",
    "\n",
    "RL systems can create self-reinforcing feedback loops:\n",
    "- Actions influence future data distribution\n",
    "- Biases get amplified over time\n",
    "- Example: Predictive policing creating more arrests in already over-policed areas\n",
    "\n",
    "**Pitfall 3: Catastrophic Forgetting**\n",
    "\n",
    "When updating policies online:\n",
    "- New experiences can overwrite important learned behaviors\n",
    "- Performance on rare but important scenarios may degrade\n",
    "- Need for experience replay and regularization\n",
    "\n",
    "**Pitfall 4: Coordination Failures**\n",
    "\n",
    "Multiple RL agents or systems interacting:\n",
    "- Race conditions in action selection\n",
    "- Emergent behaviors from agent interactions\n",
    "- Example: Multiple trading bots causing flash crashes\n",
    "\n",
    "**Pitfall 5: Infrastructure Complexity**\n",
    "\n",
    "RL systems require complex infrastructure:\n",
    "- Experience collection and storage\n",
    "- Distributed training\n",
    "- Model serving with low latency\n",
    "- A/B testing frameworks\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "\n",
    "1. **Staged Rollouts**: Gradually increase traffic to new policies\n",
    "2. **Canary Deployments**: Test on small user segments first\n",
    "3. **Automatic Rollback**: Revert to previous policy if metrics degrade\n",
    "4. **Diversity Constraints**: Prevent over-optimization on single metrics\n",
    "5. **Regular Audits**: Periodic review of system behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring and Managing RL System Performance\n",
    "\n",
    "**Key Metrics to Monitor:**\n",
    "\n",
    "1. **Reward Metrics**\n",
    "   - Average reward per episode/interaction\n",
    "   - Reward distribution (not just mean)\n",
    "   - Reward trends over time\n",
    "\n",
    "2. **Policy Metrics**\n",
    "   - Action distribution entropy (exploration level)\n",
    "   - Policy divergence from baseline\n",
    "   - Value function estimates\n",
    "\n",
    "3. **Business Metrics**\n",
    "   - Conversion rates, engagement, revenue\n",
    "   - User satisfaction scores\n",
    "   - Long-term user retention\n",
    "\n",
    "4. **Safety Metrics**\n",
    "   - Constraint violation rates\n",
    "   - Fallback policy activation frequency\n",
    "   - Human override frequency\n",
    "\n",
    "5. **System Metrics**\n",
    "   - Inference latency\n",
    "   - Model serving errors\n",
    "   - Data pipeline health\n",
    "\n",
    "**Monitoring Best Practices:**\n",
    "\n",
    "```\n",
    "Monitoring Architecture:\n",
    "\n",
    "┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n",
    "│   RL Agent      │────▶│  Logging Layer  │────▶│   Metrics DB    │\n",
    "│   (Production)  │     │  (Actions, Obs) │     │  (Time Series)  │\n",
    "└─────────────────┘     └─────────────────┘     └─────────────────┘\n",
    "                                                        │\n",
    "                                                        ▼\n",
    "┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n",
    "│   Alert System  │◀────│   Dashboards    │◀────│   Analytics     │\n",
    "│   (PagerDuty)   │     │   (Grafana)     │     │   (Anomaly Det) │\n",
    "└─────────────────┘     └─────────────────┘     └─────────────────┘\n",
    "```\n",
    "\n",
    "**Alerting Strategies:**\n",
    "\n",
    "- **Threshold Alerts**: Trigger when metrics exceed bounds\n",
    "- **Anomaly Detection**: ML-based detection of unusual patterns\n",
    "- **Trend Alerts**: Detect gradual degradation\n",
    "- **Comparative Alerts**: Compare against baseline or control group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Robustness in RL\n",
    "\n",
    "**The Adversarial Threat:**\n",
    "\n",
    "RL systems can be vulnerable to adversarial attacks:\n",
    "\n",
    "1. **Observation Perturbations**\n",
    "   - Small changes to inputs cause wrong actions\n",
    "   - Similar to adversarial examples in image classification\n",
    "   - Example: Stickers on road signs fooling autonomous vehicles\n",
    "\n",
    "2. **Reward Poisoning**\n",
    "   - Attackers manipulate the reward signal\n",
    "   - Can cause agent to learn harmful behaviors\n",
    "   - Example: Fake reviews manipulating recommendation systems\n",
    "\n",
    "3. **Environment Manipulation**\n",
    "   - Adversaries modify the environment dynamics\n",
    "   - Exploit agent's learned assumptions\n",
    "   - Example: Market manipulation against trading bots\n",
    "\n",
    "4. **Policy Extraction**\n",
    "   - Attackers reverse-engineer the policy\n",
    "   - Can then find exploits or create competing systems\n",
    "   - Intellectual property concerns\n",
    "\n",
    "**Defense Strategies:**\n",
    "\n",
    "1. **Adversarial Training**\n",
    "   - Train against adversarial perturbations\n",
    "   - Include adversarial examples in training data\n",
    "\n",
    "2. **Robust Optimization**\n",
    "   - Optimize for worst-case performance\n",
    "   - $\\max_\\pi \\min_{\\delta} J(\\pi, \\delta)$ where $\\delta$ is adversarial perturbation\n",
    "\n",
    "3. **Input Validation**\n",
    "   - Detect and reject anomalous inputs\n",
    "   - Sanity checks on observations\n",
    "\n",
    "4. **Ensemble Methods**\n",
    "   - Use multiple policies and aggregate decisions\n",
    "   - Harder to fool all policies simultaneously\n",
    "\n",
    "5. **Certified Defenses**\n",
    "   - Provable bounds on robustness\n",
    "   - Guarantee correct behavior within perturbation bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RL for Data Center Energy Efficiency\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "Data centers consume approximately 1-2% of global electricity. Cooling systems alone can account for 30-40% of a data center's energy consumption. RL offers a promising approach to optimize this.\n",
    "\n",
    "**Google DeepMind's Success:**\n",
    "\n",
    "In 2016, DeepMind applied RL to Google's data center cooling:\n",
    "- **40% reduction** in cooling energy consumption\n",
    "- **15% reduction** in overall PUE (Power Usage Effectiveness)\n",
    "- Saved hundreds of millions of dollars\n",
    "\n",
    "**RL Formulation:**\n",
    "\n",
    "- **State**: Temperature sensors, power consumption, weather, workload\n",
    "- **Actions**: Cooling system settings, airflow adjustments\n",
    "- **Reward**: Negative energy consumption while maintaining safe temperatures\n",
    "\n",
    "**Key Challenges:**\n",
    "\n",
    "1. **Safety Constraints**: Cannot allow temperatures to exceed safe limits\n",
    "2. **Delayed Effects**: Cooling changes take time to propagate\n",
    "3. **Complex Dynamics**: Interactions between many systems\n",
    "4. **Rare Events**: Must handle unusual situations (heat waves, equipment failures)\n",
    "\n",
    "**Implementation Approach:**\n",
    "\n",
    "1. Build accurate simulator from historical data\n",
    "2. Train RL agent in simulation with safety constraints\n",
    "3. Deploy with human oversight and safety bounds\n",
    "4. Continuously improve with real-world data\n",
    "\n",
    "**Broader Applications:**\n",
    "\n",
    "- HVAC optimization in commercial buildings\n",
    "- Smart grid load balancing\n",
    "- Industrial process optimization\n",
    "- Renewable energy integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emerging Trends in RL for Financial Technology\n",
    "\n",
    "**Current Applications:**\n",
    "\n",
    "1. **Algorithmic Trading**\n",
    "   - Portfolio optimization and rebalancing\n",
    "   - Market making and liquidity provision\n",
    "   - Execution optimization (minimizing market impact)\n",
    "\n",
    "2. **Risk Management**\n",
    "   - Dynamic hedging strategies\n",
    "   - Credit risk assessment\n",
    "   - Fraud detection and prevention\n",
    "\n",
    "3. **Personalized Finance**\n",
    "   - Robo-advisors with personalized strategies\n",
    "   - Dynamic pricing and offers\n",
    "   - Customer lifetime value optimization\n",
    "\n",
    "**Emerging Trends:**\n",
    "\n",
    "1. **Multi-Agent Market Simulation**\n",
    "   - Simulating market dynamics with RL agents\n",
    "   - Testing strategies before deployment\n",
    "   - Understanding emergent market behaviors\n",
    "\n",
    "2. **Explainable Financial RL**\n",
    "   - Regulatory requirements for explainability\n",
    "   - Building trust with clients and regulators\n",
    "   - Audit trails for trading decisions\n",
    "\n",
    "3. **Safe Exploration in Finance**\n",
    "   - Constrained RL for risk limits\n",
    "   - Conservative exploration strategies\n",
    "   - Worst-case optimization\n",
    "\n",
    "4. **Transfer Learning Across Markets**\n",
    "   - Leveraging knowledge from one market to another\n",
    "   - Adapting to new financial instruments\n",
    "   - Handling regime changes\n",
    "\n",
    "**Challenges Specific to Finance:**\n",
    "\n",
    "- **Non-Stationarity**: Markets constantly evolve\n",
    "- **Low Signal-to-Noise**: Financial data is noisy\n",
    "- **Adversarial Environment**: Other agents actively compete\n",
    "- **Regulatory Constraints**: Must comply with financial regulations\n",
    "- **Tail Risks**: Must handle rare but catastrophic events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipeline'></a>\n",
    "### End-to-End Deployment Pipeline\n",
    "\n",
    "This section provides a complete pipeline for training, validating, and deploying RL models in production. We'll cover each stage with practical code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Pipeline Overview\n",
    "\n",
    "A production RL pipeline consists of several interconnected stages:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                        RL Production Pipeline                                │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                              │\n",
    "│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐              │\n",
    "│  │  Data    │───▶│ Training │───▶│Validation│───▶│Deployment│              │\n",
    "│  │Collection│    │ Pipeline │    │ & Testing│    │ & Serving│              │\n",
    "│  └──────────┘    └──────────┘    └──────────┘    └──────────┘              │\n",
    "│       │                                               │                     │\n",
    "│       │         ┌──────────────────────────┐          │                     │\n",
    "│       └────────▶│   Monitoring & Logging   │◀─────────┘                     │\n",
    "│                 └──────────────────────────┘                                │\n",
    "│                              │                                              │\n",
    "│                              ▼                                              │\n",
    "│                 ┌──────────────────────────┐                                │\n",
    "│                 │   Feedback & Retraining  │                                │\n",
    "│                 └──────────────────────────┘                                │\n",
    "│                                                                              │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Data Collection**: Gathering experiences from environments or simulations\n",
    "2. **Training Pipeline**: Distributed training with hyperparameter optimization\n",
    "3. **Validation & Testing**: Ensuring policy quality before deployment\n",
    "4. **Deployment & Serving**: Low-latency model serving infrastructure\n",
    "5. **Monitoring & Logging**: Tracking performance and detecting issues\n",
    "6. **Feedback & Retraining**: Continuous improvement from production data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Pipeline Setup\n",
    "\n",
    "A robust training pipeline handles data collection, model training, and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Pipeline class defined!\n",
      "\n",
      "Key features:\n",
      "  - Automatic checkpointing of best and periodic models\n",
      "  - Metrics logging with timestamps\n",
      "  - Configurable evaluation intervals\n",
      "  - Support for both old and new Gym API\n"
     ]
    }
   ],
   "source": [
    "# Training Pipeline for Production RL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "class TrainingPipeline:\n",
    "    \"\"\"\n",
    "    A production-ready training pipeline for RL agents.\n",
    "    \n",
    "    Features:\n",
    "    - Checkpointing and model versioning\n",
    "    - Metrics logging\n",
    "    - Hyperparameter tracking\n",
    "    - Early stopping\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, agent, env, config):\n",
    "        \"\"\"\n",
    "        Initialize the training pipeline.\n",
    "        \n",
    "        Args:\n",
    "            agent: The RL agent to train\n",
    "            env: The training environment\n",
    "            config: Training configuration dictionary\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "        \n",
    "        # Training state\n",
    "        self.episode = 0\n",
    "        self.total_steps = 0\n",
    "        self.best_reward = float('-inf')\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.metrics_history = {\n",
    "            'episode_rewards': [],\n",
    "            'episode_lengths': [],\n",
    "            'losses': [],\n",
    "            'timestamps': []\n",
    "        }\n",
    "        \n",
    "        # Setup directories\n",
    "        self.checkpoint_dir = config.get('checkpoint_dir', './checkpoints')\n",
    "        self.log_dir = config.get('log_dir', './logs')\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        \n",
    "    def train(self, num_episodes, eval_interval=100):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "        \n",
    "        Args:\n",
    "            num_episodes: Number of episodes to train\n",
    "            eval_interval: Episodes between evaluations\n",
    "        \"\"\"\n",
    "        print(f\"Starting training for {num_episodes} episodes...\")\n",
    "        print(f\"Config: {self.config}\")\n",
    "        \n",
    "        for ep in range(num_episodes):\n",
    "            self.episode = ep\n",
    "            \n",
    "            # Collect episode\n",
    "            episode_reward, episode_length = self._run_episode()\n",
    "            \n",
    "            # Log metrics\n",
    "            self._log_metrics(episode_reward, episode_length)\n",
    "            \n",
    "            # Periodic evaluation and checkpointing\n",
    "            if (ep + 1) % eval_interval == 0:\n",
    "                eval_reward = self._evaluate()\n",
    "                print(f\"Episode {ep+1}/{num_episodes} | \"\n",
    "                      f\"Train Reward: {episode_reward:.2f} | \"\n",
    "                      f\"Eval Reward: {eval_reward:.2f}\")\n",
    "                \n",
    "                # Save best model\n",
    "                if eval_reward > self.best_reward:\n",
    "                    self.best_reward = eval_reward\n",
    "                    self._save_checkpoint('best')\n",
    "                    \n",
    "            # Regular checkpointing\n",
    "            if (ep + 1) % (eval_interval * 5) == 0:\n",
    "                self._save_checkpoint(f'episode_{ep+1}')\n",
    "                \n",
    "        # Final save\n",
    "        self._save_checkpoint('final')\n",
    "        self._save_metrics()\n",
    "        print(\"Training complete!\")\n",
    "        \n",
    "    def _run_episode(self):\n",
    "        \"\"\"Run a single training episode.\"\"\"\n",
    "        state = self.env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = self.agent.select_action(state)\n",
    "            \n",
    "            # Take step\n",
    "            result = self.env.step(action)\n",
    "            if len(result) == 5:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = result\n",
    "            \n",
    "            # Update agent\n",
    "            if hasattr(self.agent, 'update'):\n",
    "                self.agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            self.total_steps += 1\n",
    "            \n",
    "        return episode_reward, episode_length\n",
    "    \n",
    "    def _evaluate(self, num_episodes=10):\n",
    "        \"\"\"Evaluate current policy.\"\"\"\n",
    "        rewards = []\n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.agent.select_action(state, explore=False)\n",
    "                result = self.env.step(action)\n",
    "                if len(result) == 5:\n",
    "                    state, reward, terminated, truncated, _ = result\n",
    "                    done = terminated or truncated\n",
    "                else:\n",
    "                    state, reward, done, _ = result\n",
    "                episode_reward += reward\n",
    "                \n",
    "            rewards.append(episode_reward)\n",
    "        return np.mean(rewards)\n",
    "    \n",
    "    def _log_metrics(self, reward, length):\n",
    "        \"\"\"Log training metrics.\"\"\"\n",
    "        self.metrics_history['episode_rewards'].append(reward)\n",
    "        self.metrics_history['episode_lengths'].append(length)\n",
    "        self.metrics_history['timestamps'].append(datetime.now().isoformat())\n",
    "        \n",
    "    def _save_checkpoint(self, name):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint = {\n",
    "            'episode': self.episode,\n",
    "            'total_steps': self.total_steps,\n",
    "            'best_reward': self.best_reward,\n",
    "            'config': self.config,\n",
    "            'agent_state': self.agent.get_state() if hasattr(self.agent, 'get_state') else None\n",
    "        }\n",
    "        \n",
    "        path = os.path.join(self.checkpoint_dir, f'{name}.pt')\n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"Saved checkpoint: {path}\")\n",
    "        \n",
    "    def _save_metrics(self):\n",
    "        \"\"\"Save training metrics to JSON.\"\"\"\n",
    "        path = os.path.join(self.log_dir, 'metrics.json')\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.metrics_history, f, indent=2)\n",
    "        print(f\"Saved metrics: {path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"Training Pipeline class defined!\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - Automatic checkpointing of best and periodic models\")\n",
    "print(\"  - Metrics logging with timestamps\")\n",
    "print(\"  - Configurable evaluation intervals\")\n",
    "print(\"  - Support for both old and new Gym API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Strategies\n",
    "\n",
    "Before deploying an RL model, thorough validation is essential to ensure safe and effective behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLValidator class defined!\n",
      "\n",
      "Validation capabilities:\n",
      "  - Performance validation against minimum thresholds\n",
      "  - Safety constraint checking\n",
      "  - Robustness testing with observation noise\n",
      "  - Comprehensive report generation\n"
     ]
    }
   ],
   "source": [
    "# Validation Strategies for RL Models\n",
    "import numpy as np\n",
    "from typing import List, Dict, Callable\n",
    "\n",
    "class RLValidator:\n",
    "    \"\"\"\n",
    "    Comprehensive validation suite for RL policies.\n",
    "    \n",
    "    Validates:\n",
    "    - Performance metrics\n",
    "    - Safety constraints\n",
    "    - Robustness to perturbations\n",
    "    - Edge case handling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, policy):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.results = {}\n",
    "        \n",
    "    def validate_performance(self, num_episodes=100, min_reward=None):\n",
    "        \"\"\"\n",
    "        Validate policy performance meets minimum requirements.\n",
    "        \n",
    "        Args:\n",
    "            num_episodes: Number of evaluation episodes\n",
    "            min_reward: Minimum acceptable average reward\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.policy(state)\n",
    "                result = self.env.step(action)\n",
    "                if len(result) == 5:\n",
    "                    state, reward, terminated, truncated, _ = result\n",
    "                    done = terminated or truncated\n",
    "                else:\n",
    "                    state, reward, done, _ = result\n",
    "                episode_reward += reward\n",
    "                \n",
    "            rewards.append(episode_reward)\n",
    "        \n",
    "        avg_reward = np.mean(rewards)\n",
    "        std_reward = np.std(rewards)\n",
    "        \n",
    "        self.results['performance'] = {\n",
    "            'mean_reward': avg_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'min_reward': np.min(rewards),\n",
    "            'max_reward': np.max(rewards),\n",
    "            'passed': min_reward is None or avg_reward >= min_reward\n",
    "        }\n",
    "        \n",
    "        return self.results['performance']\n",
    "    \n",
    "    def validate_safety(self, constraint_fn: Callable, num_episodes=100, max_violations=0):\n",
    "        \"\"\"\n",
    "        Validate policy satisfies safety constraints.\n",
    "        \n",
    "        Args:\n",
    "            constraint_fn: Function(state, action) -> bool (True if safe)\n",
    "            num_episodes: Number of episodes to test\n",
    "            max_violations: Maximum allowed constraint violations\n",
    "        \"\"\"\n",
    "        total_violations = 0\n",
    "        total_steps = 0\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.policy(state)\n",
    "                \n",
    "                # Check constraint\n",
    "                if not constraint_fn(state, action):\n",
    "                    total_violations += 1\n",
    "                \n",
    "                result = self.env.step(action)\n",
    "                if len(result) == 5:\n",
    "                    state, _, terminated, truncated, _ = result\n",
    "                    done = terminated or truncated\n",
    "                else:\n",
    "                    state, _, done, _ = result\n",
    "                total_steps += 1\n",
    "        \n",
    "        violation_rate = total_violations / total_steps if total_steps > 0 else 0\n",
    "        \n",
    "        self.results['safety'] = {\n",
    "            'total_violations': total_violations,\n",
    "            'total_steps': total_steps,\n",
    "            'violation_rate': violation_rate,\n",
    "            'passed': total_violations <= max_violations\n",
    "        }\n",
    "        \n",
    "        return self.results['safety']\n",
    "    \n",
    "    def validate_robustness(self, noise_levels=[0.01, 0.05, 0.1], num_episodes=50):\n",
    "        \"\"\"\n",
    "        Test policy robustness to observation noise.\n",
    "        \n",
    "        Args:\n",
    "            noise_levels: List of noise standard deviations to test\n",
    "            num_episodes: Episodes per noise level\n",
    "        \"\"\"\n",
    "        robustness_results = {}\n",
    "        \n",
    "        for noise in noise_levels:\n",
    "            rewards = []\n",
    "            for _ in range(num_episodes):\n",
    "                state = self.env.reset()\n",
    "                if isinstance(state, tuple):\n",
    "                    state = state[0]\n",
    "                episode_reward = 0\n",
    "                done = False\n",
    "                \n",
    "                while not done:\n",
    "                    # Add noise to observation\n",
    "                    noisy_state = state + np.random.normal(0, noise, state.shape)\n",
    "                    action = self.policy(noisy_state)\n",
    "                    \n",
    "                    result = self.env.step(action)\n",
    "                    if len(result) == 5:\n",
    "                        state, reward, terminated, truncated, _ = result\n",
    "                        done = terminated or truncated\n",
    "                    else:\n",
    "                        state, reward, done, _ = result\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                rewards.append(episode_reward)\n",
    "            \n",
    "            robustness_results[f'noise_{noise}'] = {\n",
    "                'mean_reward': np.mean(rewards),\n",
    "                'std_reward': np.std(rewards)\n",
    "            }\n",
    "        \n",
    "        self.results['robustness'] = robustness_results\n",
    "        return robustness_results\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate validation report.\"\"\"\n",
    "        report = \"=\" * 60 + \"\\n\"\n",
    "        report += \"RL Policy Validation Report\\n\"\n",
    "        report += \"=\" * 60 + \"\\n\\n\"\n",
    "        \n",
    "        for test_name, results in self.results.items():\n",
    "            report += f\"### {test_name.upper()} ###\\n\"\n",
    "            if isinstance(results, dict):\n",
    "                for key, value in results.items():\n",
    "                    report += f\"  {key}: {value}\\n\"\n",
    "            report += \"\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "\n",
    "# Example usage demonstration\n",
    "print(\"RLValidator class defined!\")\n",
    "print(\"\\nValidation capabilities:\")\n",
    "print(\"  - Performance validation against minimum thresholds\")\n",
    "print(\"  - Safety constraint checking\")\n",
    "print(\"  - Robustness testing with observation noise\")\n",
    "print(\"  - Comprehensive report generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Serialization and Loading\n",
    "\n",
    "Proper model serialization is crucial for reproducibility and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicySerializer class defined!\n",
      "\n",
      "Serialization features:\n",
      "  - Metadata preservation (timestamp, architecture, parameters)\n",
      "  - Integrity verification via MD5 hash\n",
      "  - TorchScript export for optimized inference\n",
      "\n",
      "Example usage:\n",
      "  PolicySerializer.save_policy(model, 'policy.pt', {'version': '1.0'})\n",
      "  PolicySerializer.load_policy(model, 'policy.pt')\n"
     ]
    }
   ],
   "source": [
    "# Model Serialization for RL Policies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "class PolicySerializer:\n",
    "    \"\"\"\n",
    "    Handles serialization and loading of RL policies with metadata.\n",
    "    \n",
    "    Features:\n",
    "    - Version tracking\n",
    "    - Metadata preservation\n",
    "    - Integrity verification\n",
    "    - Format compatibility\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_policy(policy_network, path, metadata=None):\n",
    "        \"\"\"\n",
    "        Save a policy network with metadata.\n",
    "        \n",
    "        Args:\n",
    "            policy_network: PyTorch nn.Module\n",
    "            path: Save path\n",
    "            metadata: Optional metadata dictionary\n",
    "        \"\"\"\n",
    "        # Prepare metadata\n",
    "        save_metadata = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'architecture': str(policy_network),\n",
    "            'num_parameters': sum(p.numel() for p in policy_network.parameters()),\n",
    "        }\n",
    "        \n",
    "        if metadata:\n",
    "            save_metadata.update(metadata)\n",
    "        \n",
    "        # Create checkpoint\n",
    "        checkpoint = {\n",
    "            'model_state_dict': policy_network.state_dict(),\n",
    "            'metadata': save_metadata\n",
    "        }\n",
    "        \n",
    "        # Save\n",
    "        torch.save(checkpoint, path)\n",
    "        \n",
    "        # Calculate and store hash for integrity\n",
    "        with open(path, 'rb') as f:\n",
    "            file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "        \n",
    "        # Save metadata separately for quick access\n",
    "        meta_path = path.replace('.pt', '_meta.json')\n",
    "        save_metadata['file_hash'] = file_hash\n",
    "        with open(meta_path, 'w') as f:\n",
    "            json.dump(save_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved policy to {path}\")\n",
    "        print(f\"  Parameters: {save_metadata['num_parameters']:,}\")\n",
    "        print(f\"  Hash: {file_hash}\")\n",
    "        \n",
    "        return file_hash\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_policy(policy_network, path, verify_hash=True):\n",
    "        \"\"\"\n",
    "        Load a policy network with optional integrity verification.\n",
    "        \n",
    "        Args:\n",
    "            policy_network: PyTorch nn.Module (architecture must match)\n",
    "            path: Load path\n",
    "            verify_hash: Whether to verify file integrity\n",
    "        \"\"\"\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "        \n",
    "        # Verify integrity if requested\n",
    "        if verify_hash:\n",
    "            meta_path = path.replace('.pt', '_meta.json')\n",
    "            try:\n",
    "                with open(meta_path, 'r') as f:\n",
    "                    saved_meta = json.load(f)\n",
    "                \n",
    "                with open(path, 'rb') as f:\n",
    "                    current_hash = hashlib.md5(f.read()).hexdigest()\n",
    "                \n",
    "                if current_hash != saved_meta.get('file_hash'):\n",
    "                    raise ValueError(\"File integrity check failed!\")\n",
    "                print(\"Integrity verified ✓\")\n",
    "            except FileNotFoundError:\n",
    "                print(\"Warning: Metadata file not found, skipping integrity check\")\n",
    "        \n",
    "        # Load state dict\n",
    "        policy_network.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(f\"Loaded policy from {path}\")\n",
    "        print(f\"  Saved: {checkpoint['metadata'].get('timestamp', 'Unknown')}\")\n",
    "        \n",
    "        return checkpoint['metadata']\n",
    "    \n",
    "    @staticmethod\n",
    "    def export_for_inference(policy_network, path, example_input):\n",
    "        \"\"\"\n",
    "        Export policy for optimized inference (TorchScript).\n",
    "        \n",
    "        Args:\n",
    "            policy_network: PyTorch nn.Module\n",
    "            path: Export path\n",
    "            example_input: Example input tensor for tracing\n",
    "        \"\"\"\n",
    "        policy_network.eval()\n",
    "        \n",
    "        # Trace the model\n",
    "        traced = torch.jit.trace(policy_network, example_input)\n",
    "        \n",
    "        # Save traced model\n",
    "        traced.save(path)\n",
    "        print(f\"Exported TorchScript model to {path}\")\n",
    "        \n",
    "        return traced\n",
    "\n",
    "\n",
    "# Example policy network for demonstration\n",
    "class SimplePolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim=4, action_dim=2, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# Demonstrate serialization\n",
    "print(\"PolicySerializer class defined!\")\n",
    "print(\"\\nSerialization features:\")\n",
    "print(\"  - Metadata preservation (timestamp, architecture, parameters)\")\n",
    "print(\"  - Integrity verification via MD5 hash\")\n",
    "print(\"  - TorchScript export for optimized inference\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  PolicySerializer.save_policy(model, 'policy.pt', {'version': '1.0'})\")\n",
    "print(\"  PolicySerializer.load_policy(model, 'policy.pt')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment Architecture Considerations\n",
    "\n",
    "A production RL deployment requires careful architectural planning for reliability and scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLServingSystem class defined!\n",
      "\n",
      "Deployment features:\n",
      "  - Low-latency inference with batching support\n",
      "  - Automatic fallback to backup policy on errors\n",
      "  - A/B testing with configurable traffic splits\n",
      "  - Real-time latency and error tracking\n"
     ]
    }
   ],
   "source": [
    "# Deployment Architecture for RL Systems\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "class RLServingSystem:\n",
    "    \"\"\"\n",
    "    Production serving system for RL policies.\n",
    "    \n",
    "    Features:\n",
    "    - Low-latency inference\n",
    "    - Request batching\n",
    "    - Fallback policies\n",
    "    - A/B testing support\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, primary_policy, fallback_policy=None):\n",
    "        \"\"\"\n",
    "        Initialize serving system.\n",
    "        \n",
    "        Args:\n",
    "            primary_policy: Main policy model\n",
    "            fallback_policy: Backup policy for failures\n",
    "        \"\"\"\n",
    "        self.primary_policy = primary_policy\n",
    "        self.fallback_policy = fallback_policy\n",
    "        self.primary_policy.eval()\n",
    "        \n",
    "        # Metrics\n",
    "        self.request_count = 0\n",
    "        self.fallback_count = 0\n",
    "        self.latencies = deque(maxlen=1000)\n",
    "        \n",
    "        # A/B testing\n",
    "        self.ab_policies = {}\n",
    "        self.ab_traffic_split = {}\n",
    "        \n",
    "    def predict(self, state: np.ndarray, experiment_id: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get action prediction for a state.\n",
    "        \n",
    "        Args:\n",
    "            state: Environment state\n",
    "            experiment_id: Optional A/B test experiment ID\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with action and metadata\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.request_count += 1\n",
    "        \n",
    "        try:\n",
    "            # Select policy (A/B testing)\n",
    "            if experiment_id and experiment_id in self.ab_policies:\n",
    "                policy = self._select_ab_policy(experiment_id)\n",
    "                policy_name = f\"ab_{experiment_id}\"\n",
    "            else:\n",
    "                policy = self.primary_policy\n",
    "                policy_name = \"primary\"\n",
    "            \n",
    "            # Convert to tensor\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                action_probs = policy(state_tensor)\n",
    "                action = torch.argmax(action_probs, dim=-1).item()\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            self.latencies.append(latency)\n",
    "            \n",
    "            return {\n",
    "                'action': action,\n",
    "                'action_probs': action_probs.numpy().tolist(),\n",
    "                'policy': policy_name,\n",
    "                'latency_ms': latency * 1000,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback to backup policy\n",
    "            self.fallback_count += 1\n",
    "            \n",
    "            if self.fallback_policy is not None:\n",
    "                action = self.fallback_policy(state)\n",
    "                return {\n",
    "                    'action': action,\n",
    "                    'policy': 'fallback',\n",
    "                    'status': 'fallback',\n",
    "                    'error': str(e)\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'action': 0,  # Default action\n",
    "                    'policy': 'default',\n",
    "                    'status': 'error',\n",
    "                    'error': str(e)\n",
    "                }\n",
    "    \n",
    "    def register_ab_policy(self, experiment_id: str, policy, traffic_fraction: float):\n",
    "        \"\"\"Register a policy for A/B testing.\"\"\"\n",
    "        self.ab_policies[experiment_id] = policy\n",
    "        self.ab_traffic_split[experiment_id] = traffic_fraction\n",
    "        policy.eval()\n",
    "        print(f\"Registered A/B policy: {experiment_id} ({traffic_fraction*100}% traffic)\")\n",
    "    \n",
    "    def _select_ab_policy(self, experiment_id: str):\n",
    "        \"\"\"Select policy based on traffic split.\"\"\"\n",
    "        if np.random.random() < self.ab_traffic_split[experiment_id]:\n",
    "            return self.ab_policies[experiment_id]\n",
    "        return self.primary_policy\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get serving metrics.\"\"\"\n",
    "        return {\n",
    "            'total_requests': self.request_count,\n",
    "            'fallback_rate': self.fallback_count / max(1, self.request_count),\n",
    "            'avg_latency_ms': np.mean(self.latencies) * 1000 if self.latencies else 0,\n",
    "            'p99_latency_ms': np.percentile(self.latencies, 99) * 1000 if self.latencies else 0,\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"RLServingSystem class defined!\")\n",
    "print(\"\\nDeployment features:\")\n",
    "print(\"  - Low-latency inference with batching support\")\n",
    "print(\"  - Automatic fallback to backup policy on errors\")\n",
    "print(\"  - A/B testing with configurable traffic splits\")\n",
    "print(\"  - Real-time latency and error tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring Setup\n",
    "\n",
    "Comprehensive monitoring is essential for maintaining RL system health in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLMonitor class defined!\n",
      "\n",
      "Monitoring capabilities:\n",
      "  - Rolling window metrics for rewards, latency, errors\n",
      "  - Baseline calibration for anomaly detection\n",
      "  - Automatic alerting on degradation\n",
      "  - Dashboard-ready metrics export\n"
     ]
    }
   ],
   "source": [
    "# Monitoring System for Production RL\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class RLMonitor:\n",
    "    \"\"\"\n",
    "    Monitoring system for production RL deployments.\n",
    "    \n",
    "    Tracks:\n",
    "    - Reward metrics\n",
    "    - Policy behavior\n",
    "    - System health\n",
    "    - Anomaly detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=1000, alert_thresholds=None):\n",
    "        \"\"\"\n",
    "        Initialize monitoring system.\n",
    "        \n",
    "        Args:\n",
    "            window_size: Size of rolling window for metrics\n",
    "            alert_thresholds: Dictionary of metric thresholds for alerts\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.alert_thresholds = alert_thresholds or {}\n",
    "        \n",
    "        # Metric buffers\n",
    "        self.rewards = deque(maxlen=window_size)\n",
    "        self.actions = deque(maxlen=window_size)\n",
    "        self.latencies = deque(maxlen=window_size)\n",
    "        self.errors = deque(maxlen=window_size)\n",
    "        \n",
    "        # Baseline statistics (set during calibration)\n",
    "        self.baseline_reward_mean = None\n",
    "        self.baseline_reward_std = None\n",
    "        \n",
    "        # Alert history\n",
    "        self.alerts = []\n",
    "        \n",
    "    def log_interaction(self, state, action, reward, latency_ms, error=None):\n",
    "        \"\"\"Log a single interaction.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "        self.actions.append(action)\n",
    "        self.latencies.append(latency_ms)\n",
    "        self.errors.append(1 if error else 0)\n",
    "        \n",
    "        # Check for anomalies\n",
    "        self._check_alerts()\n",
    "    \n",
    "    def calibrate_baseline(self, rewards):\n",
    "        \"\"\"Set baseline statistics from historical data.\"\"\"\n",
    "        self.baseline_reward_mean = np.mean(rewards)\n",
    "        self.baseline_reward_std = np.std(rewards)\n",
    "        print(f\"Baseline calibrated: mean={self.baseline_reward_mean:.3f}, std={self.baseline_reward_std:.3f}\")\n",
    "    \n",
    "    def _check_alerts(self):\n",
    "        \"\"\"Check for alert conditions.\"\"\"\n",
    "        if len(self.rewards) < 100:\n",
    "            return\n",
    "        \n",
    "        # Reward degradation alert\n",
    "        if self.baseline_reward_mean is not None:\n",
    "            recent_mean = np.mean(list(self.rewards)[-100:])\n",
    "            threshold = self.alert_thresholds.get('reward_degradation', 2.0)\n",
    "            \n",
    "            if recent_mean < self.baseline_reward_mean - threshold * self.baseline_reward_std:\n",
    "                self._raise_alert('reward_degradation', \n",
    "                    f'Reward dropped to {recent_mean:.3f} (baseline: {self.baseline_reward_mean:.3f})')\n",
    "        \n",
    "        # High error rate alert\n",
    "        error_rate = np.mean(list(self.errors)[-100:])\n",
    "        threshold = self.alert_thresholds.get('error_rate', 0.05)\n",
    "        if error_rate > threshold:\n",
    "            self._raise_alert('high_error_rate', f'Error rate: {error_rate:.2%}')\n",
    "        \n",
    "        # High latency alert\n",
    "        p99_latency = np.percentile(list(self.latencies)[-100:], 99)\n",
    "        threshold = self.alert_thresholds.get('latency_p99_ms', 100)\n",
    "        if p99_latency > threshold:\n",
    "            self._raise_alert('high_latency', f'P99 latency: {p99_latency:.1f}ms')\n",
    "    \n",
    "    def _raise_alert(self, alert_type, message):\n",
    "        \"\"\"Raise an alert.\"\"\"\n",
    "        alert = {\n",
    "            'type': alert_type,\n",
    "            'message': message,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        self.alerts.append(alert)\n",
    "        print(f\"⚠️  ALERT [{alert_type}]: {message}\")\n",
    "    \n",
    "    def get_dashboard_metrics(self):\n",
    "        \"\"\"Get metrics for dashboard display.\"\"\"\n",
    "        if not self.rewards:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'reward': {\n",
    "                'current': self.rewards[-1] if self.rewards else None,\n",
    "                'mean': np.mean(self.rewards),\n",
    "                'std': np.std(self.rewards),\n",
    "                'min': np.min(self.rewards),\n",
    "                'max': np.max(self.rewards)\n",
    "            },\n",
    "            'latency': {\n",
    "                'mean_ms': np.mean(self.latencies),\n",
    "                'p50_ms': np.percentile(self.latencies, 50),\n",
    "                'p99_ms': np.percentile(self.latencies, 99)\n",
    "            },\n",
    "            'errors': {\n",
    "                'rate': np.mean(self.errors),\n",
    "                'total': sum(self.errors)\n",
    "            },\n",
    "            'actions': {\n",
    "                'distribution': dict(zip(*np.unique(self.actions, return_counts=True)))\n",
    "            },\n",
    "            'alerts': {\n",
    "                'recent': self.alerts[-5:] if self.alerts else [],\n",
    "                'total': len(self.alerts)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def export_report(self, path):\n",
    "        \"\"\"Export monitoring report to JSON.\"\"\"\n",
    "        report = {\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'metrics': self.get_dashboard_metrics(),\n",
    "            'baseline': {\n",
    "                'reward_mean': self.baseline_reward_mean,\n",
    "                'reward_std': self.baseline_reward_std\n",
    "            },\n",
    "            'all_alerts': self.alerts\n",
    "        }\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        print(f\"Exported report to {path}\")\n",
    "\n",
    "\n",
    "print(\"RLMonitor class defined!\")\n",
    "print(\"\\nMonitoring capabilities:\")\n",
    "print(\"  - Rolling window metrics for rewards, latency, errors\")\n",
    "print(\"  - Baseline calibration for anomaly detection\")\n",
    "print(\"  - Automatic alerting on degradation\")\n",
    "print(\"  - Dashboard-ready metrics export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maintenance and Updates\n",
    "\n",
    "**Continuous Improvement Cycle:**\n",
    "\n",
    "1. **Data Collection**: Continuously gather production experiences\n",
    "2. **Offline Evaluation**: Test new policies on historical data\n",
    "3. **A/B Testing**: Gradually roll out improvements\n",
    "4. **Monitoring**: Track performance and detect regressions\n",
    "5. **Rollback**: Quickly revert if issues arise\n",
    "\n",
    "**Best Practices for Updates:**\n",
    "\n",
    "- **Version Control**: Track all model versions with metadata\n",
    "- **Staged Rollouts**: Start with small traffic percentage\n",
    "- **Canary Deployments**: Test on subset of users first\n",
    "- **Feature Flags**: Enable quick rollback without redeployment\n",
    "- **Shadow Mode**: Run new policy alongside old one without affecting users\n",
    "\n",
    "**Retraining Triggers:**\n",
    "\n",
    "- Performance degradation below threshold\n",
    "- Significant distribution shift in inputs\n",
    "- New data availability\n",
    "- Scheduled periodic retraining\n",
    "- Business requirement changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recent-research'></a>\n",
    "### Recent Research Highlights\n",
    "\n",
    "This section highlights significant recent advances in reinforcement learning from top conferences like NeurIPS, ICML, and ICLR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}